{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphReader: Building Graph-based Agent to Enhance\n",
      "Long-Context Abilities of Large Language Models\n",
      "Shilong Li∗1, Yancheng He∗1, Hangyu Guo∗1, Xingyuan Bu∗†‡1, Ge Bai1, Jie Liu2,3,\n",
      "Jiaheng Liu1, Xingwei Qu4, Yangguang Li3, Wanli Ouyang2,3, Wenbo Su1, Bo Zheng1\n",
      "1Alibaba Group2The Chinese University of Hong Kong\n",
      "3Shanghai AI Laboratory4University of Manchester\n",
      "{zhuli.lsl, buxingyuan.bxy}@taobao.com\n",
      "Abstract\n",
      "Long-context capabilities are essential for large\n",
      "language models (LLMs) to tackle complex\n",
      "and long-input tasks. Despite numerous efforts\n",
      "made to optimize LLMs for long contexts, chal-\n",
      "lenges persist in robustly processing long in-\n",
      "puts. In this paper, we introduce GraphReader,\n",
      "a graph-based agent system designed to han-\n",
      "dle long texts by structuring them into a graph\n",
      "and employing an agent to explore this graph\n",
      "autonomously. Upon receiving a question, the\n",
      "agent first undertakes a step-by-step analysis\n",
      "and devises a rational plan. It then invokes a\n",
      "set of predefined functions to read node con-\n",
      "tent and neighbors, facilitating a coarse-to-fine\n",
      "exploration of the graph. Throughout the ex-\n",
      "ploration, the agent continuously records new\n",
      "insights and reflects on current circumstances\n",
      "to optimize the process until it has gathered suf-\n",
      "ficient information to generate an answer. Ex-\n",
      "perimental results on the LV-Eval dataset reveal\n",
      "that GraphReader, using a 4k context window,\n",
      "consistently outperforms GPT-4-128k across\n",
      "context lengths from 16k to 256k by a large\n",
      "margin. Additionally, our approach demon-\n",
      "strates superior performance on four challeng-\n",
      "ing single-hop and multi-hop benchmarks.\n",
      "1 Introduction\n",
      "Large language models (LLMs) have made\n",
      "great progress on natural language understand-\n",
      "ing and generation (Zhao et al., 2023). However,\n",
      "transformer-based LLMs still struggle in handling\n",
      "long contexts due to the limitation of context win-\n",
      "dow and memory usage.\n",
      "Current techniques for solving the long-context\n",
      "tasks of LLMs can be divided into two perspec-\n",
      "tives: 1) Model-level, which includes finetuning\n",
      "with modified positional embeddings (Chen et al.,\n",
      "2023b; Zhu et al., 2023; Peng et al., 2023; Ding\n",
      "et al., 2024), and applying transformer variants\n",
      "∗First four authors contributed equally.\n",
      "†Corresponding Author. ‡Project Leader.\n",
      "16k 32k 64k 128k 256k\n",
      "Input Length5101520253035Average Scores (%)\n",
      "GraphReader\n",
      "ReadAgent\n",
      "GPT-4-128k\n",
      "GPT-4-128k (chunk w/ notes)GPT-4-128k (chunk)\n",
      "Ada-002 (top-1)\n",
      "BM25 (top-1)Figure 1: Performance on LV-Eval at 5 context length\n",
      "levels. GraphReader outperforms existing open-sourced\n",
      "and closed-source models while demonstrating a scal-\n",
      "able performance in very long contexts. In contrast,\n",
      "other models exhibit a significant decrease in perfor-\n",
      "mance as context length increases.\n",
      "with modified attention mechanisms (Dai et al.,\n",
      "2019; Munkhdalai et al., 2024; Gu and Dao, 2023);\n",
      "2) Agent-level, i.e.,employing retrieval-augmented\n",
      "LLM or agent to process long contexts with a lim-\n",
      "ited context window LLM (Nakano et al., 2021;\n",
      "Lee et al., 2024).\n",
      "However, model-level methods typically train\n",
      "LLMs with target length texts, posing challenges\n",
      "in constructing training datasets and incurring high\n",
      "training costs (Zhu et al., 2023). Additionally, long-\n",
      "context LLMs optimized with these methods tend\n",
      "to overlook crucial details in long contexts, known\n",
      "as “lost in the middle” (Liu et al., 2024), limit-\n",
      "ing their ability to address complex tasks, such as\n",
      "multi-hop questions. Agent-level approaches trans-\n",
      "form input text into a tree (Chen et al., 2023a) or\n",
      "paginated pages (Lee et al., 2024), failing to cap-\n",
      "ture multi-hop and long-range dependencies, thus\n",
      "limiting their effectiveness on very long contexts,\n",
      "as shown in Figure 1.arXiv:2406.14550v1  [cs.CL]  20 Jun 2024To address these issues, we propose a graph-\n",
      "based agent named GraphReader . As illustrated\n",
      "in Figure 2, GraphReader first segments long texts\n",
      "into discrete chunks, extracts essential informa-\n",
      "tion, and compresses these into key elements and\n",
      "atomic facts. These key elements and facts are\n",
      "then used to construct a graph with nodes repre-\n",
      "senting key elements and their associated atomic\n",
      "facts. This graph structure effectively captures\n",
      "long-range dependencies and multi-hop relation-\n",
      "ships within long text. Subsequently, GraphReader\n",
      "autonomously explores this graph using predefined\n",
      "functions, guided by a step-by-step rational plan.\n",
      "Based on a given question, the agent progressively\n",
      "accesses information from coarse key elements and\n",
      "atomic facts to detailed original text chunks, tak-\n",
      "ing notes and reflecting until it gathers sufficient\n",
      "information to generate an answer. In summary,\n",
      "our main contributions are threefold:\n",
      "•We introduce GraphReader, a novel agent sys-\n",
      "tem designed to organize long texts into a graph\n",
      "structure, leveraging predefined functions and\n",
      "notebook to facilitate planning and reflection\n",
      "during exploration.\n",
      "•GraphReader establishes a scalable long-context\n",
      "capability based on a 4k context window, demon-\n",
      "strating performance that is comparable to or\n",
      "surpasses GPT-4 with a 128k context window\n",
      "across varying context lengths.\n",
      "•Extensive experiments conducted on four\n",
      "challenging benchmarks demonstrate that\n",
      "GraphReader achieves superior performance in\n",
      "complex single-hop and multi-hop QA tasks.\n",
      "2 Related Work\n",
      "Long-Context LLMs Recent efforts (Chen et al.,\n",
      "2023b; Ding et al., 2024; Peng et al., 2023) have\n",
      "focused on positional interpolation (PI) to enhance\n",
      "long-context capabilities. However, these methods\n",
      "require training on full-length texts, leading to sig-\n",
      "nificant increases in data and training costs (Chen\n",
      "et al., 2023c; Fu et al., 2024; Bai et al., 2024b).\n",
      "Thus, PoSE (Zhu et al., 2023) and SkipAlign (Wu\n",
      "et al., 2024a) investigate data skip strategy, but tend\n",
      "to neglect detailed information in long texts (Liu\n",
      "et al., 2024; Bai et al., 2024a; Wu et al., 2024b).\n",
      "Furthermore, despite how extensively the context\n",
      "window is expanded, it remains constrained by a\n",
      "predefined fixed length. To address these limita-\n",
      "tions, transformer variants with modified attention\n",
      "mechanisms have been proposed (Dai et al., 2019;Gu and Dao, 2023; Munkhdalai et al., 2024). How-\n",
      "ever, these models are prone to losing earlier infor-\n",
      "mation.\n",
      "Retrieval Retrieval Augmented Generation\n",
      "(RAG) leverages an extensive database of docu-\n",
      "ments to extract task-related information that aids\n",
      "in response generation. Many efforts investigate\n",
      "various levels of retrieval granularity, including\n",
      "tokens (Khandelwal et al., 2019), entities (Févry\n",
      "et al., 2020; De Jong et al., 2021), and chunks (Liu,\n",
      "2024; LangChain-team, 2024). Other approaches\n",
      "have explored diverse retrieval methods, such\n",
      "as BM25 (Rasooli and Tetreault, 2015) and\n",
      "learning-based strategies (Khattab and Zaharia,\n",
      "2020; Sachan et al., 2023; Sun et al., 2021).\n",
      "Despite its capabilities, RAG faces challenges in\n",
      "addressing complex questions due to difficulties in\n",
      "developing robust decision-making mechanisms.\n",
      "In contrast, we employ agents that use planning\n",
      "and reflection to gather essential information,\n",
      "effectively tackling complex problems.\n",
      "Agent for Retrieval Recent work has increas-\n",
      "ingly leveraged LLMs as agents to tackle complex\n",
      "problems, utilizing their strong planning and reflec-\n",
      "tion abilities (Yao et al., 2022; Park et al., 2023).\n",
      "These abilities have been applied to complex tasks\n",
      "such as function call (Li et al., 2023) and KGQA\n",
      "(Sun et al., 2023; Luo et al., 2023). Agents are also\n",
      "capable of retrieving unstructured information. For\n",
      "example, WebGPT (Nakano et al., 2021) simulates\n",
      "human actions to search on internet for specific\n",
      "answers. Additionally, MemWalker (Chen et al.,\n",
      "2023a) and PEARL (Sarthi et al., 2024) organize\n",
      "documents into a tree structure, while ReadAgent\n",
      "(Lee et al., 2024) condenses documents into a gist\n",
      "memory directory. However, these approaches of-\n",
      "ten struggle with multi-hop questions. KGP (Wang\n",
      "et al., 2024) organizes documents into graphs, but\n",
      "it primarily uses the agent to generate queries,\n",
      "thereby not fully exploiting the agent’s capabili-\n",
      "ties for planning and reflection.\n",
      "3 Approach\n",
      "3.1 Preliminary\n",
      "GraphReader is built on a graph G={V,E},\n",
      "where each node vi∈ V contains a key element ki\n",
      "and a set of summarized content, namely atomic\n",
      "factsAi. In other words, vi={ki,Ai}. And\n",
      "each edge eij∈ E represents the relationship be-\n",
      "tween nodes viandvj. This graph structure en-\n",
      "ables GraphReader to capture global informationAtomicFactsFromChunkID31.Thereareoftenpolicemanpresenceattheintersectionnexttotheschool.Node\n",
      "Question…NeighborsCanadaTorontoAlbum…(a)GraphConstructionLongContext\n",
      "Extract\n",
      "TorontoCasa LomaDankoJonesGothicAlbumCanada(b)GraphExplorationWhatisthenameofthecastleinthecitywheretheperformerofNeverTooLoudwasformed?\n",
      "Pre-planning\n",
      "InitialNodeSelectionRational Planweneedtoidentifytheperformerorbandassociatedwith\"NeverTooLoud\",determinethecitywheretheywereformed,andthenfindoutthenameofanynotablecastleinthatcity.KeyElementDankoJones\n",
      "read_chunk(1,3,5)stop_and_read_neighbor()Never Too LoudAlbumNotebookInitialization\n",
      "135QueueExploringChunks\n",
      "read_previous/subsequent_chunk()search_more()termination()35Queue\n",
      "ChunkID1ID1235Queuepop()insert()\n",
      "UpdatedNotebook1. The performer of Never Too Loud is DankoJones, which is a band from Toronto, Canada.2. The text mentions that the castle in Toronto is Casa Loma.InitialNotebookNone\n",
      "read_neighbor_node()\n",
      "Exploring Neighborstermination()\n",
      "Exploring Atomic Facts\n",
      "Question\n",
      "ResponseCasa Loma\n",
      "read_chunk(x,y,z)(c)AnswerReasoningAtomicFactsFromChunkID21.Thereareoftenpolicemanpresenceattheintersectionnexttotheschool.AtomicFactsFromChunkID11.\"NeverTooLoud\"isthefourthstudioalbumbyCanadianhardrockbandDankoJones.2.DankoJonesisaCanadianhardrocktriofromToronto.\n",
      "Rational PlanNotebookFigure 2: The illustration of our GraphReader approach, consisting of graph construction, graph exploration, and\n",
      "answer reasoning.\n",
      "from the input document Dwithin a limited context\n",
      "window, allowing it to decide whether to explore\n",
      "the current node in detail or jump to a neighbor-\n",
      "ing node. During graph exploration, GraphReader\n",
      "collects supporting facts and terminates the explo-\n",
      "ration once sufficient information has been gath-\n",
      "ered to answer the question. As illustrated in Fig-\n",
      "ure 2, the entire process of GraphReader consists\n",
      "of the following three phases: graph construction,\n",
      "graph exploration, and answer reasoning. The\n",
      "prompts utilized in these three stages are detailed in\n",
      "Appendix A, and a detailed example of our process\n",
      "can be found in Appendix H.\n",
      "3.2 Graph Construction\n",
      "To extract nodes from a document Dwithin the\n",
      "LLM’s context limit, we first split Dinto chunks\n",
      "of maximum length Lwhile preserving paragraph\n",
      "structure. For each chunk, we prompt the LLM to\n",
      "summarize it into atomic facts, the smallest indivis-\n",
      "ible facts that simplify the original text. We also\n",
      "prompt the LLM to extract key elements from each\n",
      "atomic fact like essential nouns, verbs, and adjec-\n",
      "tives. After processing all chunks, we normalizethe key elements as described by Lu et al. (2023) to\n",
      "handle lexical noise and granularity issues, creating\n",
      "a final set of key elements. We then construct each\n",
      "nodevi= (ki,Ai), where kiis a key element and\n",
      "Aiis the set of atomic facts corresponding to ki.\n",
      "Finally, we link two nodes viandvjif key element\n",
      "kiappears in Ajand vice versa.\n",
      "3.3 Graph Exploration\n",
      "3.3.1 Agent Initialization\n",
      "Given a graph Gand a question Q, our goal is to\n",
      "design an agent that can autonomously explore the\n",
      "graph using predefined functions. The agent begins\n",
      "by maintaining a notebook to record supporting\n",
      "facts, which are eventually used to derive the final\n",
      "answer. Then the agent performs two key initializa-\n",
      "tions: defining the rational plan and selecting the\n",
      "initial node.\n",
      "Rational Plan To tackle complex real-world\n",
      "multi-hop questions, pre-planning the solution is\n",
      "crucial. The agent breaks down the original ques-\n",
      "tion step-by-step, identifies the key information\n",
      "needed, and forms a rational plan.Initial Node Choosing strategic starting points\n",
      "is essential for improving search efficiency. The\n",
      "agent evaluates the key elements of all nodes Vand\n",
      "selects Ninitial nodes based on the question and\n",
      "the rational plan.\n",
      "3.3.2 Exploration\n",
      "After selecting Ninitial nodes as starting points, an\n",
      "agent explores each initial node by first exploring\n",
      "atomic facts, then chunks of the node. Next, it\n",
      "explores neighboring nodes, guided by the question\n",
      "and rational plan. The agent continuously updates\n",
      "the notebook with relevant information during the\n",
      "exploration process.\n",
      "Exploring Atomic Facts It is impractical to in-\n",
      "clude all original text chunks related to a node\n",
      "within the context window. Therefore, the agent\n",
      "employs a coarse-to-fine strategy, progressing from\n",
      "reading atomic facts to the original text, as all\n",
      "atomic facts can fit within the context window.\n",
      "Initially, all atomic facts associated with a node\n",
      "are grouped by their corresponding chunks, la-\n",
      "beled with the respective chunk IDs, and fed to\n",
      "the agent. This allows the agent to capture an\n",
      "overview of each chunk by reading all groups of\n",
      "atomic facts. Meanwhile, the agent utilizes the\n",
      "question, rational plan, and notes in its notebook to\n",
      "reflect on the required clues and determine which\n",
      "chunk is likely to contain useful information. Sub-\n",
      "sequently, the agent is provided with two functions:\n",
      "1)read_chunk , if the agent identifies certain chunks\n",
      "as valuable for further reading, it will complete\n",
      "the function parameters with the chunk IDs, i.e.,\n",
      "read_chunk(List[ID]) , and append these IDs to a\n",
      "chunk queue. 2) stop_and_read_neighbor , con-\n",
      "versely, if the agent deems that none of the chunks\n",
      "are worth further reading, it will finish reading this\n",
      "node and proceed to explore neighboring nodes.\n",
      "Exploring Chunks When the chunk queue is\n",
      "non-empty, it indicates that the agent has iden-\n",
      "tified multiple text chunks of interest. We then\n",
      "traverse the queue, reading each chunk. This\n",
      "step is essential because atomic facts merely sum-\n",
      "marize key information and provide brief clues,\n",
      "whereas specific details are best obtained directly\n",
      "from the original text chunks. While reading\n",
      "the chunks, the agent will once again consider\n",
      "the question and the plan, thinking about what\n",
      "can be added to the current notebook. Any sup-\n",
      "porting facts discovered will be recorded in the\n",
      "notebook. Depending on the updated notebook,the agent will then select one of the following\n",
      "four functions: 1) search_more , if supporting fact\n",
      "is insufficient, the agent will continue exploring\n",
      "chunks in the queue; 2) read_previous_chunk and\n",
      "3)read_subsequent_chunk , due to truncation issues,\n",
      "adjacent chunks might contain relevant and useful\n",
      "information, the agent may insert these IDs to the\n",
      "queue; 4) termination , if sufficient information has\n",
      "been gathered for answering the question, the agent\n",
      "will finish exploration.\n",
      "Exploring Neighbors Once the atomic facts and\n",
      "chunk queue of the current node have been fully\n",
      "processed, it indicates that this node has been thor-\n",
      "oughly explored, and the agent needs to access the\n",
      "next node. Taking into account the question, ra-\n",
      "tional plan, and the content of the notebook, the\n",
      "agent checks all neighboring nodes, i.e.,key el-\n",
      "ements, and performs one of two functions: 1)\n",
      "read_neighbor_node , the agent selects a neighbor-\n",
      "ing node that might be helpful in answering the\n",
      "question and re-enters the process of exploring\n",
      "atomic facts and chunks; 2) termination , the agent\n",
      "determines that none of the neighboring nodes con-\n",
      "tain useful information, it finish the exploration.\n",
      "3.4 Answer Reasoning\n",
      "After Nagents have independently gathered in-\n",
      "formation and stopped their exploration, we will\n",
      "compile all notes from each agent for reasoning\n",
      "and generating the final answer. Employing Chain-\n",
      "of-Thought (Wei et al., 2022), the LLM first an-\n",
      "alyzes each note by considering complementary\n",
      "information from other memories and using a ma-\n",
      "jority voting strategy to resolve any inconsistencies.\n",
      "Ultimately, the LLM will consider all the available\n",
      "information to generate the final answer.\n",
      "4 Experiments\n",
      "4.1 Experimental Settings\n",
      "Evaluation Benchmarks We conduct experi-\n",
      "ments on two types of long-context QA bench-\n",
      "marks, including multi-hop long-context QA,\n",
      "i.e.,HotpotQA (Yang et al., 2018), 2WikiMulti-\n",
      "hopQA (Ho et al., 2020), MuSiQue (Trivedi et al.,\n",
      "2022), and a single-hop long-context QA bench-\n",
      "mark, i.e.,NarrativeQA (Kociský et al., 2018) from\n",
      "LongBench (Bai et al., 2023). Additionally, we\n",
      "also incorporate HotpotWikiQA-mixup from LV-\n",
      "Eval (Yuan et al., 2024), a multi-hop benchmark\n",
      "that features five levels of text length: 16k, 32k,\n",
      "64k, 128k, and 256k. Table 1 presents the statisticsabout these benchmarks, and detailed information\n",
      "is provided in Appendix C.\n",
      "Evaluation Metrics We employ several auto-\n",
      "matic evaluation metrics, i.e.,F1score, Exact\n",
      "Match (EM) score, and an optimized F1* score, as\n",
      "introduced by LV-Eval (Yuan et al., 2024). Specif-\n",
      "ically, F1* first computes the recall of golden an-\n",
      "swer keywords and only calculates the F1score if\n",
      "it exceeds a certain threshold. Otherwise, the score\n",
      "defaults to zero. Despite the cost-effectiveness of\n",
      "automatic metrics, their accuracy may be affected\n",
      "by the response format. Hence, we implement\n",
      "LLM Raters for answer correctness evaluation us-\n",
      "ing an LLM, denoted as LLM-Rating-1 (LR-1) and\n",
      "LLM-Rating-1 (LR-2), following ReadAgent (Lee\n",
      "et al., 2024). Details on the evaluation metrics can\n",
      "be found in Appendix B.\n",
      "Baseline Methods We compare our approach\n",
      "with the following baselines: retrieval augmented\n",
      "generation (RAG), long-context LLM, and agent-\n",
      "based methods. (1) RAG : We choose Okapi\n",
      "BM25 (Robertson and Zaragoza, 2009) or Ope-\n",
      "nAI API embedding model Ada-002 to retrieve\n",
      "the chunks most relevant to the question and em-\n",
      "ploy GPT-4-128k ( gpt-4-1106-preview ) to read\n",
      "retrieved chunks and answer the question. (2)\n",
      "Long-context LLM : We select GPT-4-128k for\n",
      "directly reading full text when the text content fits\n",
      "within the input window, or for segmenting the\n",
      "text into chunks for sequential reading. (3) Agent-\n",
      "based Method : We select ReadAgent (Lee et al.,\n",
      "2024), which employs an agent-based system for\n",
      "the execution of retrieval and reading processes for\n",
      "long-context QA. The detailed description of these\n",
      "methods is provided in Appendix D.\n",
      "Implementation Details In our experiments, we\n",
      "employ GPT-4-128k for both our method and base-\n",
      "line approaches, setting the temperature to 0.2. For\n",
      "GraphReader, the input window size is configured\n",
      "to 4k tokens unless stated otherwise. We limit the\n",
      "maximum chunk size to 2k tokens, initiate searches\n",
      "from 5 initial nodes, and impose a function call\n",
      "limit of 10 for each search path.\n",
      "4.2 Main Results\n",
      "The results of three types of methods on four multi-\n",
      "hop long-context benchmarks and one single-hop\n",
      "https://platform.openai.com/docs/guides/embeddings/\n",
      "embedding-models\n",
      "https://github.com/openai/tiktokenTask Dataset Avg #Tokens Max #Tokens #Samples\n",
      "Multi-hop QAHotpotQA 9.4k 15.9k 300\n",
      "2WikiMultihopQA 8.8k 15.9k 300\n",
      "MuSiQue 15.5k 16.0k 200\n",
      "HotpotWikiQA-mixup 142.4k 370.8k 250\n",
      "Single-hop QA NarrativeQA 29.7k 63.7k 200\n",
      "Table 1: The statistics of benchmarks employed in our\n",
      "evaluation. The token number is calculated using the\n",
      "GPT-4 tokenizer from the TikToken. #Samples denote\n",
      "the total number of benchmarks.\n",
      "long-context benchmark are shown in Table 2 and\n",
      "Table 3. Based on the results, we have the following\n",
      "findings:\n",
      "Results of RAG methods As the results shown\n",
      "in Table 2, RAG methods based on BM25 and Ada-\n",
      "002 exhibit the worst performance in comparison\n",
      "to long-context LLM and agent-based methods. A\n",
      "possible reason is that text retrieval has difficulty\n",
      "recalling all chunks that contain the supporting\n",
      "facts for answering the input question. Although\n",
      "increasing the number of recalled chunks could im-\n",
      "prove the performance of text retrieval, the context\n",
      "window will limit the effectiveness of these RAG\n",
      "methods.\n",
      "Results of Long-Context LLMs From the re-\n",
      "sults shown in Table 2, we can see that employ-\n",
      "ing GPT-4-128k to directly answer the question\n",
      "with long contexts significantly outperforms RAG\n",
      "methods and even outperforms ReadAgent on three\n",
      "long-context benchmarks. This is because of the\n",
      "superior performance of GPT-4-128k in processing\n",
      "long texts and executing multi-hop reasoning tasks.\n",
      "Additionally, the lengths of these four benchmarks\n",
      "are significantly shorter than the 128k context win-\n",
      "dow, thereby mitigating the impact of “lost in the\n",
      "middle” on the model’s performance.\n",
      "Results of Agent-based Methods By comparing\n",
      "our approach with all baselines in Table 2, it is\n",
      "obvious that our approach consistently performs\n",
      "better than them on four long-context benchmarks\n",
      "and demonstrates superior performance in multi-\n",
      "hop long-context tasks. In our approach, benefiting\n",
      "from the graph’s ability to capture the relationships\n",
      "between detailed information, our method can iden-\n",
      "tify crucial information and search for the support-\n",
      "ing facts for the input question efficiently. This\n",
      "strategy significantly boosts the agent’s capability\n",
      "in multi-hop reasoning and capturing long-range\n",
      "dependencies of key information in a long context.\n",
      "Moreover, the results in Table 2 show that ReadA-MethodInput HotpotQA 2WikiMultihopQA MuSiQue NarrativeQA\n",
      "Window LR-1 LR-2 EM F1LR-1 LR-2 EM F1LR-1 LR-2 EM F1LR-1 LR-2 EM F1\n",
      "BM25 (top-1) 4k 57.7 63.0 33.7 43.8 36.0 39.0 25.0 30.4 33.0 36.5 19.0 23.9 29.5 34.5 4.0 11.3\n",
      "BM25 (top-3) 4k 74.7 78.3 45.7 58.5 59.7 62.0 42.3 51.9 43.5 49.5 25.0 31.1 44.5 52.5 7.0 20.5\n",
      "Ada-002 (top-1) 4k 63.0 70.7 40.0 53.2 57.0 59.3 41.0 49.4 34.5 37.0 20.0 26.6 37.5 46.5 5.0 15.5\n",
      "Ada-002 (top-3) 4k 72.0 77.3 45.0 58.1 65.7 66.7 44.7 55.3 40.0 45.5 24.5 32.1 45.5 53.0 7.5 19.5\n",
      "GPT-4-128k 128k 83.3 88.3 53.0 68.4 77.3 80.0 58.7 70.0 52.0 59.5 33.5 42.7 63.5 77.0 11.5 29.4\n",
      "GPT-4-128k (chunk) 4k 71.3 74.7 45.7 59.5 59.3 62.3 40.7 50.5 41.0 43.0 23.0 32.1 58.0 69.5 9.50 25.5\n",
      "GPT-4-128k (chunk w/ notes) 4k 72.3 76.7 45.7 59.5 65.7 68.7 46.3 56.6 39.5 43.0 25.0 32.5 56.5 65.0 8.5 24.3\n",
      "ReadAgent 128k 72.3 78.7 48.0 62.0 79.0 81.0 52.7 63.7 54.5 61.0 35.0 45.1 63.0 75.5 5.0 18.9\n",
      "GraphReader 4k 84.3 89.7 55.0 70.0 83.7 87.0 59.3 70.1 59.0 63.5 38.0 47.4 65.0 80.0 15.5 29.8\n",
      "Golden 4k 92.3 93.7 57.0 73.8 88.3 89.7 63.0 73.4 66.0 69.0 45.0 56.0 - - - -\n",
      "Table 2: Performance (%) comparison of different baselines on datasets from LongBench. The best performance\n",
      "and the second-best performance are denoted in bold and underlined fonts, respectively. “Golden” denotes the\n",
      "settings in which we add question and its supporting facts to LLM directly.\n",
      "MethodInputHotpotWikiQA-mixup\n",
      "Window16k 32k 64k 128k 256k\n",
      "LR-1 LR-2 F1*LR-1 LR-2 F1*LR-1 LR-2 F1*LR-1 LR-2 F1*LR-1 LR-2 F1*\n",
      "BM25 (top-1) 4k 10.0 16.0 12.0 16.0 18.0 11.9 6.0 8.0 8.5 10.0 8.0 7.0 14.0 20.0 5.9\n",
      "BM25 (top-3) 4k 16.0 22.0 13.9 18.0 28.0 13.3 16.0 18.0 11.8 12.0 16.0 11.8 12.0 22.0 9.3\n",
      "Ada-002 (top-1) 4k 10.0 12.0 14.5 14.0 18.0 11.3 10.0 12.0 12.5 12.0 14.0 9.4 8.0 8.0 7.0\n",
      "Ada-002 (top-3) 4k 24.0 28.0 21.3 20.0 30.0 19.8 14.0 20.0 12.9 16.0 20.0 12.0 14.0 18.0 10.8\n",
      "GPT-4-128k 128k 38.0 38.0 35.7 26.0 30.0 26.0 22.0 24.0 20.6 16.0 16.0 14.6 14.0 16.0 10.3\n",
      "GPT-4-128k (chunk) 4k 18.0 22.0 24.6 16.0 20.0 17.7 20.0 24.0 17.0 20.0 24.0 14.7 28.0 30.0 10.7\n",
      "GPT-4-128k (chunk w/ notes) 4k 22.0 32.0 24.2 26.0 30.0 21.3 28.0 32.0 22.0 24.0 26.0 17.4 26.0 26.0 14.8\n",
      "ReadAgent 128k 24.0 26.0 29.2 20.0 22.0 16.9 24.0 30.0 15.3 14.0 18.0 13.6 20.0 22.0 10.4\n",
      "GraphReader 4k 42.0 42.0 38.2 32.0 38.0 36.4 30.0 36.0 32.9 28.0 34.0 30.6 30.0 38.0 33.0\n",
      "Table 3: Performance (%) of different baselines on datasets from LV-Eval, where F1* donates LV-Eval’s optimized\n",
      "F1. The best performance and the second-best performance are denoted in bold and underlined fonts, respectively.\n",
      "We truncate to keep the longest possible initial fragment while preserving paragraph structure, in contexts that\n",
      "exceed the input window (128k and 256k) for GPT-4-128k.\n",
      "gent, with a 128k context window setup, under-\n",
      "performs GraphReader with a 4k context window\n",
      "and even performs worse than GPT-4-128k full-text\n",
      "reading. We attribute this to ReadAgent’s strategy\n",
      "of excessively compressing the original texts into\n",
      "gist memories, and feeding all mixed memories to\n",
      "the model for page number selection. Compared to\n",
      "our GraphReader, the strategy of ReadAgent may\n",
      "restrict the agent’s ability to identify specific de-\n",
      "tails and capture intrinsic connections among key\n",
      "elements in a long context, consequently affect-\n",
      "ing its overall performance. This further indicates\n",
      "that our approach can more efficiently unlock the\n",
      "capabilities of constrained context window LLMs\n",
      "in processing long context. Additionally, we ob-\n",
      "serve that the performance of our method closely\n",
      "matches that achieved by directly supplying sup-\n",
      "porting facts to the LLM ( i.e.,Golden in Table 2).\n",
      "This is because our method incorporates not only\n",
      "pre-planning, reflection, and various actions but\n",
      "also the usage of a graph containing key informa-tion, facilitating the agent to search for the correct\n",
      "supporting facts.\n",
      "Evaluation on Extremely Long Context Tasks\n",
      "As shown in previous experiments, it demonstrates\n",
      "the effectiveness of employing a limited context\n",
      "window LLM for long-context tasks with our\n",
      "GraphReader. Here, we would like to study the im-\n",
      "pact of extremely long context on our GraphReader.\n",
      "As shown in Table 3, compared with all baselines,\n",
      "our GraphReader not only consistently outperforms\n",
      "these methods across text lengths ranging from 16k\n",
      "to 256k tokens but also exhibits robustness with\n",
      "the expansion of context length. It indicates that\n",
      "our method is still effective in handling extremely\n",
      "long texts by graph exploration with limited context\n",
      "window LLMs. With the increase in the length of\n",
      "the input context, the performance of GPT-4-128k\n",
      "full-text reading degrades gradually. As a com-\n",
      "parison, our method achieves a performance gain\n",
      "of 10.53% relatively on LR-1 over GPT-4-128k\n",
      "full-text reading under 16k context length. WithDataset MethodResults(%)\n",
      "LR-1 LR-2 F1\n",
      "HotpotQAGraphReader 84.3 89.7 70.0\n",
      "w/o Rational Plan 81.7 87.7 63.8\n",
      "w/o Node Selection 66.0 71.7 54.1\n",
      "2WikiMultihopQAGraphReader 83.7 87.0 70.1\n",
      "w/o Rational Plan 81.3 86.0 65.4\n",
      "w/o Node Selection 65.3 68.7 49.7\n",
      "MuSiQueGraphReader 59.0 63.5 47.4\n",
      "w/o Rational Plan 56.0 61.0 42.4\n",
      "w/o Node Selection 35.0 38.5 25.2\n",
      "NarrativeQAGraphReader 65.0 80.0 29.8\n",
      "w/o Rational Plan 63.0 78.5 26.6\n",
      "w/o Node Selection 53.0 65.5 24.0\n",
      "Table 4: The results of our ablation study. “w/o Rational\n",
      "Plan” refers to removing the rational plan in the agent\n",
      "initialization stage, and “w/o Node Selection” denotes\n",
      "applying the random selection of initial nodes and neigh-\n",
      "bor nodes in graph exploration.\n",
      "the context length increasing to 128k, our method\n",
      "achieves a performance gain of 75.00% relatively\n",
      "over GPT-4-128k. This can be attributed to the fact\n",
      "that as the context length increases, the impact of\n",
      "the “lost in the middle” effect on GPT-4-128k be-\n",
      "comes progressively more severe. Secondly, we ob-\n",
      "serve that ReadAgent significantly underperforms\n",
      "our method in handling extremely long contexts.\n",
      "This is because the lack of detailed information\n",
      "about the content of each page can make page selec-\n",
      "tion very difficult for ReadAgent, especially when\n",
      "dealing with extremely long contexts. This further\n",
      "demonstrates that our method can effectively ad-\n",
      "dress the challenges of processing extremely long\n",
      "context with limited context window LLMs by ex-\n",
      "ploring graphs containing fine-grained information.\n",
      "4.3 Ablation study\n",
      "The Effect of Rational Plan In the graph explo-\n",
      "ration stage, we introduce a rational plan to help\n",
      "the agent analyze complex input questions step by\n",
      "step, guiding the agent in exploring the graph. To\n",
      "verify the effectiveness of the rational plan, we re-\n",
      "moved it during agent initialization and conducted\n",
      "experiments on four long-context QA benchmarks.\n",
      "Table 4 shows that the rational plan is effective in\n",
      "guiding the agent in node selection and exploration\n",
      "on the graph.\n",
      "The Effect of Node Selection We conduct ran-\n",
      "domly selecting initial nodes and neighbor nodes\n",
      "experiments to demonstrate the necessity of our sys-\n",
      "tem in selecting which nodes to visit based on rea-\n",
      "soning about the required information. As shown\n",
      "in Table 4, random selection results in a significant\n",
      "1 3 5 7707580859095100Average Scores (%)\n",
      "(a) 2WikiMultihopQA\n",
      "LR-1\n",
      "LR-2\n",
      "1 3 5 7406080100\n",
      "(b) NarrativeQA\n",
      "LR-1\n",
      "LR-2\n",
      "Initial Node Numbers Initial Node NumbersFigure 3: Performance of GraphReader with different\n",
      "initial node numbers on 2WikiMultihopQA and Narra-\n",
      "tiveQA. Results show the robustness of GraphReader\n",
      "towards different initial node numbers.\n",
      "performance drop, with an average decline of 18%.\n",
      "This demonstrates that GraphReader carefully con-\n",
      "siders node selection, leading to more reasonable\n",
      "and effective exploration.\n",
      "Impact of the Number of Initial Nodes We con-\n",
      "duct experiments with different initial node counts\n",
      "on multi-hop and single-hop QA datasets to as-\n",
      "sess the effect of the number of initial nodes on\n",
      "GraphReader’s performance. The results are shown\n",
      "in Figure 3. Increasing the number of nodes im-\n",
      "proves performance up to a certain point, with opti-\n",
      "mal performance at 5 initial nodes, which we set as\n",
      "the default. However, beyond this threshold, perfor-\n",
      "mance declines, especially in single-hop scenarios,\n",
      "likely due to increased noise from too many initial\n",
      "nodes.\n",
      "Impact of the Chunk Size We investigate the\n",
      "impact of chunk size Lon GraphReader’s perfor-\n",
      "mance. As shown in Figure 4, the best performance\n",
      "is achieved with L= 2k. When Lexceeds a cer-\n",
      "tain threshold, performance declines because larger\n",
      "chunks cause the model to overlook essential de-\n",
      "tails. Conversely, smaller chunks lead to more\n",
      "semantic truncation, hindering comprehension and\n",
      "accuracy in extracting atomic facts. Thus, we chose\n",
      "L= 2kas the default chunk size.\n",
      "4.4 Further Analysis\n",
      "Cost Analysis To evaluate the inference cost of\n",
      "our approach, we compare the average token con-\n",
      "sumption of ReadAgent and GraphReader for each\n",
      "question. As shown in Table 5, GraphReader uses\n",
      "only 1.08 times more tokens than ReadAgent but\n",
      "achieves over twice the performance improvement,\n",
      "demonstrating its superiority. Additionally, after\n",
      "the first round of document preprocessing, token\n",
      "consumption in subsequent exploration is signifi-1k 2k 4k 6k\n",
      "Chunk Size152025303540Average Scores (%)\n",
      "LR-1\n",
      "LR-2Figure 4: The impact of chunk size Lof GraphReader\n",
      "on the 256k length level of HotpotWikiQA-mixup.\n",
      "Method Avg. Ctx. #Tokens Avg. Cost #Tokens\n",
      "ReadAgent 358.3k 48.7k\n",
      "GraphReader 358.3k 52.8k\n",
      "Table 5: Comparison of token consumption per\n",
      "question between ReadAgent and GraphReader on\n",
      "HotpotWikiQA-mixup-256k, where “Avg. Ctx. #To-\n",
      "kens” refers to the average token number of the original\n",
      "dataset. The “Avg. Cost #Tokens” comprise both input\n",
      "tokens and output tokens during exploration.\n",
      "16k 32k 64k 128k 256k\n",
      "Input Length010203040506070Recall Scores (%)\n",
      "GraphReader\n",
      "ReadAgent\n",
      "GPT-4-128k (chunk w/ notes)Ada-002 (top-1)\n",
      "BM25 (top-1)\n",
      "Figure 5: Recall of supporting facts by different meth-\n",
      "ods on HotpotWikiQA-mixup.\n",
      "cantly reduced and much lower than the original\n",
      "dataset’s token count.\n",
      "Recall Rate Analysis To evaluate our method’s\n",
      "advantages in key information recall, we utilize\n",
      "GPT-4 to assess the recall of supporting facts on\n",
      "the HotpotWikiQA-mixup dataset. As shown in\n",
      "Figure 5, our model consistently outperforms other\n",
      "baseline methods, regardless of the input length.SourceRecall(%)\n",
      "SF-wise Sample-wise\n",
      "Atomic Facts 76.4 64.7\n",
      "Final Notebook 90.5 85.3\n",
      "Table 6: GraphReader’s recall performance at different\n",
      "granularities on HotpotQA. “SF-wise” refers to the gran-\n",
      "ularity of supporting facts, and “Sample-wise” refers to\n",
      "the granularity of sample evaluation.\n",
      "As context length increases from 16k to 256k, re-\n",
      "call of supporting facts declines across all meth-\n",
      "ods. However, GraphReader maintains around 60%\n",
      "recall at 256k context length, in contrast to the\n",
      "significant degradation in ReadAgent. This demon-\n",
      "strates GraphReader’s scalability and effectiveness\n",
      "in processing long contexts. Further details and\n",
      "evaluation prompts can be found in Appendix E.\n",
      "To further demonstrate the recall rate of\n",
      "GraphReader at different granularities, we calcu-\n",
      "late the recall rate of Supporting Facts andSample\n",
      "granularity respectively using the same method,\n",
      "detailed in the Appendix E. The granularity of sup-\n",
      "porting facts refers to the recall rate of all support-\n",
      "ing facts across the entire dataset. As for sample\n",
      "granularity, a sample is considered to be recalled\n",
      "only if all of its supporting facts are recalled. As\n",
      "shown in the Tabel 6, the recall for the final note-\n",
      "book is slightly higher than the recall of atomic\n",
      "facts, which indicates that our method is capable\n",
      "of extracting more valid information from chunks\n",
      "during the exploration, indirectly reflecting its in-\n",
      "telligence and effectiveness in exploration.\n",
      "5 Conclusion\n",
      "This paper introduces GraphReader, a graph-based\n",
      "agent designed to enhance the long-context capa-\n",
      "bilities of large language models. GraphReader\n",
      "organizes long texts into graph structures and em-\n",
      "ploys an autonomous agent to explore the graph,\n",
      "successfully establishing long-range dependencies\n",
      "within a relatively small 4k context window. Exper-\n",
      "iments demonstrate that GraphReader outperforms\n",
      "GPT-4 with a 128k input length across various\n",
      "long-context single-hop and multi-hop question-\n",
      "answering benchmarks.\n",
      "6 Limitations\n",
      "Firstly, GraphReader is constructed using an off-\n",
      "the-shelf GPT-4 API. Since it is close-sourced,\n",
      "there may be potential restrictions such as limits onQueries Per Second (QPS) and regional constraints.\n",
      "Therefore, future work will involve collecting data,\n",
      "training models, and making them open-source to\n",
      "contribute to the wider community. Secondly, the\n",
      "efficiency of the agent depends on its planning and\n",
      "reasoning capabilities. Future research will also\n",
      "explore enhancements of these features to improve\n",
      "the effectiveness of our method.\n",
      "References\n",
      "Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jia-\n",
      "heng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su,\n",
      "Tiezheng Ge, Bo Zheng, et al. 2024a. Mt-bench-101:\n",
      "A fine-grained benchmark for evaluating large lan-\n",
      "guage models in multi-turn dialogues. arXiv preprint\n",
      "arXiv:2402.14762 .\n",
      "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou,\n",
      "Jie Tang, Yuxiao Dong, and Juanzi Li. 2024b. Lon-\n",
      "galign: A recipe for long context alignment of large\n",
      "language models. arXiv preprint arXiv:2401.18058 .\n",
      "Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai\n",
      "Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-\n",
      "han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and\n",
      "Juanzi Li. 2023. Longbench: A bilingual, multitask\n",
      "benchmark for long context understanding. ArXiv ,\n",
      "abs/2308.14508.\n",
      "Howard Chen, Ramakanth Pasunuru, Jason Weston, and\n",
      "Asli Celikyilmaz. 2023a. Walking down the mem-\n",
      "ory maze: Beyond context limit through interactive\n",
      "reading. arXiv preprint arXiv:2310.05029 .\n",
      "Shouyuan Chen, Sherman Wong, Liangjian Chen, and\n",
      "Yuandong Tian. 2023b. Extending context window\n",
      "of large language models via positional interpolation.\n",
      "arXiv preprint arXiv:2306.15595 .\n",
      "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,\n",
      "Zhijian Liu, Song Han, and Jiaya Jia. 2023c. Lon-\n",
      "glora: Efficient fine-tuning of long-context large lan-\n",
      "guage models. arXiv preprint arXiv:2309.12307 .\n",
      "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\n",
      "bonell, Quoc V Le, and Ruslan Salakhutdinov.\n",
      "2019. Transformer-xl: Attentive language mod-\n",
      "els beyond a fixed-length context. arXiv preprint\n",
      "arXiv:1901.02860 .\n",
      "Michiel De Jong, Yury Zemlyanskiy, Nicholas FitzGer-\n",
      "ald, Fei Sha, and William Cohen. 2021. Mention\n",
      "memory: incorporating textual knowledge into trans-\n",
      "formers through entity mention attention. arXiv\n",
      "preprint arXiv:2110.06176 .\n",
      "Yiran Ding, Li Lyna Zhang, Chengruidong Zhang,\n",
      "Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,\n",
      "and Mao Yang. 2024. Longrope: Extending llm con-\n",
      "text window beyond 2 million tokens. arXiv preprint\n",
      "arXiv:2402.13753 .Thibault Févry, Livio Baldini Soares, Nicholas FitzGer-\n",
      "ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\n",
      "tities as experts: Sparse memory access with entity\n",
      "supervision. arXiv preprint arXiv:2004.07202 .\n",
      "Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Han-\n",
      "naneh Hajishirzi, Yoon Kim, and Hao Peng. 2024.\n",
      "Data engineering for scaling language models to 128k\n",
      "context. arXiv preprint arXiv:2402.10171 .\n",
      "Albert Gu and Tri Dao. 2023. Mamba: Linear-time\n",
      "sequence modeling with selective state spaces. arXiv\n",
      "preprint arXiv:2312.00752 .\n",
      "Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\n",
      "and Akiko Aizawa. 2020. Constructing A multi-hop\n",
      "QA dataset for comprehensive evaluation of reason-\n",
      "ing steps. In COLING , pages 6609–6625. Interna-\n",
      "tional Committee on Computational Linguistics.\n",
      "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n",
      "Zettlemoyer, and Mike Lewis. 2019. Generalization\n",
      "through memorization: Nearest neighbor language\n",
      "models. arXiv preprint arXiv:1911.00172 .\n",
      "Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-\n",
      "cient and effective passage search via contextualized\n",
      "late interaction over bert. In Proceedings of the 43rd\n",
      "International ACM SIGIR conference on research\n",
      "and development in Information Retrieval , pages 39–\n",
      "48.\n",
      "Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris\n",
      "Dyer, Karl Moritz Hermann, Gábor Melis, and Ed-\n",
      "ward Grefenstette. 2018. The narrativeqa reading\n",
      "comprehension challenge. Trans. Assoc. Comput.\n",
      "Linguistics , 6:317–328.\n",
      "LangChain-team. 2024. LangChain.\n",
      "Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John\n",
      "Canny, and Ian Fischer. 2024. A human-inspired\n",
      "reading agent with gist memory of very long contexts.\n",
      "arXiv preprint arXiv:2402.09727 .\n",
      "Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng\n",
      "Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing.\n",
      "2023. Chain-of-knowledge: Grounding large lan-\n",
      "guage models via dynamic knowledge adapting over\n",
      "heterogeneous sources. In The Twelfth International\n",
      "Conference on Learning Representations .\n",
      "Jerry Liu. 2024. LlamaIndex.\n",
      "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\n",
      "jape, Michele Bevilacqua, Fabio Petroni, and Percy\n",
      "Liang. 2024. Lost in the middle: How language mod-\n",
      "els use long contexts. Transactions of the Association\n",
      "for Computational Linguistics , 12:157–173.\n",
      "Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-\n",
      "yang Lin, Chuanqi Tan, Chang Zhou, and Jingren\n",
      "Zhou. 2023. # instag: Instruction tagging for analyz-\n",
      "ing supervised fine-tuning of large language models.\n",
      "InThe Twelfth International Conference on Learning\n",
      "Representations .Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and\n",
      "Shirui Pan. 2023. Reasoning on graphs: Faithful and\n",
      "interpretable large language model reasoning. arXiv\n",
      "preprint arXiv:2310.01061 .\n",
      "Tsendsuren Munkhdalai, Manaal Faruqui, and Sid-\n",
      "dharth Gopal. 2024. Leave no context behind:\n",
      "Efficient infinite context transformers with infini-\n",
      "attention. arXiv preprint arXiv:2404.07143 .\n",
      "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\n",
      "Long Ouyang, Christina Kim, Christopher Hesse,\n",
      "Shantanu Jain, Vineet Kosaraju, William Saunders,\n",
      "et al. 2021. Webgpt: Browser-assisted question-\n",
      "answering with human feedback. arXiv preprint\n",
      "arXiv:2112.09332 .\n",
      "Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\n",
      "ith Ringel Morris, Percy Liang, and Michael S Bern-\n",
      "stein. 2023. Generative agents: Interactive simulacra\n",
      "of human behavior. In Proceedings of the 36th An-\n",
      "nual ACM Symposium on User Interface Software\n",
      "and Technology , pages 1–22.\n",
      "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\n",
      "rico Shippole. 2023. Yarn: Efficient context window\n",
      "extension of large language models. arXiv preprint\n",
      "arXiv:2309.00071 .\n",
      "Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015.\n",
      "Yara parser: A fast and accurate dependency parser.\n",
      "Computing Research Repository , arXiv:1503.06733.\n",
      "Version 2.\n",
      "Stephen E. Robertson and Hugo Zaragoza. 2009. The\n",
      "probabilistic relevance framework: Bm25 and be-\n",
      "yond. Found. Trends Inf. Retr. , 3:333–389.\n",
      "Devendra Singh Sachan, Mike Lewis, Dani Yogatama,\n",
      "Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.\n",
      "2023. Questions are all you need to train a dense\n",
      "passage retriever. Transactions of the Association for\n",
      "Computational Linguistics , 11:600–616.\n",
      "Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh\n",
      "Khanna, Anna Goldie, and Christopher D Man-\n",
      "ning. 2024. Raptor: Recursive abstractive pro-\n",
      "cessing for tree-organized retrieval. arXiv preprint\n",
      "arXiv:2401.18059 .\n",
      "Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\n",
      "Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,\n",
      "and Jian Guo. 2023. Think-on-graph: Deep and\n",
      "responsible reasoning of large language model with\n",
      "knowledge graph. arXiv preprint arXiv:2307.07697 .\n",
      "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-\n",
      "Micke, and Mohit Iyyer. 2021. Do long-range lan-\n",
      "guage models actually use long-range context? arXiv\n",
      "preprint arXiv:2109.09115 .\n",
      "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\n",
      "and Ashish Sabharwal. 2022. Musique: Multi-\n",
      "hop questions via single-hop question composition.\n",
      "Trans. Assoc. Comput. Linguistics , 10:539–554.Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi\n",
      "Zhang, and Tyler Derr. 2024. Knowledge graph\n",
      "prompting for multi-document question answering.\n",
      "InProceedings of the AAAI Conference on Artificial\n",
      "Intelligence , volume 38, pages 19206–19214.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\n",
      "et al. 2022. Chain-of-thought prompting elicits rea-\n",
      "soning in large language models. Advances in neural\n",
      "information processing systems , 35:24824–24837.\n",
      "Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei\n",
      "Zhu, and Sujian Li. 2024a. Long context align-\n",
      "ment with short instructions and synthesized posi-\n",
      "tions. arXiv preprint arXiv:2405.03939 .\n",
      "Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhan-\n",
      "hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi\n",
      "Bai, Haibin Chen, Tiezheng Ge, et al. 2024b. Con-\n",
      "ceptmath: A bilingual concept-wise benchmark for\n",
      "measuring mathematical reasoning of large language\n",
      "models. arXiv preprint arXiv:2402.14660 .\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\n",
      "gio, William W. Cohen, Ruslan Salakhutdinov, and\n",
      "Christopher D. Manning. 2018. Hotpotqa: A dataset\n",
      "for diverse, explainable multi-hop question answer-\n",
      "ing. In EMNLP , pages 2369–2380. Association for\n",
      "Computational Linguistics.\n",
      "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\n",
      "Shafran, Karthik Narasimhan, and Yuan Cao. 2022.\n",
      "React: Synergizing reasoning and acting in language\n",
      "models. arXiv preprint arXiv:2210.03629 .\n",
      "Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang,\n",
      "Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu\n",
      "Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen\n",
      "Yan, and Yu Wang. 2024. Lv-eval: A balanced long-\n",
      "context benchmark with 5 length levels up to 256k.\n",
      "ArXiv , abs/2402.05136.\n",
      "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\n",
      "Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\n",
      "Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\n",
      "Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\n",
      "Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\n",
      "Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\n",
      "survey of large language models. abs/2303.18223.\n",
      "Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-\n",
      "hao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-\n",
      "cient context window extension of llms via positional\n",
      "skip-wise training. arXiv preprint arXiv:2309.10400 .A GraphReader Prompt\n",
      "Figure 6 illustrates the prompt used for Graph Con-\n",
      "struction. Figures 7 to 11 present the prompts em-\n",
      "ployed for Graph Exploration. Figure 12 shows the\n",
      "prompt used for Answer Reasoning.\n",
      "B LLM Rater Evaluation Details\n",
      "Given a question, a golden answer, and an answer\n",
      "to be evaluated, we utilize an LLM to assess the\n",
      "accuracy of the latter based on the question and\n",
      "correct answer. This involves two scores: LLM-\n",
      "Rating-1 (LR-1) and LLM-Rating-2 (LR-2), where\n",
      "LR-1 represents a strict scoring criterion, and LR-2\n",
      "is a more lenient one. Following the approach of\n",
      "ReadAgent, if either LLM Rater deems an answer\n",
      "correct, it is considered as such. If the strict scorer\n",
      "finds an answer incorrect while the lenient scorer\n",
      "deems it partially correct, we classify the answer as\n",
      "partially correct; otherwise, it is adjudged incorrect.\n",
      "The prompts used for evaluation are presented in\n",
      "Figure 13 and Figure 14 respectively.\n",
      "For the evaluation, we utilize GPT-4-128k as the\n",
      "LLM Rater, with the temperature set to 0.1.\n",
      "C Dataset\n",
      "Multi-hop QA Datasets HotpotQA features a\n",
      "collection of 2-hop questions directly authored by\n",
      "native speakers, based on two interconnected para-\n",
      "graphs. 2WikiMultihopQA is comprised of com-\n",
      "plex questions up to 5-hops in length, constructed\n",
      "through carefully designed templates to prevent the\n",
      "possibility of shortcut solutions.\n",
      "In the MuSiQue dataset, questions are intricately\n",
      "crafted starting from straightforward scenarios that\n",
      "require up to 4-hops reasoning. Annotators sub-\n",
      "sequently rephrase these with a dual purpose: to\n",
      "avoid shortcut answers and to maintain a natural\n",
      "linguistic quality. Each question within the orig-\n",
      "inal datasets is complemented by 2-4 supporting\n",
      "paragraphs, delivering evidence for simple one-step\n",
      "reasoning, alongside multiple paragraphs designed\n",
      "to serve as decoys.\n",
      "HotpotWikiQA-mixup originates from LV-Eval\n",
      "and employs a construction method known as a\n",
      "mixup. This method randomly blends support doc-\n",
      "uments with various distracting documents to gen-\n",
      "erate five different context lengths for a given QA\n",
      "pair, including 16k, 32k, 64k, 128k, and 256k. Due\n",
      "to the excessive length of this dataset, we select\n",
      "the first 50 data entries from each different context\n",
      "length for experimentation to control costs.Single-hop QA Datasets NarrativeQA is a\n",
      "dataset designed to test comprehension abilities\n",
      "for long documents, primarily sourced from movie\n",
      "scripts. As a single-hop QA dataset, the informa-\n",
      "tion required to answer its questions appears at a\n",
      "single location within the text.\n",
      "D Baseline Methods\n",
      "Full or Chunked Text Content For texts with\n",
      "fewer tokens than the LLM’s input window, we\n",
      "can input the text directly into the LLM to obtain\n",
      "an answer. We refer to this method as Full Text\n",
      "Read , with the specific prompt provided in Figure\n",
      "15. However, this approach is not applicable to\n",
      "texts exceeding the token limit of the LLM’s input\n",
      "window. In such cases, Lee et al. truncated the\n",
      "text to fit it into the LLM, but this method obvi-\n",
      "ously results in information loss. We propose a\n",
      "method that does not lose information, offering a\n",
      "better comparison. This method involves dividing\n",
      "the entire text into chunks (using the same chunk-\n",
      "ing method as GraphReader) and then having the\n",
      "LLM read these chunks sequentially according to\n",
      "the text order, thus enabling the handling of overly\n",
      "long texts with a limited input window. During\n",
      "the reading process, there are two main strategies:\n",
      "Chunk Read andChunk Read with Notes . In the\n",
      "Chunk Read approach, the LLM only sees the cur-\n",
      "rent chunk during each reading, which is suitable\n",
      "for single-hop QA tasks. In the Chunk Read with\n",
      "Notes approach, the LLM can summarize useful\n",
      "information from the current chunk and provide it\n",
      "to the subsequent reading process, which is suitable\n",
      "for multi-hop QA tasks.\n",
      "In the experiment, we divide the chunks in the\n",
      "same way as GraphReader, and the maximum\n",
      "length of the chunk is set to 2k. The specific\n",
      "prompts are in Figure 16 and 17 respectively.\n",
      "Retrieval-Augmented Generation (RAG) RAG\n",
      "is a commonly used approach for addressing long-\n",
      "text problems. In this work, we compare the tra-\n",
      "ditional RAG method, including retrieval methods\n",
      "based on Okapi BM25 (Robertson and Zaragoza,\n",
      "2009) and the OpenAI API embedding model\n",
      "(text-embedding-ada-002). Specifically, we first\n",
      "split the text into chunks in the same method as\n",
      "GraphReader, then use the aforementioned meth-\n",
      "ods to calculate the relevance scores between the\n",
      "question and these chunks, and finally input the top-\n",
      "nchunks with the highest relevance scores together\n",
      "with the question for the LLM to answer.To ensure a fair comparison, we control the input\n",
      "window to 4k in the experiments. Specifically, in\n",
      "order to fill the input window as much as possi-\n",
      "ble, we set the maximum length of the chunk to\n",
      "38k when selecting the top-1 chunk for answering;\n",
      "when opting for the top-3 chunks, we set the max-\n",
      "imum length of each chunk to 1k. The specific\n",
      "prompt can be found in Figure 18.\n",
      "ReadAgent We also compared our method with\n",
      "similar approaches for handling long texts with\n",
      "small input windows, such as ReadAgent (Lee\n",
      "et al., 2024). ReadAgent is a method that segments\n",
      "long texts and generates gist memories, which are\n",
      "then looked up to search for information in order to\n",
      "answer questions. In the experiments, for datasets\n",
      "from LongBench, we adopted the default hyperpa-\n",
      "rameters declared in the ReadAgent paper, specifi-\n",
      "cally a max_words of 600 and min_words of 280\n",
      "when splitting pages. For HotpotWikiQA-mixup\n",
      "from LV-Eval, we scaled these two hyperparam-\n",
      "eters using the same approach as in the ReadA-\n",
      "gent paper. Specifically, for datasets with lengths\n",
      "of 256k and 128k, we used max_words=10000\n",
      "and min_words=2000; for those with lengths of\n",
      "64k, 32k and 16k, we used max_words=5000 and\n",
      "min_words=1000. At the same time, we employed\n",
      "the ReadAgent-S method, which ReadAgent claims\n",
      "to be the most effective, reading the pages in se-\n",
      "quence. Additionally, we allowed reading up to 5\n",
      "pages (Look up 1-5 pages).\n",
      "EEvaluation Recall for Supporting Facts\n",
      "We evaluate the recall rate of supporting facts for\n",
      "different methods using GPT-4-128k, with the tem-\n",
      "perature set to 0.1. Figure 19 shows the specific\n",
      "evaluation prompt.\n",
      "For GraphReader, we evaluate the memory\n",
      "recorded in the final notebook. For ReadAgent,\n",
      "the evaluation focused on the final text segments\n",
      "reviewed. In the case of Chunk Read with Notes,\n",
      "we evaluate both the memory and the chunk read at\n",
      "the time of the final answer; for the RAG methods,\n",
      "we assess the retrieved chunks.\n",
      "F The Analysis of Function Calls\n",
      "To verify the rationality and utility of agent actions\n",
      "under various circumstances of GraphReader, we\n",
      "made statistics on its function calls at each stage\n",
      "across two datasets. From the statistical results in\n",
      "Table 7, it can be observed that each piece of datawill perform an average of 3 to 4 actions, corre-\n",
      "sponding to the average number of function calls\n",
      "in the table. This indicates the effectiveness of\n",
      "the graph we constructed, with GraphReader being\n",
      "able to swiftly locate key information while mini-\n",
      "mizing resource usage. Furthermore, each action\n",
      "has a certain probability of being chosen, justifying\n",
      "the rationality of the action set. Among them, the\n",
      "most commonly used action on multi-hop QA tasks\n",
      "is to read neighbor nodes, and the most common\n",
      "action on single-hop QA tasks is to read chunks.\n",
      "This difference is caused by the fact that multi-hop\n",
      "questions need to gather information contained by\n",
      "multiple nodes to answer questions, while single-\n",
      "hop data sets often require only one atomic fact.\n",
      "G Statistics of Graph\n",
      "The statistics of graphs from various datasets are\n",
      "presented in Table 8. For longer texts, there tends\n",
      "to be a higher average number of nodes and atomic\n",
      "facts. After normalization, each node has an aver-\n",
      "age of about 10 neighbor nodes. This is because\n",
      "the number of key elements occurring simultane-\n",
      "ously in each atomic fact is generally of this mag-\n",
      "nitude. Furthermore, the aggregation of similar\n",
      "nodes caused by normalization results in a slight\n",
      "increase in the number of neighboring nodes.\n",
      "On average, each node is associated with about\n",
      "2 atomic facts, and the average number of atomic\n",
      "facts in the node with the most atomic facts in\n",
      "each graph ranges from 15 to 50, indicating a rela-\n",
      "tively even distribution of atomic facts. The max-\n",
      "imum average number of atomic facts is found\n",
      "in NarrativeQA, a possible explanation being that\n",
      "NarrativeQA is mainly derived from movie scripts,\n",
      "where characters, such as the protagonist, appear\n",
      "frequently throughout the text, thus including a\n",
      "larger number of atomic facts.\n",
      "H GraphReader Example\n",
      "This section presents a case study of the\n",
      "GraphReader workflow. Figure 20 displays the\n",
      "posed question alongside the answer and pertinent\n",
      "supporting passages. Subsequently, Figure 21 de-\n",
      "lineates the methodology for constructing the graph.\n",
      "Figure 22 further elaborates on the initialization of\n",
      "a pre-planned rational path by GraphReader and\n",
      "the selection of initial nodes. Figure 23 illustrates\n",
      "the sequence of function invocations during the ex-\n",
      "ploration phase. Finally, Figure 24 showcases how\n",
      "GraphReader formulates the answer by leveraging\n",
      "the insights obtained through exploration.Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%)\n",
      "HotpotQA 3.0Exploring Atomic Facts 42.0read_chunk 46.5\n",
      "stop_and_read_neighbor 53.5\n",
      "Exploring Chunks 31.9search_more 12.1\n",
      "read_previous_chunk 21.1\n",
      "read_subsequent_chunk 22.9\n",
      "termination 43.9\n",
      "Exploring Neighbors 26.1read_neighbor_node 35.5\n",
      "termination 65.5\n",
      "2WikiMultihopQA 3.2Exploring Atomic Facts 40.4read_chunk 48.6\n",
      "stop_and_read_neighbor 51.4\n",
      "Exploring Chunks 34.5search_more 14.5\n",
      "read_previous_chunk 25.1\n",
      "read_subsequent_chunk 23.3\n",
      "termination 37.1\n",
      "Exploring Neighbors 25.1read_neighbor_node 37.3\n",
      "termination 62.7\n",
      "MuSiQue 3.5Exploring Atomic Facts 40.0read_chunk 41.3\n",
      "stop_and_read_neighbor 58.7\n",
      "Exploring Chunks 31.2search_more 19.1\n",
      "read_previous_chunk 26.6\n",
      "read_subsequent_chunk 25.7\n",
      "termination 28.6\n",
      "Exploring Neighbors 28.8read_neighbor_node 40.1\n",
      "termination 59.9\n",
      "NarrativeQA 3.9Exploring Atomic Facts 32.5read_chunk 64.5\n",
      "stop_and_read_neighbor 35.5\n",
      "Exploring Chunks 54.3search_more 4.1\n",
      "read_previous_chunk 35.3\n",
      "read_subsequent_chunk 32.6\n",
      "termination 28.0\n",
      "Exploring Neighbors 13.2read_neighbor_node 51.4\n",
      "termination 48.6\n",
      "Table 7: Statistics of function calls on MuSiQue and NarrativeQA.\n",
      "datasetSample Dimension Sample & Node Dimension\n",
      "node num atomic facts num neighbor node num atomic facts num\n",
      "avg. max avg. max avg. avg. avg. max avg. avg. avg. max\n",
      "HotpotQA 583.8 1945.0 244.0 645.0 10.1 263.1 2.1 17.8\n",
      "2WikiMultihopQA 515.8 1691.0 217.7 545.0 9.2 215.7 2.1 17.0\n",
      "MusiQue 1029.4 2142.0 419.9 586.0 9.3 253.4 2.1 15.6\n",
      "NarrativeQA 966.0 3110.0 515.5 1296.0 10.3 652.6 2.3 50.0\n",
      "HotpotWikiQA-mixup16k 1741.6 3822.0 749.7 1043.0 9.4 231.0 2.2 17.1\n",
      "32k 2827.3 5086.0 1257.4 1694.0 9.8 263.3 2.2 29.3\n",
      "64k 5054.1 8918.0 2360.0 3015.0 10.4 227.2 2.3 17.1\n",
      "128k 8828.5 14592.0 4437.9 5182.0 11.1 302.0 2.4 19.2\n",
      "256k 14853.3 24981.0 8632.8 9478.0 12.2 427.6 2.5 27.8\n",
      "Table 8: Graph statistical data. Under the Sample dimension, “avg.” indicates the average number of nodes in each\n",
      "graph, and “max” refers to the largest node count across all graphs. The same logic applies to atomic facts num. In\n",
      "the Sample & Node dimensions, “avg. avg.” denotes the average of the average neighbor node counts per graph,\n",
      "and“avg. max” means the average of the maximum neighbor node counts per graph. This approach is also used for\n",
      "counting atomic facts num.You are now an intelligent assistant tasked with meticulously extracting both key elements and\n",
      "atomic facts from a long text.\n",
      "1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g.,\n",
      "actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative.\n",
      "2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include\n",
      "propositions, theories, existences, concepts, and implicit elements like logic, causality, event\n",
      "sequences, interpersonal relationships, timelines, etc.\n",
      "Requirements:\n",
      "#####\n",
      "1. Ensure that all identified key elements are reflected within the corresponding atomic facts.\n",
      "2. You should extract key elements and atomic facts comprehensively, especially those that are\n",
      "important and potentially query-worthy and do not leave out details.\n",
      "3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He,\n",
      "She to actual names).\n",
      "4. Ensure that the key elements and atomic facts you extract are presented in the same language as\n",
      "the original text (e.g., English or Chinese).\n",
      "5. You should output a total of key elements and atomic facts that do not exceed 1024 tokens.\n",
      "6. Your answer format for each line should be: [Serial Number ],[Atomic Facts ],[List of Key\n",
      "Elements, separated with ‘|’ ]\n",
      "#####\n",
      "Example:\n",
      "#####\n",
      "User:\n",
      "One day, a father and his little son ......\n",
      "Assistant:\n",
      "1. One day, a father and his little son were going home. | father | little son | going home\n",
      "2. ......\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 6: The prompt for key elements and atomic facts extraction.As an intelligent assistant, your primary objective is to answer the question by gathering\n",
      "supporting facts from a given article. To facilitate this objective, the first step is to make\n",
      "a rational plan based on the question. This plan should outline the step-by-step process to\n",
      "resolve the question and specify the key information required to formulate a comprehensive answer.\n",
      "Example:\n",
      "#####\n",
      "User: Who had a longer tennis career, Danny or Alice?\n",
      "Assistant: In order to answer this question, we first need to find the length of Danny’s\n",
      "and Alice’s tennis careers, such as the start and retirement of their careers, and then compare the\n",
      "two.\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 7: The prompt for rational plan.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
      "comprising the following elements:\n",
      "1. Text Chunks: Chunks of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "Your current task is to check a list of nodes, with the objective of selecting the most rele-\n",
      "vant initial nodes from the graph to efficiently answer the question. You are given the question, the\n",
      "rational plan, and a list of node key elements. These initial nodes are crucial because they are the\n",
      "starting point for searching for relevant information.\n",
      "Requirements:\n",
      "#####\n",
      "1. Once you have selected a starting node, assess its relevance to the potential answer by assigning\n",
      "a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer,\n",
      "whereas a score of 0 suggests minimal relevance.\n",
      "2. Present each chosen starting node in a separate line, accompanied by its relevance score. Format\n",
      "each line as follows: Node: [Key Element of Node], Score: [Relevance Score].\n",
      "3. Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.\n",
      "4. In the user’s input, each line constitutes a node. When selecting the starting node, please make\n",
      "your choice from those provided, and refrain from fabricating your own. The nodes you output\n",
      "must correspond exactly to the nodes given by the user, with identical wording.\n",
      "#####\n",
      "Example:\n",
      "#####\n",
      "User:\n",
      "Question: {QUESTION}\n",
      "Plan: {RATIONAL PLAN}\n",
      "Nodes: {LIST OF KEY ELEMENTS}\n",
      "Assistant:{LIST OF SELECTED NODES}\n",
      "#####\n",
      "Finally, I emphasize again that you need to select the starting node from the given Nodes, and\n",
      "it must be consistent with the words of the node you selected. Please strictly follow the above\n",
      "format. Let’s begin.\n",
      "Figure 8: The prompt for initial node selection.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
      "comprising the following elements:\n",
      "1. Text Chunks: Chunks of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "Your current task is to check a node and its associated atomic facts, with the objective of\n",
      "determining whether to proceed with reviewing the text chunk corresponding to these atomic facts.\n",
      "Given the question, the rational plan, previous actions, notebook content, and the current node’s\n",
      "atomic facts and their corresponding chunk IDs, you have the following Action Options:\n",
      "#####\n",
      "1. read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic\n",
      "fact may hold the necessary information to answer the question. This will allow you to access\n",
      "more complete and detailed information.\n",
      "2. stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable\n",
      "information.\n",
      "#####\n",
      "Strategy:\n",
      "#####\n",
      "1. Reflect on previous actions and prevent redundant revisiting nodes or chunks.\n",
      "2. You can choose to read multiple text chunks at the same time.\n",
      "3. Atomic facts only cover part of the information in the text chunk, so even if you feel that the\n",
      "atomic facts are slightly relevant to the question, please try to read the text chunk to get more\n",
      "complete information.\n",
      "#####\n",
      "Response format:\n",
      "#####\n",
      "*Updated Notebook*: First, combine your current notebook with new insights and findings about\n",
      "the question from current atomic facts, creating a more complete version of the notebook that\n",
      "contains more valid information.\n",
      "*Rationale for Next Action*: Based on the given question, the rational plan, previous actions, and\n",
      "notebook content, analyze how to choose the next action.\n",
      "*Chosen Action*: read_chunk(List[ID]) or stop_and_read_neighbor(). (Here is the Action you\n",
      "selected from Action Options, which is in the form of a function call as mentioned before. The\n",
      "formal parameter in parentheses should be replaced with the actual parameter.)\n",
      "#####\n",
      "Finally, it is emphasized again that even if the atomic fact is only slightly relevant to the\n",
      "question, you should still look at the text chunk to avoid missing information. You should only\n",
      "choose stop_and_read_neighbor() when you are very sure that the given text chunk is irrelevant to\n",
      "the question. Please strictly follow the above format. Let’s begin.\n",
      "Figure 9: The prompt for exploring atomic facts.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
      "following elements:\n",
      "1. Text Chunks: Segments of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "Your current task is to assess a specific text chunk and determine whether the available information\n",
      "suffices to answer the question. Given the question, rational plan, previous actions, notebook\n",
      "content, and the current text chunk, you have the following Action Options:\n",
      "#####\n",
      "1. search_more(): Choose this action if you think that the essential information necessary to\n",
      "answer the question is still lacking.\n",
      "2. read_previous_chunk(): Choose this action if you feel that the previous text chunk contains\n",
      "valuable information for answering the question.\n",
      "3. read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains\n",
      "valuable information for answering the question.\n",
      "4. termination(): Choose this action if you believe that the information you have currently obtained\n",
      "is enough to answer the question. This will allow you to summarize the gathered information and\n",
      "provide a final answer.\n",
      "#####\n",
      "Strategy:\n",
      "#####\n",
      "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
      "2. You can only choose one action.\n",
      "#####\n",
      "Response format:\n",
      "#####\n",
      "*Updated Notebook*: First, combine your previous notes with new insights and findings about the\n",
      "question from current text chunks, creating a more complete version of the notebook that contains\n",
      "more valid information.\n",
      "*Rationale for Next Action*: Based on the given question, rational plan, previous actions, and\n",
      "notebook content, analyze how to choose the next action.\n",
      "*Chosen Action*: search_more() or read_previous_chunk() or read_subsequent_chunk() or\n",
      "termination(). (Here is the Action you selected from Action Options, which is in the form of a\n",
      "function call as mentioned before. The formal parameter in parentheses should be replaced with\n",
      "the actual parameter.)\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 10: The prompt for exploring chunks.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
      "following elements:\n",
      "1. Text Chunks: Segments of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "Your current task is to assess all neighboring nodes of the current node, with the objec-\n",
      "tive of determining whether to proceed to the next neighboring node. Given the question, rational\n",
      "plan, previous actions, notebook content, and the neighbors of the current node, you have the\n",
      "following Action Options:\n",
      "#####\n",
      "1. read_neighbor_node(key element of node): Choose this action if you believe that any of the\n",
      "neighboring nodes may contain information relevant to the question. Note that you should focus\n",
      "on one neighbor node at a time.\n",
      "2. termination(): Choose this action if you believe that none of the neighboring nodes possess\n",
      "information that could answer the question.\n",
      "#####\n",
      "Strategy:\n",
      "#####\n",
      "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
      "2. You can only choose one action. This means that you can choose to read only one neighbor\n",
      "node or choose to terminate.\n",
      "#####\n",
      "Response format:\n",
      "#####\n",
      "*Rationale for Next Action*: Based on the given question, rational plan, previous actions, and\n",
      "notebook content, analyze how to choose the next action.\n",
      "*Chosen Action*: read_neighbor_node(neighbor_node) or termination(). (Here is the Action you\n",
      "selected from Action Options, which is in the form of a function call as mentioned before. The\n",
      "formal parameter in parentheses should be replaced with the actual parameter.)\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 11: The prompt for exploring neighbors.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
      "following elements:\n",
      "1. Text Chunks: Segments of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "You have now explored multiple paths from various starting nodes on this graph, record-\n",
      "ing key information for each path in a notebook.\n",
      "Your task now is to analyze these memories and reason to answer the question.\n",
      "Strategy:\n",
      "#####\n",
      "1. You should first analyze each notebook content before providing a final answer.\n",
      "2. During the analysis, consider complementary information from other notes and employ a\n",
      "majority voting strategy to resolve any inconsistencies.\n",
      "3. When generating the final answer, ensure that you take into account all available information.\n",
      "#####\n",
      "Example:\n",
      "#####\n",
      "User:\n",
      "Question: Who had a longer tennis career, Danny or Alice?\n",
      "Notebook of different exploration paths:\n",
      "1. We only know that Danny’s tennis career started in 1972 and ended in 1990, but we don’t know\n",
      "the length of Alice’s career.\n",
      "2. ......\n",
      "Assistant:\n",
      "Analyze:\n",
      "The summary of search path 1 points out that Danny’s tennis career is 1990-1972=18 years.\n",
      "Although it does not indicate the length of Alice’s career, the summary of search path 2 finds this\n",
      "information, that is, the length of Alice’s tennis career is 15 years. Then we can get the final\n",
      "answer, that is, Danny’s tennis career is longer than Alice’s.\n",
      "Final answer:\n",
      "Danny’s tennis career is longer than Alice’s.\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 12: The prompt for answer reasoning.After reading some text, John was given the following question about the text:\n",
      "{QUESTION TEXT}\n",
      "John’s answer to the question was:\n",
      "{MODEL RESPONSE TEXT}\n",
      "The ground truth answer was:\n",
      "{REFERENCE RESPONSE TEXT}\n",
      "Does John’s answer agree with the ground truth answer?\n",
      "Please answer \"Yes\" or \"No\".\n",
      "Figure 13: The prompt for LLM-Rating-1.\n",
      "After reading some text, John was given the following question about the text:\n",
      "{QUESTION TEXT}\n",
      "John’s answer to the question was:\n",
      "{MODEL RESPONSE TEXT}\n",
      "The ground truth answer was:\n",
      "{REFERENCE RESPONSE TEXT}\n",
      "Does John’s answer agree with the ground truth answer?\n",
      "Please answer “Yes”, “Yes, partially”, or “No”. If John’s response has any overlap with the ground\n",
      "truth answer, answer “Yes, partially”. If John’s response contains the ground truth answer, answer\n",
      "“Yes”. If John’s response is more specific than the ground truth answer, answer “Yes”.\n",
      "Figure 14: The prompt for LLM-Rating-2.\n",
      "Please read the passage below and answer the question based on the passage.\n",
      "Passage:\n",
      "{PASSAGE TEXT}\n",
      "Question:\n",
      "{QUESTION TEXT}\n",
      "Now please answer this question based on the passage content.\n",
      "Figure 15: The prompt for Full Text Read.\n",
      "Please read the text chunks below and answer the question.\n",
      "Text chunks:\n",
      "{CHUNKED PASSAGE TEXT}\n",
      "Question:\n",
      "{QUESTION TEXT}\n",
      "If you think you can answer the question based on the above text chunks please output\n",
      "[answerable] and then output your answer.\n",
      "Otherwise, if there is not enough information to answer the question, please output:\n",
      "[unanswerable]\n",
      "Figure 16: The prompt for Chunk Read.Please read the text chunk below and answer the questions based on your previous summary.\n",
      "Text chunk:\n",
      "{CHUNKED PASSAGE TEXT}\n",
      "Your previous summary:\n",
      "{SUMMARY TEXT}\n",
      "Question:\n",
      "{QUESTION TEXT}\n",
      "If the above text chunk has information that can help answer the question, please extract\n",
      "the effective information, output [summary], and then output the refined information. Please note\n",
      "that it must be brief.\n",
      "If you can answer the question based on the above information, please output [answerable] and\n",
      "then output your answer.\n",
      "Otherwise, if there is insufficient information to answer the question, please output [unanswerable].\n",
      "Figure 17: The prompt for Chunk Read with Note.\n",
      "Please read the text chunk below and answer the question.\n",
      "Text chunks:\n",
      "{RETRIEVED PASSAGE TEXT}\n",
      "Question:\n",
      "{QUESTION TEXT}\n",
      "Now please answer this question based on the text chunks.\n",
      "Figure 18: The prompt for RAG.Now you are an intelligent assistant. Given a text, a question, and xsupporting facts that can\n",
      "answer the question, please determine how many supporting facts the text covers.\n",
      "Requirements:\n",
      "#####\n",
      "1. It’s possible that not all supporting facts are needed to answer the question, so you’ll need to\n",
      "analyze the supporting facts to determine which supporting facts are actually needed, and then\n",
      "determine whether those supporting facts are covered. Supporting facts that are not really needed\n",
      "are discarded, and you do not need to judge whether they are covered. So the number of real\n",
      "supporting facts is t (0 < t <= x).\n",
      "2. A supporting fact has some valid information that helps answer the question. When the text\n",
      "provides this part of the valid information, it is considered to have covered the supporting fact,\n",
      "even if the text does not provide all the information supporting the fact.\n",
      "3. The number of covered items in your output should be between 0 and t (including 0 and t).\n",
      "4. Please analyze and reason first, and then output the final result.\n",
      "#####\n",
      "Example:\n",
      "#####\n",
      "{EXAMPLE}\n",
      "#####\n",
      "Please note that you should follow: 0 <= Number of recalls <= True number of support-\n",
      "ing facts <= Number of supporting facts.\n",
      "Please output according to the example format. Now let’s start.\n",
      "Figure 19: The prompt for evaluating recall.\n",
      "Question &Answer\n",
      "Question What is the name of the castle in the city where the performer of Never Too Loud was\n",
      "formed?\n",
      "Answer Casa Loma\n",
      "Supporting Passages\n",
      "1.Never Too Loud is the fourth studio album by Canadian hard rock band Danko Jones.\n",
      "It was recorded at Studio 606 in Los Angeles, with the producer Nick Raskulinecz.\n",
      "2.Danko Jones is a Canadian hard rock trio from Toronto. The band consists of Danko\n",
      "Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums). The band’s music\n",
      "includes elements of hard rock and punk and they are known for their energetic live shows.\n",
      "3.Casa Loma (improper Spanish for \"Hill House\") is a Gothic Revival castle-style mansion\n",
      "and garden in midtown Toronto, Ontario, Canada, that is now a historic house museum\n",
      "and landmark. It was constructed from 1911 to 1914 as a residence for financier Sir Henry\n",
      "Pellatt. The architect was E. J. Lennox, who designed several other city landmarks.\n",
      "Figure 20: GraphReader Example(Question and Annotations) . We provide an example question with its answer,\n",
      "along with the supporting passages for this question. This is a typical 3-hop question where we need to gather\n",
      "information and reason step-by-step to arrive at the answer.Graph Construction: Extract Atomic Facts And Key Elements\n",
      "Atomic Facts\n",
      "1. \"Never Too Loud\" is the fourth studio album by Canadian hard rock band Danko Jones.\n",
      "2. Danko Jones is a Canadian hard rock trio from Toronto.\n",
      "3. Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario,\n",
      "Canada.\n",
      "......\n",
      "Key Elements\n",
      "1. [Never Too Loud, studio album, Canadian, hard rock band, Danko Jones]\n",
      "2. [Danko Jones, Canadian, hard rock trio, Toronto]\n",
      "3. [Casa Loma, Gothic Revival, castle-style mansion, Toronto, Canada]\n",
      "......\n",
      "Graph Construction: Normalize And Link Nodes\n",
      "Never Too Loudstudio albumcastle-style mansionTorontoDankoJonesGothic RevivalCasa LomaCanadianCanadaCanadahard rock bandhard rock triohard rock band\n",
      "Canadahard rock bandAtomicFacts1AtomicFacts2AtomicFacts3NormalizeLink\n",
      "Figure 21: GraphReader Example(Graph Construction) . The atomic facts and key elements extracted from the\n",
      "passage correspond to each other, after which the latter are normalized to serve as nodes. Finally, links are formed\n",
      "based on the co-occurrence relationships of the nodes within the atomic facts.Agent Initialization: Pre-plan And Select Initial Nodes\n",
      "Rational Plan To answer the question, we need to identify the performer or band associated\n",
      "with \"Never Too Loud\", determine the city where they were formed, and then find out the name of\n",
      "any notable castle in that city.\n",
      "Initial Node Never Too Loud\n",
      "Agent Initialization: Visual Representation\n",
      "Never Too Loudstudio albumcastle-style mansionTorontoDankoJonesGothic RevivalCasa Loma\n",
      "Canadahard rock bandAtomicFacts1AtomicFacts2AtomicFacts3InitialNodeNever Too LoudNotebook\n",
      "RationalPlan\n",
      "Figure 22: GraphReader Example(Agent Initialization) . Initially, a rational plan is formulated in response to the\n",
      "question, guiding further exploration; subsequently, the plan dictates the selection of the initial node from all nodes.Exploration: Function Call Process\n",
      "Exploring Atomic Facts Node: Never Too Loud; [Atomic Fact 1 from Chunk ID-6]\n",
      "Call Function read_chunk(ID-6) .\n",
      "Exploring Chunks Realized the performer of Never Too Loud is Danko Jones.\n",
      "Call Function: search_more\n",
      "Exploring Neighbors Node: Never Too Loud; Neighbor Nodes: [hard rock band, Danko Jones,\n",
      "studio album, Canada]\n",
      "Call Function read_neighbor_node(Danko Jones)\n",
      "Exploring Atomic Facts Node: Danko Jones; [Atomic Fact 1 from Chunk ID-6, Atomic Fact 2\n",
      "from Chunk ID-9].\n",
      "Call Function read_chunk(ID-9) .\n",
      "Exploring Chunks Realized Danko Jones band is a band from Toronto, Canada.\n",
      "Call Function: search_more\n",
      "Exploring Neighbors Node: Danko Jones; Neighbor Nodes: [hard rock band, Never Too Lou,\n",
      "studio album, Canada, Toronto]\n",
      "Call Function read_neighbor_node(Toronto)\n",
      "Exploring Atomic Facts Node: Toronto; [Atomic Fact 2 from Chunk ID-9, Atomic Fact 3 from\n",
      "Chunk ID-13].\n",
      "Call Function read_chunk(ID-13) .\n",
      "Exploring Chunks Realized the castle mentioned in the text in Toronto is Casa Loma.\n",
      "Call Function: termination\n",
      "Exploration: Visual Representation\n",
      "Never Too Loudstudio albumcastle-style mansionTorontoDankoJonesGothic RevivalCasa Loma\n",
      "Canadahard rock bandAtomicFacts1AtomicFacts2AtomicFacts3InitialNodeNever Too LoudNotebook\n",
      "RationalPlan\n",
      "Figure 23: GraphReader Example(Exploration) . GraphReader begins from the initial node, guided by the rational\n",
      "plan, carrying a notebook that records memory, gradually collecting information to answer the question.Answer Reasoning: Response Based on the Notebook\n",
      "Question What is the name of the castle in the city where the performer of Never Too Loud was\n",
      "formed?\n",
      "Memory from the notebook The performer of Never Too Loud is Danko Jones, which is a band\n",
      "from Toronto, Canada. The text mentions that the castle in Toronto is Casa Loma.\n",
      "GraphReader answer Casa Loma\n",
      "Figure 24: GraphReader Example(Answer Reasoning) . Ultimately, GraphReader answers the question based on\n",
      "the notebook recorded during the exploration process.\n"
     ]
    }
   ],
   "source": [
    "from autogen import retrieve_utils\n",
    "\n",
    "paper = retrieve_utils.extract_text_from_pdf(\"data/paper.pdf\")\n",
    "print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 09-09 23:49:22] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    \"chatbot\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4o-mini\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
    "    function_map=None,  # No registered functions, by default it is None.\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_atom_prompt = \"\"\"\n",
    "You are now an intelligent assistant tasked with meticulously extracting both key elements and atomic facts from a long text. 1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g., actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative. 2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include propositions, theories, existences, concepts, and implicit elements like logic, causality, event sequences, interpersonal relationships, timelines, etc.  Requirements: ##### 1. Ensure that all identified key elements are reflected within the corresponding atomic facts. 2. You should extract key elements and atomic facts comprehensively, especially those that are important and potentially query-worthy and do not leave out details. 3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He, She to actual names). 4. Ensure that the key elements and atomic facts you extract are presented in the same language as the original text (e.g., English or Chinese). 5. You should output a total of key elements and atomic facts that do not exceed 1024 tokens. 6. Your answer format for each line should be: [Serial Number], [Atomic Facts], [List of Key Elements, separated with ‘|’] #####  Example: ##### User: One day, a father and his little son ......  Assistant: 1. One day, a father and his little son were going home. | father | little son | going home 2. ...... #####  Please strictly follow the above format. Let’s begin.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are several potential flaws in the methods of the paper \"GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models\":\n",
      "\n",
      "1. **Dependence on Graph Construction**: The effectiveness of GraphReader relies heavily on the quality and completeness of the graph constructed from the text. If key elements or atomic facts are missed during the extraction, the subsequent exploration and reasoning may lead to incomplete or incorrect answers. The method describes normalization and linking processes, but these may not always guarantee that the most relevant information is represented in the graph.\n",
      "\n",
      "2. **Chunking Limitations**: The authors divide long texts into chunks while preserving paragraph structures. However, this approach may not always capture the necessary context for complex multi-hop questions. Key information might be divided across adjacent chunks, leading to incomplete understanding or loss of critical relationships between facts. Additionally, the choice of maximum chunk size (2k tokens) may not be optimal and could affect the extraction of atomic facts.\n",
      "\n",
      "3. **Scalability Concerns**: While the method claims to improve performance for long-context tasks, there is no discussion on how the performance scales with extremely large inputs (e.g., 1M tokens or more), which may still overwhelm the graph's structure or cause inefficiencies in exploration.\n",
      "\n",
      "4. **Rational Plan and Initial Node Selection**: The reliance on a rational plan for node selection and exploration may introduce bias in the exploration process. If the initial nodes selected are not the most relevant, the exploration could deviate from critical information. The process relies on effective reasoning capabilities of the agent, and any deficiencies here may hinder performance.\n",
      "\n",
      "5. **Limited Exploration**: The method proposes a coarse-to-fine exploration strategy. However, if the agent fails to access relevant neighboring nodes or does not explore all potential paths due to misjudgment of relevance, it may miss critical supporting facts. The decision-making process during exploration may not consider all relevant factors, leading to suboptimal information retrieval.\n",
      "\n",
      "6. **Evaluation Metrics and Bias**: The results depend on various evaluation metrics (F1, EM, etc.), but the paper does not thoroughly discuss the potential biases or limitations of these metrics in the context of long-context question answering. The choice of benchmarks may not fully capture the complexity of real-world scenarios that GraphReader aims to address.\n",
      "\n",
      "7. **Ablation Studies**: Although ablation studies are presented, they may not cover all possible variations of the method. For instance, exploring the impact of different graph structures, node connections, or alternative exploration strategies could provide deeper insights into how different factors influence performance. Limited ablation studies may lead to overgeneralization of the findings.\n",
      "\n",
      "8. **Dependence on External Models**: The paper uses GPT-4 for implementing GraphReader. Any limitations or biases inherent to GPT-4 could impact the performance of the GraphReader system as a whole. Moreover, reliance on off-the-shelf API limits opportunities for more nuanced improvements and could impose constraints on handling queries in specific domains.\n",
      "\n",
      "9. **Robustness Across Domains**: The experimental results primarily focus on specific datasets, and the paper does not address how well the method performs across varied domains or types of long-context tasks beyond those tested. The transferability of the approach to other datasets or different types of input data may be limited.\n",
      "\n",
      "10. **Computational Efficiency**: While the paper addresses performance in terms of accuracy, computational efficiency (in terms of time and resource usage) is not thoroughly examined. As the method relies on exploring numerous nodes and functions, attention to resource management and efficiency would be critical, especially for larger texts.\n",
      "\n",
      "These points represent considerations that could be further scrutinized to enhance the robustness, scalability, and applicability of the GraphReader method in real-world long-context applications.\n"
     ]
    }
   ],
   "source": [
    "reply = agent.generate_reply(messages=[{\"content\": paper, \"role\": \"system\"},{\"content\": \"Find flaws in this paper's methods.\", \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\tGraphReader: Building Graph-based Agent to Enhance ...\n",
      "Failed to split docs with must_break_at_empty_line being True, set to False.\n"
     ]
    }
   ],
   "source": [
    "chunks = retrieve_utils.split_text_to_chunks(paper, 300, \"multi_lines\")\n",
    "\n",
    "chunk_dict = {i: chunk for i, chunk in enumerate(chunks)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. | Long-context capabilities | large language models | complex | long-input tasks  \n",
      "2. Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs. | Numerous efforts | optimize | LLMs | long contexts | challenges | robustly processing | long inputs  \n",
      "3. This paper introduces GraphReader, a graph-based agent system designed to handle long texts. | GraphReader | graph-based agent system | handle | long texts  \n",
      "4. GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously. | GraphReader | structures | long texts | graph | agent | explore | autonomously  \n",
      "5. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. | agent | receiving | question | undertakes | step-by-step analysis | devises | rational plan  \n",
      "6. The agent invokes a set of predefined functions to read node content and neighbors. | agent | invokes | predefined functions | read | node content | neighbors  \n",
      "7. The graph-based approach facilitates a coarse-to-fine reading process of long texts. | graph-based approach | facilitates | coarse-to-fine reading process | long texts  \n"
     ]
    }
   ],
   "source": [
    "reply = agent.generate_reply(\n",
    "    messages=[\n",
    "        {\"content\": key_atom_prompt, \"role\": \"system\"},\n",
    "        {\"content\": chunks[0], \"role\": \"user\"},\n",
    "    ]\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
