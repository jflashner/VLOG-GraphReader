{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphReader: Building Graph-based Agent to Enhance\n",
      "Long-Context Abilities of Large Language Models\n",
      "Shilong Li∗1, Yancheng He∗1, Hangyu Guo∗1, Xingyuan Bu∗†‡1, Ge Bai1, Jie Liu2,3,\n",
      "Jiaheng Liu1, Xingwei Qu4, Yangguang Li3, Wanli Ouyang2,3, Wenbo Su1, Bo Zheng1\n",
      "1Alibaba Group2The Chinese University of Hong Kong\n",
      "3Shanghai AI Laboratory4University of Manchester\n",
      "{zhuli.lsl, buxingyuan.bxy}@taobao.com\n",
      "Abstract\n",
      "Long-context capabilities are essential for large\n",
      "language models (LLMs) to tackle complex\n",
      "and long-input tasks. Despite numerous efforts\n",
      "made to optimize LLMs for long contexts, chal-\n",
      "lenges persist in robustly processing long in-\n",
      "puts. In this paper, we introduce GraphReader,\n",
      "a graph-based agent system designed to han-\n",
      "dle long texts by structuring them into a graph\n",
      "and employing an agent to explore this graph\n",
      "autonomously. Upon receiving a question, the\n",
      "agent first undertakes a step-by-step analysis\n",
      "and devises a rational plan. It then invokes a\n",
      "set of predefined functions to read node con-\n",
      "tent and neighbors, facilitating a coarse-to-fine\n",
      "exploration of the graph. Throughout the ex-\n",
      "ploration, the agent continuously records new\n",
      "insights and reflects on current circumstances\n",
      "to optimize the process until it has gathered suf-\n",
      "ficient information to generate an answer. Ex-\n",
      "perimental results on the LV-Eval dataset reveal\n",
      "that GraphReader, using a 4k context window,\n",
      "consistently outperforms GPT-4-128k across\n",
      "context lengths from 16k to 256k by a large\n",
      "margin. Additionally, our approach demon-\n",
      "strates superior performance on four challeng-\n",
      "ing single-hop and multi-hop benchmarks.\n",
      "1 Introduction\n",
      "Large language models (LLMs) have made\n",
      "great progress on natural language understand-\n",
      "ing and generation (Zhao et al., 2023). However,\n",
      "transformer-based LLMs still struggle in handling\n",
      "long contexts due to the limitation of context win-\n",
      "dow and memory usage.\n",
      "Current techniques for solving the long-context\n",
      "tasks of LLMs can be divided into two perspec-\n",
      "tives: 1) Model-level, which includes finetuning\n",
      "with modified positional embeddings (Chen et al.,\n",
      "2023b; Zhu et al., 2023; Peng et al., 2023; Ding\n",
      "et al., 2024), and applying transformer variants\n",
      "∗First four authors contributed equally.\n",
      "†Corresponding Author. ‡Project Leader.\n",
      "16k 32k 64k 128k 256k\n",
      "Input Length5101520253035Average Scores (%)\n",
      "GraphReader\n",
      "ReadAgent\n",
      "GPT-4-128k\n",
      "GPT-4-128k (chunk w/ notes)GPT-4-128k (chunk)\n",
      "Ada-002 (top-1)\n",
      "BM25 (top-1)Figure 1: Performance on LV-Eval at 5 context length\n",
      "levels. GraphReader outperforms existing open-sourced\n",
      "and closed-source models while demonstrating a scal-\n",
      "able performance in very long contexts. In contrast,\n",
      "other models exhibit a significant decrease in perfor-\n",
      "mance as context length increases.\n",
      "with modified attention mechanisms (Dai et al.,\n",
      "2019; Munkhdalai et al., 2024; Gu and Dao, 2023);\n",
      "2) Agent-level, i.e.,employing retrieval-augmented\n",
      "LLM or agent to process long contexts with a lim-\n",
      "ited context window LLM (Nakano et al., 2021;\n",
      "Lee et al., 2024).\n",
      "However, model-level methods typically train\n",
      "LLMs with target length texts, posing challenges\n",
      "in constructing training datasets and incurring high\n",
      "training costs (Zhu et al., 2023). Additionally, long-\n",
      "context LLMs optimized with these methods tend\n",
      "to overlook crucial details in long contexts, known\n",
      "as “lost in the middle” (Liu et al., 2024), limit-\n",
      "ing their ability to address complex tasks, such as\n",
      "multi-hop questions. Agent-level approaches trans-\n",
      "form input text into a tree (Chen et al., 2023a) or\n",
      "paginated pages (Lee et al., 2024), failing to cap-\n",
      "ture multi-hop and long-range dependencies, thus\n",
      "limiting their effectiveness on very long contexts,\n",
      "as shown in Figure 1.arXiv:2406.14550v1  [cs.CL]  20 Jun 2024To address these issues, we propose a graph-\n",
      "based agent named GraphReader . As illustrated\n",
      "in Figure 2, GraphReader first segments long texts\n",
      "into discrete chunks, extracts essential informa-\n",
      "tion, and compresses these into key elements and\n",
      "atomic facts. These key elements and facts are\n",
      "then used to construct a graph with nodes repre-\n",
      "senting key elements and their associated atomic\n",
      "facts. This graph structure effectively captures\n",
      "long-range dependencies and multi-hop relation-\n",
      "ships within long text. Subsequently, GraphReader\n",
      "autonomously explores this graph using predefined\n",
      "functions, guided by a step-by-step rational plan.\n",
      "Based on a given question, the agent progressively\n",
      "accesses information from coarse key elements and\n",
      "atomic facts to detailed original text chunks, tak-\n",
      "ing notes and reflecting until it gathers sufficient\n",
      "information to generate an answer. In summary,\n",
      "our main contributions are threefold:\n",
      "•We introduce GraphReader, a novel agent sys-\n",
      "tem designed to organize long texts into a graph\n",
      "structure, leveraging predefined functions and\n",
      "notebook to facilitate planning and reflection\n",
      "during exploration.\n",
      "•GraphReader establishes a scalable long-context\n",
      "capability based on a 4k context window, demon-\n",
      "strating performance that is comparable to or\n",
      "surpasses GPT-4 with a 128k context window\n",
      "across varying context lengths.\n",
      "•Extensive experiments conducted on four\n",
      "challenging benchmarks demonstrate that\n",
      "GraphReader achieves superior performance in\n",
      "complex single-hop and multi-hop QA tasks.\n",
      "2 Related Work\n",
      "Long-Context LLMs Recent efforts (Chen et al.,\n",
      "2023b; Ding et al., 2024; Peng et al., 2023) have\n",
      "focused on positional interpolation (PI) to enhance\n",
      "long-context capabilities. However, these methods\n",
      "require training on full-length texts, leading to sig-\n",
      "nificant increases in data and training costs (Chen\n",
      "et al., 2023c; Fu et al., 2024; Bai et al., 2024b).\n",
      "Thus, PoSE (Zhu et al., 2023) and SkipAlign (Wu\n",
      "et al., 2024a) investigate data skip strategy, but tend\n",
      "to neglect detailed information in long texts (Liu\n",
      "et al., 2024; Bai et al., 2024a; Wu et al., 2024b).\n",
      "Furthermore, despite how extensively the context\n",
      "window is expanded, it remains constrained by a\n",
      "predefined fixed length. To address these limita-\n",
      "tions, transformer variants with modified attention\n",
      "mechanisms have been proposed (Dai et al., 2019;Gu and Dao, 2023; Munkhdalai et al., 2024). How-\n",
      "ever, these models are prone to losing earlier infor-\n",
      "mation.\n",
      "Retrieval Retrieval Augmented Generation\n",
      "(RAG) leverages an extensive database of docu-\n",
      "ments to extract task-related information that aids\n",
      "in response generation. Many efforts investigate\n",
      "various levels of retrieval granularity, including\n",
      "tokens (Khandelwal et al., 2019), entities (Févry\n",
      "et al., 2020; De Jong et al., 2021), and chunks (Liu,\n",
      "2024; LangChain-team, 2024). Other approaches\n",
      "have explored diverse retrieval methods, such\n",
      "as BM25 (Rasooli and Tetreault, 2015) and\n",
      "learning-based strategies (Khattab and Zaharia,\n",
      "2020; Sachan et al., 2023; Sun et al., 2021).\n",
      "Despite its capabilities, RAG faces challenges in\n",
      "addressing complex questions due to difficulties in\n",
      "developing robust decision-making mechanisms.\n",
      "In contrast, we employ agents that use planning\n",
      "and reflection to gather essential information,\n",
      "effectively tackling complex problems.\n",
      "Agent for Retrieval Recent work has increas-\n",
      "ingly leveraged LLMs as agents to tackle complex\n",
      "problems, utilizing their strong planning and reflec-\n",
      "tion abilities (Yao et al., 2022; Park et al., 2023).\n",
      "These abilities have been applied to complex tasks\n",
      "such as function call (Li et al., 2023) and KGQA\n",
      "(Sun et al., 2023; Luo et al., 2023). Agents are also\n",
      "capable of retrieving unstructured information. For\n",
      "example, WebGPT (Nakano et al., 2021) simulates\n",
      "human actions to search on internet for specific\n",
      "answers. Additionally, MemWalker (Chen et al.,\n",
      "2023a) and PEARL (Sarthi et al., 2024) organize\n",
      "documents into a tree structure, while ReadAgent\n",
      "(Lee et al., 2024) condenses documents into a gist\n",
      "memory directory. However, these approaches of-\n",
      "ten struggle with multi-hop questions. KGP (Wang\n",
      "et al., 2024) organizes documents into graphs, but\n",
      "it primarily uses the agent to generate queries,\n",
      "thereby not fully exploiting the agent’s capabili-\n",
      "ties for planning and reflection.\n",
      "3 Approach\n",
      "3.1 Preliminary\n",
      "GraphReader is built on a graph G={V,E},\n",
      "where each node vi∈ V contains a key element ki\n",
      "and a set of summarized content, namely atomic\n",
      "factsAi. In other words, vi={ki,Ai}. And\n",
      "each edge eij∈ E represents the relationship be-\n",
      "tween nodes viandvj. This graph structure en-\n",
      "ables GraphReader to capture global informationAtomicFactsFromChunkID31.Thereareoftenpolicemanpresenceattheintersectionnexttotheschool.Node\n",
      "Question…NeighborsCanadaTorontoAlbum…(a)GraphConstructionLongContext\n",
      "Extract\n",
      "TorontoCasa LomaDankoJonesGothicAlbumCanada(b)GraphExplorationWhatisthenameofthecastleinthecitywheretheperformerofNeverTooLoudwasformed?\n",
      "Pre-planning\n",
      "InitialNodeSelectionRational Planweneedtoidentifytheperformerorbandassociatedwith\"NeverTooLoud\",determinethecitywheretheywereformed,andthenfindoutthenameofanynotablecastleinthatcity.KeyElementDankoJones\n",
      "read_chunk(1,3,5)stop_and_read_neighbor()Never Too LoudAlbumNotebookInitialization\n",
      "135QueueExploringChunks\n",
      "read_previous/subsequent_chunk()search_more()termination()35Queue\n",
      "ChunkID1ID1235Queuepop()insert()\n",
      "UpdatedNotebook1. The performer of Never Too Loud is DankoJones, which is a band from Toronto, Canada.2. The text mentions that the castle in Toronto is Casa Loma.InitialNotebookNone\n",
      "read_neighbor_node()\n",
      "Exploring Neighborstermination()\n",
      "Exploring Atomic Facts\n",
      "Question\n",
      "ResponseCasa Loma\n",
      "read_chunk(x,y,z)(c)AnswerReasoningAtomicFactsFromChunkID21.Thereareoftenpolicemanpresenceattheintersectionnexttotheschool.AtomicFactsFromChunkID11.\"NeverTooLoud\"isthefourthstudioalbumbyCanadianhardrockbandDankoJones.2.DankoJonesisaCanadianhardrocktriofromToronto.\n",
      "Rational PlanNotebookFigure 2: The illustration of our GraphReader approach, consisting of graph construction, graph exploration, and\n",
      "answer reasoning.\n",
      "from the input document Dwithin a limited context\n",
      "window, allowing it to decide whether to explore\n",
      "the current node in detail or jump to a neighbor-\n",
      "ing node. During graph exploration, GraphReader\n",
      "collects supporting facts and terminates the explo-\n",
      "ration once sufficient information has been gath-\n",
      "ered to answer the question. As illustrated in Fig-\n",
      "ure 2, the entire process of GraphReader consists\n",
      "of the following three phases: graph construction,\n",
      "graph exploration, and answer reasoning. The\n",
      "prompts utilized in these three stages are detailed in\n",
      "Appendix A, and a detailed example of our process\n",
      "can be found in Appendix H.\n",
      "3.2 Graph Construction\n",
      "To extract nodes from a document Dwithin the\n",
      "LLM’s context limit, we first split Dinto chunks\n",
      "of maximum length Lwhile preserving paragraph\n",
      "structure. For each chunk, we prompt the LLM to\n",
      "summarize it into atomic facts, the smallest indivis-\n",
      "ible facts that simplify the original text. We also\n",
      "prompt the LLM to extract key elements from each\n",
      "atomic fact like essential nouns, verbs, and adjec-\n",
      "tives. After processing all chunks, we normalizethe key elements as described by Lu et al. (2023) to\n",
      "handle lexical noise and granularity issues, creating\n",
      "a final set of key elements. We then construct each\n",
      "nodevi= (ki,Ai), where kiis a key element and\n",
      "Aiis the set of atomic facts corresponding to ki.\n",
      "Finally, we link two nodes viandvjif key element\n",
      "kiappears in Ajand vice versa.\n",
      "3.3 Graph Exploration\n",
      "3.3.1 Agent Initialization\n",
      "Given a graph Gand a question Q, our goal is to\n",
      "design an agent that can autonomously explore the\n",
      "graph using predefined functions. The agent begins\n",
      "by maintaining a notebook to record supporting\n",
      "facts, which are eventually used to derive the final\n",
      "answer. Then the agent performs two key initializa-\n",
      "tions: defining the rational plan and selecting the\n",
      "initial node.\n",
      "Rational Plan To tackle complex real-world\n",
      "multi-hop questions, pre-planning the solution is\n",
      "crucial. The agent breaks down the original ques-\n",
      "tion step-by-step, identifies the key information\n",
      "needed, and forms a rational plan.Initial Node Choosing strategic starting points\n",
      "is essential for improving search efficiency. The\n",
      "agent evaluates the key elements of all nodes Vand\n",
      "selects Ninitial nodes based on the question and\n",
      "the rational plan.\n",
      "3.3.2 Exploration\n",
      "After selecting Ninitial nodes as starting points, an\n",
      "agent explores each initial node by first exploring\n",
      "atomic facts, then chunks of the node. Next, it\n",
      "explores neighboring nodes, guided by the question\n",
      "and rational plan. The agent continuously updates\n",
      "the notebook with relevant information during the\n",
      "exploration process.\n",
      "Exploring Atomic Facts It is impractical to in-\n",
      "clude all original text chunks related to a node\n",
      "within the context window. Therefore, the agent\n",
      "employs a coarse-to-fine strategy, progressing from\n",
      "reading atomic facts to the original text, as all\n",
      "atomic facts can fit within the context window.\n",
      "Initially, all atomic facts associated with a node\n",
      "are grouped by their corresponding chunks, la-\n",
      "beled with the respective chunk IDs, and fed to\n",
      "the agent. This allows the agent to capture an\n",
      "overview of each chunk by reading all groups of\n",
      "atomic facts. Meanwhile, the agent utilizes the\n",
      "question, rational plan, and notes in its notebook to\n",
      "reflect on the required clues and determine which\n",
      "chunk is likely to contain useful information. Sub-\n",
      "sequently, the agent is provided with two functions:\n",
      "1)read_chunk , if the agent identifies certain chunks\n",
      "as valuable for further reading, it will complete\n",
      "the function parameters with the chunk IDs, i.e.,\n",
      "read_chunk(List[ID]) , and append these IDs to a\n",
      "chunk queue. 2) stop_and_read_neighbor , con-\n",
      "versely, if the agent deems that none of the chunks\n",
      "are worth further reading, it will finish reading this\n",
      "node and proceed to explore neighboring nodes.\n",
      "Exploring Chunks When the chunk queue is\n",
      "non-empty, it indicates that the agent has iden-\n",
      "tified multiple text chunks of interest. We then\n",
      "traverse the queue, reading each chunk. This\n",
      "step is essential because atomic facts merely sum-\n",
      "marize key information and provide brief clues,\n",
      "whereas specific details are best obtained directly\n",
      "from the original text chunks. While reading\n",
      "the chunks, the agent will once again consider\n",
      "the question and the plan, thinking about what\n",
      "can be added to the current notebook. Any sup-\n",
      "porting facts discovered will be recorded in the\n",
      "notebook. Depending on the updated notebook,the agent will then select one of the following\n",
      "four functions: 1) search_more , if supporting fact\n",
      "is insufficient, the agent will continue exploring\n",
      "chunks in the queue; 2) read_previous_chunk and\n",
      "3)read_subsequent_chunk , due to truncation issues,\n",
      "adjacent chunks might contain relevant and useful\n",
      "information, the agent may insert these IDs to the\n",
      "queue; 4) termination , if sufficient information has\n",
      "been gathered for answering the question, the agent\n",
      "will finish exploration.\n",
      "Exploring Neighbors Once the atomic facts and\n",
      "chunk queue of the current node have been fully\n",
      "processed, it indicates that this node has been thor-\n",
      "oughly explored, and the agent needs to access the\n",
      "next node. Taking into account the question, ra-\n",
      "tional plan, and the content of the notebook, the\n",
      "agent checks all neighboring nodes, i.e.,key el-\n",
      "ements, and performs one of two functions: 1)\n",
      "read_neighbor_node , the agent selects a neighbor-\n",
      "ing node that might be helpful in answering the\n",
      "question and re-enters the process of exploring\n",
      "atomic facts and chunks; 2) termination , the agent\n",
      "determines that none of the neighboring nodes con-\n",
      "tain useful information, it finish the exploration.\n",
      "3.4 Answer Reasoning\n",
      "After Nagents have independently gathered in-\n",
      "formation and stopped their exploration, we will\n",
      "compile all notes from each agent for reasoning\n",
      "and generating the final answer. Employing Chain-\n",
      "of-Thought (Wei et al., 2022), the LLM first an-\n",
      "alyzes each note by considering complementary\n",
      "information from other memories and using a ma-\n",
      "jority voting strategy to resolve any inconsistencies.\n",
      "Ultimately, the LLM will consider all the available\n",
      "information to generate the final answer.\n",
      "4 Experiments\n",
      "4.1 Experimental Settings\n",
      "Evaluation Benchmarks We conduct experi-\n",
      "ments on two types of long-context QA bench-\n",
      "marks, including multi-hop long-context QA,\n",
      "i.e.,HotpotQA (Yang et al., 2018), 2WikiMulti-\n",
      "hopQA (Ho et al., 2020), MuSiQue (Trivedi et al.,\n",
      "2022), and a single-hop long-context QA bench-\n",
      "mark, i.e.,NarrativeQA (Kociský et al., 2018) from\n",
      "LongBench (Bai et al., 2023). Additionally, we\n",
      "also incorporate HotpotWikiQA-mixup from LV-\n",
      "Eval (Yuan et al., 2024), a multi-hop benchmark\n",
      "that features five levels of text length: 16k, 32k,\n",
      "64k, 128k, and 256k. Table 1 presents the statisticsabout these benchmarks, and detailed information\n",
      "is provided in Appendix C.\n",
      "Evaluation Metrics We employ several auto-\n",
      "matic evaluation metrics, i.e.,F1score, Exact\n",
      "Match (EM) score, and an optimized F1* score, as\n",
      "introduced by LV-Eval (Yuan et al., 2024). Specif-\n",
      "ically, F1* first computes the recall of golden an-\n",
      "swer keywords and only calculates the F1score if\n",
      "it exceeds a certain threshold. Otherwise, the score\n",
      "defaults to zero. Despite the cost-effectiveness of\n",
      "automatic metrics, their accuracy may be affected\n",
      "by the response format. Hence, we implement\n",
      "LLM Raters for answer correctness evaluation us-\n",
      "ing an LLM, denoted as LLM-Rating-1 (LR-1) and\n",
      "LLM-Rating-1 (LR-2), following ReadAgent (Lee\n",
      "et al., 2024). Details on the evaluation metrics can\n",
      "be found in Appendix B.\n",
      "Baseline Methods We compare our approach\n",
      "with the following baselines: retrieval augmented\n",
      "generation (RAG), long-context LLM, and agent-\n",
      "based methods. (1) RAG : We choose Okapi\n",
      "BM25 (Robertson and Zaragoza, 2009) or Ope-\n",
      "nAI API embedding model Ada-002 to retrieve\n",
      "the chunks most relevant to the question and em-\n",
      "ploy GPT-4-128k ( gpt-4-1106-preview ) to read\n",
      "retrieved chunks and answer the question. (2)\n",
      "Long-context LLM : We select GPT-4-128k for\n",
      "directly reading full text when the text content fits\n",
      "within the input window, or for segmenting the\n",
      "text into chunks for sequential reading. (3) Agent-\n",
      "based Method : We select ReadAgent (Lee et al.,\n",
      "2024), which employs an agent-based system for\n",
      "the execution of retrieval and reading processes for\n",
      "long-context QA. The detailed description of these\n",
      "methods is provided in Appendix D.\n",
      "Implementation Details In our experiments, we\n",
      "employ GPT-4-128k for both our method and base-\n",
      "line approaches, setting the temperature to 0.2. For\n",
      "GraphReader, the input window size is configured\n",
      "to 4k tokens unless stated otherwise. We limit the\n",
      "maximum chunk size to 2k tokens, initiate searches\n",
      "from 5 initial nodes, and impose a function call\n",
      "limit of 10 for each search path.\n",
      "4.2 Main Results\n",
      "The results of three types of methods on four multi-\n",
      "hop long-context benchmarks and one single-hop\n",
      "https://platform.openai.com/docs/guides/embeddings/\n",
      "embedding-models\n",
      "https://github.com/openai/tiktokenTask Dataset Avg #Tokens Max #Tokens #Samples\n",
      "Multi-hop QAHotpotQA 9.4k 15.9k 300\n",
      "2WikiMultihopQA 8.8k 15.9k 300\n",
      "MuSiQue 15.5k 16.0k 200\n",
      "HotpotWikiQA-mixup 142.4k 370.8k 250\n",
      "Single-hop QA NarrativeQA 29.7k 63.7k 200\n",
      "Table 1: The statistics of benchmarks employed in our\n",
      "evaluation. The token number is calculated using the\n",
      "GPT-4 tokenizer from the TikToken. #Samples denote\n",
      "the total number of benchmarks.\n",
      "long-context benchmark are shown in Table 2 and\n",
      "Table 3. Based on the results, we have the following\n",
      "findings:\n",
      "Results of RAG methods As the results shown\n",
      "in Table 2, RAG methods based on BM25 and Ada-\n",
      "002 exhibit the worst performance in comparison\n",
      "to long-context LLM and agent-based methods. A\n",
      "possible reason is that text retrieval has difficulty\n",
      "recalling all chunks that contain the supporting\n",
      "facts for answering the input question. Although\n",
      "increasing the number of recalled chunks could im-\n",
      "prove the performance of text retrieval, the context\n",
      "window will limit the effectiveness of these RAG\n",
      "methods.\n",
      "Results of Long-Context LLMs From the re-\n",
      "sults shown in Table 2, we can see that employ-\n",
      "ing GPT-4-128k to directly answer the question\n",
      "with long contexts significantly outperforms RAG\n",
      "methods and even outperforms ReadAgent on three\n",
      "long-context benchmarks. This is because of the\n",
      "superior performance of GPT-4-128k in processing\n",
      "long texts and executing multi-hop reasoning tasks.\n",
      "Additionally, the lengths of these four benchmarks\n",
      "are significantly shorter than the 128k context win-\n",
      "dow, thereby mitigating the impact of “lost in the\n",
      "middle” on the model’s performance.\n",
      "Results of Agent-based Methods By comparing\n",
      "our approach with all baselines in Table 2, it is\n",
      "obvious that our approach consistently performs\n",
      "better than them on four long-context benchmarks\n",
      "and demonstrates superior performance in multi-\n",
      "hop long-context tasks. In our approach, benefiting\n",
      "from the graph’s ability to capture the relationships\n",
      "between detailed information, our method can iden-\n",
      "tify crucial information and search for the support-\n",
      "ing facts for the input question efficiently. This\n",
      "strategy significantly boosts the agent’s capability\n",
      "in multi-hop reasoning and capturing long-range\n",
      "dependencies of key information in a long context.\n",
      "Moreover, the results in Table 2 show that ReadA-MethodInput HotpotQA 2WikiMultihopQA MuSiQue NarrativeQA\n",
      "Window LR-1 LR-2 EM F1LR-1 LR-2 EM F1LR-1 LR-2 EM F1LR-1 LR-2 EM F1\n",
      "BM25 (top-1) 4k 57.7 63.0 33.7 43.8 36.0 39.0 25.0 30.4 33.0 36.5 19.0 23.9 29.5 34.5 4.0 11.3\n",
      "BM25 (top-3) 4k 74.7 78.3 45.7 58.5 59.7 62.0 42.3 51.9 43.5 49.5 25.0 31.1 44.5 52.5 7.0 20.5\n",
      "Ada-002 (top-1) 4k 63.0 70.7 40.0 53.2 57.0 59.3 41.0 49.4 34.5 37.0 20.0 26.6 37.5 46.5 5.0 15.5\n",
      "Ada-002 (top-3) 4k 72.0 77.3 45.0 58.1 65.7 66.7 44.7 55.3 40.0 45.5 24.5 32.1 45.5 53.0 7.5 19.5\n",
      "GPT-4-128k 128k 83.3 88.3 53.0 68.4 77.3 80.0 58.7 70.0 52.0 59.5 33.5 42.7 63.5 77.0 11.5 29.4\n",
      "GPT-4-128k (chunk) 4k 71.3 74.7 45.7 59.5 59.3 62.3 40.7 50.5 41.0 43.0 23.0 32.1 58.0 69.5 9.50 25.5\n",
      "GPT-4-128k (chunk w/ notes) 4k 72.3 76.7 45.7 59.5 65.7 68.7 46.3 56.6 39.5 43.0 25.0 32.5 56.5 65.0 8.5 24.3\n",
      "ReadAgent 128k 72.3 78.7 48.0 62.0 79.0 81.0 52.7 63.7 54.5 61.0 35.0 45.1 63.0 75.5 5.0 18.9\n",
      "GraphReader 4k 84.3 89.7 55.0 70.0 83.7 87.0 59.3 70.1 59.0 63.5 38.0 47.4 65.0 80.0 15.5 29.8\n",
      "Golden 4k 92.3 93.7 57.0 73.8 88.3 89.7 63.0 73.4 66.0 69.0 45.0 56.0 - - - -\n",
      "Table 2: Performance (%) comparison of different baselines on datasets from LongBench. The best performance\n",
      "and the second-best performance are denoted in bold and underlined fonts, respectively. “Golden” denotes the\n",
      "settings in which we add question and its supporting facts to LLM directly.\n",
      "MethodInputHotpotWikiQA-mixup\n",
      "Window16k 32k 64k 128k 256k\n",
      "LR-1 LR-2 F1*LR-1 LR-2 F1*LR-1 LR-2 F1*LR-1 LR-2 F1*LR-1 LR-2 F1*\n",
      "BM25 (top-1) 4k 10.0 16.0 12.0 16.0 18.0 11.9 6.0 8.0 8.5 10.0 8.0 7.0 14.0 20.0 5.9\n",
      "BM25 (top-3) 4k 16.0 22.0 13.9 18.0 28.0 13.3 16.0 18.0 11.8 12.0 16.0 11.8 12.0 22.0 9.3\n",
      "Ada-002 (top-1) 4k 10.0 12.0 14.5 14.0 18.0 11.3 10.0 12.0 12.5 12.0 14.0 9.4 8.0 8.0 7.0\n",
      "Ada-002 (top-3) 4k 24.0 28.0 21.3 20.0 30.0 19.8 14.0 20.0 12.9 16.0 20.0 12.0 14.0 18.0 10.8\n",
      "GPT-4-128k 128k 38.0 38.0 35.7 26.0 30.0 26.0 22.0 24.0 20.6 16.0 16.0 14.6 14.0 16.0 10.3\n",
      "GPT-4-128k (chunk) 4k 18.0 22.0 24.6 16.0 20.0 17.7 20.0 24.0 17.0 20.0 24.0 14.7 28.0 30.0 10.7\n",
      "GPT-4-128k (chunk w/ notes) 4k 22.0 32.0 24.2 26.0 30.0 21.3 28.0 32.0 22.0 24.0 26.0 17.4 26.0 26.0 14.8\n",
      "ReadAgent 128k 24.0 26.0 29.2 20.0 22.0 16.9 24.0 30.0 15.3 14.0 18.0 13.6 20.0 22.0 10.4\n",
      "GraphReader 4k 42.0 42.0 38.2 32.0 38.0 36.4 30.0 36.0 32.9 28.0 34.0 30.6 30.0 38.0 33.0\n",
      "Table 3: Performance (%) of different baselines on datasets from LV-Eval, where F1* donates LV-Eval’s optimized\n",
      "F1. The best performance and the second-best performance are denoted in bold and underlined fonts, respectively.\n",
      "We truncate to keep the longest possible initial fragment while preserving paragraph structure, in contexts that\n",
      "exceed the input window (128k and 256k) for GPT-4-128k.\n",
      "gent, with a 128k context window setup, under-\n",
      "performs GraphReader with a 4k context window\n",
      "and even performs worse than GPT-4-128k full-text\n",
      "reading. We attribute this to ReadAgent’s strategy\n",
      "of excessively compressing the original texts into\n",
      "gist memories, and feeding all mixed memories to\n",
      "the model for page number selection. Compared to\n",
      "our GraphReader, the strategy of ReadAgent may\n",
      "restrict the agent’s ability to identify specific de-\n",
      "tails and capture intrinsic connections among key\n",
      "elements in a long context, consequently affect-\n",
      "ing its overall performance. This further indicates\n",
      "that our approach can more efficiently unlock the\n",
      "capabilities of constrained context window LLMs\n",
      "in processing long context. Additionally, we ob-\n",
      "serve that the performance of our method closely\n",
      "matches that achieved by directly supplying sup-\n",
      "porting facts to the LLM ( i.e.,Golden in Table 2).\n",
      "This is because our method incorporates not only\n",
      "pre-planning, reflection, and various actions but\n",
      "also the usage of a graph containing key informa-tion, facilitating the agent to search for the correct\n",
      "supporting facts.\n",
      "Evaluation on Extremely Long Context Tasks\n",
      "As shown in previous experiments, it demonstrates\n",
      "the effectiveness of employing a limited context\n",
      "window LLM for long-context tasks with our\n",
      "GraphReader. Here, we would like to study the im-\n",
      "pact of extremely long context on our GraphReader.\n",
      "As shown in Table 3, compared with all baselines,\n",
      "our GraphReader not only consistently outperforms\n",
      "these methods across text lengths ranging from 16k\n",
      "to 256k tokens but also exhibits robustness with\n",
      "the expansion of context length. It indicates that\n",
      "our method is still effective in handling extremely\n",
      "long texts by graph exploration with limited context\n",
      "window LLMs. With the increase in the length of\n",
      "the input context, the performance of GPT-4-128k\n",
      "full-text reading degrades gradually. As a com-\n",
      "parison, our method achieves a performance gain\n",
      "of 10.53% relatively on LR-1 over GPT-4-128k\n",
      "full-text reading under 16k context length. WithDataset MethodResults(%)\n",
      "LR-1 LR-2 F1\n",
      "HotpotQAGraphReader 84.3 89.7 70.0\n",
      "w/o Rational Plan 81.7 87.7 63.8\n",
      "w/o Node Selection 66.0 71.7 54.1\n",
      "2WikiMultihopQAGraphReader 83.7 87.0 70.1\n",
      "w/o Rational Plan 81.3 86.0 65.4\n",
      "w/o Node Selection 65.3 68.7 49.7\n",
      "MuSiQueGraphReader 59.0 63.5 47.4\n",
      "w/o Rational Plan 56.0 61.0 42.4\n",
      "w/o Node Selection 35.0 38.5 25.2\n",
      "NarrativeQAGraphReader 65.0 80.0 29.8\n",
      "w/o Rational Plan 63.0 78.5 26.6\n",
      "w/o Node Selection 53.0 65.5 24.0\n",
      "Table 4: The results of our ablation study. “w/o Rational\n",
      "Plan” refers to removing the rational plan in the agent\n",
      "initialization stage, and “w/o Node Selection” denotes\n",
      "applying the random selection of initial nodes and neigh-\n",
      "bor nodes in graph exploration.\n",
      "the context length increasing to 128k, our method\n",
      "achieves a performance gain of 75.00% relatively\n",
      "over GPT-4-128k. This can be attributed to the fact\n",
      "that as the context length increases, the impact of\n",
      "the “lost in the middle” effect on GPT-4-128k be-\n",
      "comes progressively more severe. Secondly, we ob-\n",
      "serve that ReadAgent significantly underperforms\n",
      "our method in handling extremely long contexts.\n",
      "This is because the lack of detailed information\n",
      "about the content of each page can make page selec-\n",
      "tion very difficult for ReadAgent, especially when\n",
      "dealing with extremely long contexts. This further\n",
      "demonstrates that our method can effectively ad-\n",
      "dress the challenges of processing extremely long\n",
      "context with limited context window LLMs by ex-\n",
      "ploring graphs containing fine-grained information.\n",
      "4.3 Ablation study\n",
      "The Effect of Rational Plan In the graph explo-\n",
      "ration stage, we introduce a rational plan to help\n",
      "the agent analyze complex input questions step by\n",
      "step, guiding the agent in exploring the graph. To\n",
      "verify the effectiveness of the rational plan, we re-\n",
      "moved it during agent initialization and conducted\n",
      "experiments on four long-context QA benchmarks.\n",
      "Table 4 shows that the rational plan is effective in\n",
      "guiding the agent in node selection and exploration\n",
      "on the graph.\n",
      "The Effect of Node Selection We conduct ran-\n",
      "domly selecting initial nodes and neighbor nodes\n",
      "experiments to demonstrate the necessity of our sys-\n",
      "tem in selecting which nodes to visit based on rea-\n",
      "soning about the required information. As shown\n",
      "in Table 4, random selection results in a significant\n",
      "1 3 5 7707580859095100Average Scores (%)\n",
      "(a) 2WikiMultihopQA\n",
      "LR-1\n",
      "LR-2\n",
      "1 3 5 7406080100\n",
      "(b) NarrativeQA\n",
      "LR-1\n",
      "LR-2\n",
      "Initial Node Numbers Initial Node NumbersFigure 3: Performance of GraphReader with different\n",
      "initial node numbers on 2WikiMultihopQA and Narra-\n",
      "tiveQA. Results show the robustness of GraphReader\n",
      "towards different initial node numbers.\n",
      "performance drop, with an average decline of 18%.\n",
      "This demonstrates that GraphReader carefully con-\n",
      "siders node selection, leading to more reasonable\n",
      "and effective exploration.\n",
      "Impact of the Number of Initial Nodes We con-\n",
      "duct experiments with different initial node counts\n",
      "on multi-hop and single-hop QA datasets to as-\n",
      "sess the effect of the number of initial nodes on\n",
      "GraphReader’s performance. The results are shown\n",
      "in Figure 3. Increasing the number of nodes im-\n",
      "proves performance up to a certain point, with opti-\n",
      "mal performance at 5 initial nodes, which we set as\n",
      "the default. However, beyond this threshold, perfor-\n",
      "mance declines, especially in single-hop scenarios,\n",
      "likely due to increased noise from too many initial\n",
      "nodes.\n",
      "Impact of the Chunk Size We investigate the\n",
      "impact of chunk size Lon GraphReader’s perfor-\n",
      "mance. As shown in Figure 4, the best performance\n",
      "is achieved with L= 2k. When Lexceeds a cer-\n",
      "tain threshold, performance declines because larger\n",
      "chunks cause the model to overlook essential de-\n",
      "tails. Conversely, smaller chunks lead to more\n",
      "semantic truncation, hindering comprehension and\n",
      "accuracy in extracting atomic facts. Thus, we chose\n",
      "L= 2kas the default chunk size.\n",
      "4.4 Further Analysis\n",
      "Cost Analysis To evaluate the inference cost of\n",
      "our approach, we compare the average token con-\n",
      "sumption of ReadAgent and GraphReader for each\n",
      "question. As shown in Table 5, GraphReader uses\n",
      "only 1.08 times more tokens than ReadAgent but\n",
      "achieves over twice the performance improvement,\n",
      "demonstrating its superiority. Additionally, after\n",
      "the first round of document preprocessing, token\n",
      "consumption in subsequent exploration is signifi-1k 2k 4k 6k\n",
      "Chunk Size152025303540Average Scores (%)\n",
      "LR-1\n",
      "LR-2Figure 4: The impact of chunk size Lof GraphReader\n",
      "on the 256k length level of HotpotWikiQA-mixup.\n",
      "Method Avg. Ctx. #Tokens Avg. Cost #Tokens\n",
      "ReadAgent 358.3k 48.7k\n",
      "GraphReader 358.3k 52.8k\n",
      "Table 5: Comparison of token consumption per\n",
      "question between ReadAgent and GraphReader on\n",
      "HotpotWikiQA-mixup-256k, where “Avg. Ctx. #To-\n",
      "kens” refers to the average token number of the original\n",
      "dataset. The “Avg. Cost #Tokens” comprise both input\n",
      "tokens and output tokens during exploration.\n",
      "16k 32k 64k 128k 256k\n",
      "Input Length010203040506070Recall Scores (%)\n",
      "GraphReader\n",
      "ReadAgent\n",
      "GPT-4-128k (chunk w/ notes)Ada-002 (top-1)\n",
      "BM25 (top-1)\n",
      "Figure 5: Recall of supporting facts by different meth-\n",
      "ods on HotpotWikiQA-mixup.\n",
      "cantly reduced and much lower than the original\n",
      "dataset’s token count.\n",
      "Recall Rate Analysis To evaluate our method’s\n",
      "advantages in key information recall, we utilize\n",
      "GPT-4 to assess the recall of supporting facts on\n",
      "the HotpotWikiQA-mixup dataset. As shown in\n",
      "Figure 5, our model consistently outperforms other\n",
      "baseline methods, regardless of the input length.SourceRecall(%)\n",
      "SF-wise Sample-wise\n",
      "Atomic Facts 76.4 64.7\n",
      "Final Notebook 90.5 85.3\n",
      "Table 6: GraphReader’s recall performance at different\n",
      "granularities on HotpotQA. “SF-wise” refers to the gran-\n",
      "ularity of supporting facts, and “Sample-wise” refers to\n",
      "the granularity of sample evaluation.\n",
      "As context length increases from 16k to 256k, re-\n",
      "call of supporting facts declines across all meth-\n",
      "ods. However, GraphReader maintains around 60%\n",
      "recall at 256k context length, in contrast to the\n",
      "significant degradation in ReadAgent. This demon-\n",
      "strates GraphReader’s scalability and effectiveness\n",
      "in processing long contexts. Further details and\n",
      "evaluation prompts can be found in Appendix E.\n",
      "To further demonstrate the recall rate of\n",
      "GraphReader at different granularities, we calcu-\n",
      "late the recall rate of Supporting Facts andSample\n",
      "granularity respectively using the same method,\n",
      "detailed in the Appendix E. The granularity of sup-\n",
      "porting facts refers to the recall rate of all support-\n",
      "ing facts across the entire dataset. As for sample\n",
      "granularity, a sample is considered to be recalled\n",
      "only if all of its supporting facts are recalled. As\n",
      "shown in the Tabel 6, the recall for the final note-\n",
      "book is slightly higher than the recall of atomic\n",
      "facts, which indicates that our method is capable\n",
      "of extracting more valid information from chunks\n",
      "during the exploration, indirectly reflecting its in-\n",
      "telligence and effectiveness in exploration.\n",
      "5 Conclusion\n",
      "This paper introduces GraphReader, a graph-based\n",
      "agent designed to enhance the long-context capa-\n",
      "bilities of large language models. GraphReader\n",
      "organizes long texts into graph structures and em-\n",
      "ploys an autonomous agent to explore the graph,\n",
      "successfully establishing long-range dependencies\n",
      "within a relatively small 4k context window. Exper-\n",
      "iments demonstrate that GraphReader outperforms\n",
      "GPT-4 with a 128k input length across various\n",
      "long-context single-hop and multi-hop question-\n",
      "answering benchmarks.\n",
      "6 Limitations\n",
      "Firstly, GraphReader is constructed using an off-\n",
      "the-shelf GPT-4 API. Since it is close-sourced,\n",
      "there may be potential restrictions such as limits onQueries Per Second (QPS) and regional constraints.\n",
      "Therefore, future work will involve collecting data,\n",
      "training models, and making them open-source to\n",
      "contribute to the wider community. Secondly, the\n",
      "efficiency of the agent depends on its planning and\n",
      "reasoning capabilities. Future research will also\n",
      "explore enhancements of these features to improve\n",
      "the effectiveness of our method.\n",
      "References\n",
      "Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jia-\n",
      "heng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su,\n",
      "Tiezheng Ge, Bo Zheng, et al. 2024a. Mt-bench-101:\n",
      "A fine-grained benchmark for evaluating large lan-\n",
      "guage models in multi-turn dialogues. arXiv preprint\n",
      "arXiv:2402.14762 .\n",
      "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou,\n",
      "Jie Tang, Yuxiao Dong, and Juanzi Li. 2024b. Lon-\n",
      "galign: A recipe for long context alignment of large\n",
      "language models. arXiv preprint arXiv:2401.18058 .\n",
      "Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai\n",
      "Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-\n",
      "han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and\n",
      "Juanzi Li. 2023. Longbench: A bilingual, multitask\n",
      "benchmark for long context understanding. ArXiv ,\n",
      "abs/2308.14508.\n",
      "Howard Chen, Ramakanth Pasunuru, Jason Weston, and\n",
      "Asli Celikyilmaz. 2023a. Walking down the mem-\n",
      "ory maze: Beyond context limit through interactive\n",
      "reading. arXiv preprint arXiv:2310.05029 .\n",
      "Shouyuan Chen, Sherman Wong, Liangjian Chen, and\n",
      "Yuandong Tian. 2023b. Extending context window\n",
      "of large language models via positional interpolation.\n",
      "arXiv preprint arXiv:2306.15595 .\n",
      "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,\n",
      "Zhijian Liu, Song Han, and Jiaya Jia. 2023c. Lon-\n",
      "glora: Efficient fine-tuning of long-context large lan-\n",
      "guage models. arXiv preprint arXiv:2309.12307 .\n",
      "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\n",
      "bonell, Quoc V Le, and Ruslan Salakhutdinov.\n",
      "2019. Transformer-xl: Attentive language mod-\n",
      "els beyond a fixed-length context. arXiv preprint\n",
      "arXiv:1901.02860 .\n",
      "Michiel De Jong, Yury Zemlyanskiy, Nicholas FitzGer-\n",
      "ald, Fei Sha, and William Cohen. 2021. Mention\n",
      "memory: incorporating textual knowledge into trans-\n",
      "formers through entity mention attention. arXiv\n",
      "preprint arXiv:2110.06176 .\n",
      "Yiran Ding, Li Lyna Zhang, Chengruidong Zhang,\n",
      "Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,\n",
      "and Mao Yang. 2024. Longrope: Extending llm con-\n",
      "text window beyond 2 million tokens. arXiv preprint\n",
      "arXiv:2402.13753 .Thibault Févry, Livio Baldini Soares, Nicholas FitzGer-\n",
      "ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\n",
      "tities as experts: Sparse memory access with entity\n",
      "supervision. arXiv preprint arXiv:2004.07202 .\n",
      "Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Han-\n",
      "naneh Hajishirzi, Yoon Kim, and Hao Peng. 2024.\n",
      "Data engineering for scaling language models to 128k\n",
      "context. arXiv preprint arXiv:2402.10171 .\n",
      "Albert Gu and Tri Dao. 2023. Mamba: Linear-time\n",
      "sequence modeling with selective state spaces. arXiv\n",
      "preprint arXiv:2312.00752 .\n",
      "Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\n",
      "and Akiko Aizawa. 2020. Constructing A multi-hop\n",
      "QA dataset for comprehensive evaluation of reason-\n",
      "ing steps. In COLING , pages 6609–6625. Interna-\n",
      "tional Committee on Computational Linguistics.\n",
      "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n",
      "Zettlemoyer, and Mike Lewis. 2019. Generalization\n",
      "through memorization: Nearest neighbor language\n",
      "models. arXiv preprint arXiv:1911.00172 .\n",
      "Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-\n",
      "cient and effective passage search via contextualized\n",
      "late interaction over bert. In Proceedings of the 43rd\n",
      "International ACM SIGIR conference on research\n",
      "and development in Information Retrieval , pages 39–\n",
      "48.\n",
      "Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris\n",
      "Dyer, Karl Moritz Hermann, Gábor Melis, and Ed-\n",
      "ward Grefenstette. 2018. The narrativeqa reading\n",
      "comprehension challenge. Trans. Assoc. Comput.\n",
      "Linguistics , 6:317–328.\n",
      "LangChain-team. 2024. LangChain.\n",
      "Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John\n",
      "Canny, and Ian Fischer. 2024. A human-inspired\n",
      "reading agent with gist memory of very long contexts.\n",
      "arXiv preprint arXiv:2402.09727 .\n",
      "Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng\n",
      "Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing.\n",
      "2023. Chain-of-knowledge: Grounding large lan-\n",
      "guage models via dynamic knowledge adapting over\n",
      "heterogeneous sources. In The Twelfth International\n",
      "Conference on Learning Representations .\n",
      "Jerry Liu. 2024. LlamaIndex.\n",
      "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\n",
      "jape, Michele Bevilacqua, Fabio Petroni, and Percy\n",
      "Liang. 2024. Lost in the middle: How language mod-\n",
      "els use long contexts. Transactions of the Association\n",
      "for Computational Linguistics , 12:157–173.\n",
      "Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-\n",
      "yang Lin, Chuanqi Tan, Chang Zhou, and Jingren\n",
      "Zhou. 2023. # instag: Instruction tagging for analyz-\n",
      "ing supervised fine-tuning of large language models.\n",
      "InThe Twelfth International Conference on Learning\n",
      "Representations .Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and\n",
      "Shirui Pan. 2023. Reasoning on graphs: Faithful and\n",
      "interpretable large language model reasoning. arXiv\n",
      "preprint arXiv:2310.01061 .\n",
      "Tsendsuren Munkhdalai, Manaal Faruqui, and Sid-\n",
      "dharth Gopal. 2024. Leave no context behind:\n",
      "Efficient infinite context transformers with infini-\n",
      "attention. arXiv preprint arXiv:2404.07143 .\n",
      "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\n",
      "Long Ouyang, Christina Kim, Christopher Hesse,\n",
      "Shantanu Jain, Vineet Kosaraju, William Saunders,\n",
      "et al. 2021. Webgpt: Browser-assisted question-\n",
      "answering with human feedback. arXiv preprint\n",
      "arXiv:2112.09332 .\n",
      "Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\n",
      "ith Ringel Morris, Percy Liang, and Michael S Bern-\n",
      "stein. 2023. Generative agents: Interactive simulacra\n",
      "of human behavior. In Proceedings of the 36th An-\n",
      "nual ACM Symposium on User Interface Software\n",
      "and Technology , pages 1–22.\n",
      "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\n",
      "rico Shippole. 2023. Yarn: Efficient context window\n",
      "extension of large language models. arXiv preprint\n",
      "arXiv:2309.00071 .\n",
      "Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015.\n",
      "Yara parser: A fast and accurate dependency parser.\n",
      "Computing Research Repository , arXiv:1503.06733.\n",
      "Version 2.\n",
      "Stephen E. Robertson and Hugo Zaragoza. 2009. The\n",
      "probabilistic relevance framework: Bm25 and be-\n",
      "yond. Found. Trends Inf. Retr. , 3:333–389.\n",
      "Devendra Singh Sachan, Mike Lewis, Dani Yogatama,\n",
      "Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.\n",
      "2023. Questions are all you need to train a dense\n",
      "passage retriever. Transactions of the Association for\n",
      "Computational Linguistics , 11:600–616.\n",
      "Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh\n",
      "Khanna, Anna Goldie, and Christopher D Man-\n",
      "ning. 2024. Raptor: Recursive abstractive pro-\n",
      "cessing for tree-organized retrieval. arXiv preprint\n",
      "arXiv:2401.18059 .\n",
      "Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\n",
      "Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,\n",
      "and Jian Guo. 2023. Think-on-graph: Deep and\n",
      "responsible reasoning of large language model with\n",
      "knowledge graph. arXiv preprint arXiv:2307.07697 .\n",
      "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-\n",
      "Micke, and Mohit Iyyer. 2021. Do long-range lan-\n",
      "guage models actually use long-range context? arXiv\n",
      "preprint arXiv:2109.09115 .\n",
      "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\n",
      "and Ashish Sabharwal. 2022. Musique: Multi-\n",
      "hop questions via single-hop question composition.\n",
      "Trans. Assoc. Comput. Linguistics , 10:539–554.Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi\n",
      "Zhang, and Tyler Derr. 2024. Knowledge graph\n",
      "prompting for multi-document question answering.\n",
      "InProceedings of the AAAI Conference on Artificial\n",
      "Intelligence , volume 38, pages 19206–19214.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\n",
      "et al. 2022. Chain-of-thought prompting elicits rea-\n",
      "soning in large language models. Advances in neural\n",
      "information processing systems , 35:24824–24837.\n",
      "Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei\n",
      "Zhu, and Sujian Li. 2024a. Long context align-\n",
      "ment with short instructions and synthesized posi-\n",
      "tions. arXiv preprint arXiv:2405.03939 .\n",
      "Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhan-\n",
      "hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi\n",
      "Bai, Haibin Chen, Tiezheng Ge, et al. 2024b. Con-\n",
      "ceptmath: A bilingual concept-wise benchmark for\n",
      "measuring mathematical reasoning of large language\n",
      "models. arXiv preprint arXiv:2402.14660 .\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\n",
      "gio, William W. Cohen, Ruslan Salakhutdinov, and\n",
      "Christopher D. Manning. 2018. Hotpotqa: A dataset\n",
      "for diverse, explainable multi-hop question answer-\n",
      "ing. In EMNLP , pages 2369–2380. Association for\n",
      "Computational Linguistics.\n",
      "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\n",
      "Shafran, Karthik Narasimhan, and Yuan Cao. 2022.\n",
      "React: Synergizing reasoning and acting in language\n",
      "models. arXiv preprint arXiv:2210.03629 .\n",
      "Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang,\n",
      "Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu\n",
      "Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen\n",
      "Yan, and Yu Wang. 2024. Lv-eval: A balanced long-\n",
      "context benchmark with 5 length levels up to 256k.\n",
      "ArXiv , abs/2402.05136.\n",
      "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\n",
      "Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\n",
      "Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\n",
      "Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\n",
      "Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\n",
      "Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\n",
      "survey of large language models. abs/2303.18223.\n",
      "Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-\n",
      "hao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-\n",
      "cient context window extension of llms via positional\n",
      "skip-wise training. arXiv preprint arXiv:2309.10400 .A GraphReader Prompt\n",
      "Figure 6 illustrates the prompt used for Graph Con-\n",
      "struction. Figures 7 to 11 present the prompts em-\n",
      "ployed for Graph Exploration. Figure 12 shows the\n",
      "prompt used for Answer Reasoning.\n",
      "B LLM Rater Evaluation Details\n",
      "Given a question, a golden answer, and an answer\n",
      "to be evaluated, we utilize an LLM to assess the\n",
      "accuracy of the latter based on the question and\n",
      "correct answer. This involves two scores: LLM-\n",
      "Rating-1 (LR-1) and LLM-Rating-2 (LR-2), where\n",
      "LR-1 represents a strict scoring criterion, and LR-2\n",
      "is a more lenient one. Following the approach of\n",
      "ReadAgent, if either LLM Rater deems an answer\n",
      "correct, it is considered as such. If the strict scorer\n",
      "finds an answer incorrect while the lenient scorer\n",
      "deems it partially correct, we classify the answer as\n",
      "partially correct; otherwise, it is adjudged incorrect.\n",
      "The prompts used for evaluation are presented in\n",
      "Figure 13 and Figure 14 respectively.\n",
      "For the evaluation, we utilize GPT-4-128k as the\n",
      "LLM Rater, with the temperature set to 0.1.\n",
      "C Dataset\n",
      "Multi-hop QA Datasets HotpotQA features a\n",
      "collection of 2-hop questions directly authored by\n",
      "native speakers, based on two interconnected para-\n",
      "graphs. 2WikiMultihopQA is comprised of com-\n",
      "plex questions up to 5-hops in length, constructed\n",
      "through carefully designed templates to prevent the\n",
      "possibility of shortcut solutions.\n",
      "In the MuSiQue dataset, questions are intricately\n",
      "crafted starting from straightforward scenarios that\n",
      "require up to 4-hops reasoning. Annotators sub-\n",
      "sequently rephrase these with a dual purpose: to\n",
      "avoid shortcut answers and to maintain a natural\n",
      "linguistic quality. Each question within the orig-\n",
      "inal datasets is complemented by 2-4 supporting\n",
      "paragraphs, delivering evidence for simple one-step\n",
      "reasoning, alongside multiple paragraphs designed\n",
      "to serve as decoys.\n",
      "HotpotWikiQA-mixup originates from LV-Eval\n",
      "and employs a construction method known as a\n",
      "mixup. This method randomly blends support doc-\n",
      "uments with various distracting documents to gen-\n",
      "erate five different context lengths for a given QA\n",
      "pair, including 16k, 32k, 64k, 128k, and 256k. Due\n",
      "to the excessive length of this dataset, we select\n",
      "the first 50 data entries from each different context\n",
      "length for experimentation to control costs.Single-hop QA Datasets NarrativeQA is a\n",
      "dataset designed to test comprehension abilities\n",
      "for long documents, primarily sourced from movie\n",
      "scripts. As a single-hop QA dataset, the informa-\n",
      "tion required to answer its questions appears at a\n",
      "single location within the text.\n",
      "D Baseline Methods\n",
      "Full or Chunked Text Content For texts with\n",
      "fewer tokens than the LLM’s input window, we\n",
      "can input the text directly into the LLM to obtain\n",
      "an answer. We refer to this method as Full Text\n",
      "Read , with the specific prompt provided in Figure\n",
      "15. However, this approach is not applicable to\n",
      "texts exceeding the token limit of the LLM’s input\n",
      "window. In such cases, Lee et al. truncated the\n",
      "text to fit it into the LLM, but this method obvi-\n",
      "ously results in information loss. We propose a\n",
      "method that does not lose information, offering a\n",
      "better comparison. This method involves dividing\n",
      "the entire text into chunks (using the same chunk-\n",
      "ing method as GraphReader) and then having the\n",
      "LLM read these chunks sequentially according to\n",
      "the text order, thus enabling the handling of overly\n",
      "long texts with a limited input window. During\n",
      "the reading process, there are two main strategies:\n",
      "Chunk Read andChunk Read with Notes . In the\n",
      "Chunk Read approach, the LLM only sees the cur-\n",
      "rent chunk during each reading, which is suitable\n",
      "for single-hop QA tasks. In the Chunk Read with\n",
      "Notes approach, the LLM can summarize useful\n",
      "information from the current chunk and provide it\n",
      "to the subsequent reading process, which is suitable\n",
      "for multi-hop QA tasks.\n",
      "In the experiment, we divide the chunks in the\n",
      "same way as GraphReader, and the maximum\n",
      "length of the chunk is set to 2k. The specific\n",
      "prompts are in Figure 16 and 17 respectively.\n",
      "Retrieval-Augmented Generation (RAG) RAG\n",
      "is a commonly used approach for addressing long-\n",
      "text problems. In this work, we compare the tra-\n",
      "ditional RAG method, including retrieval methods\n",
      "based on Okapi BM25 (Robertson and Zaragoza,\n",
      "2009) and the OpenAI API embedding model\n",
      "(text-embedding-ada-002). Specifically, we first\n",
      "split the text into chunks in the same method as\n",
      "GraphReader, then use the aforementioned meth-\n",
      "ods to calculate the relevance scores between the\n",
      "question and these chunks, and finally input the top-\n",
      "nchunks with the highest relevance scores together\n",
      "with the question for the LLM to answer.To ensure a fair comparison, we control the input\n",
      "window to 4k in the experiments. Specifically, in\n",
      "order to fill the input window as much as possi-\n",
      "ble, we set the maximum length of the chunk to\n",
      "38k when selecting the top-1 chunk for answering;\n",
      "when opting for the top-3 chunks, we set the max-\n",
      "imum length of each chunk to 1k. The specific\n",
      "prompt can be found in Figure 18.\n",
      "ReadAgent We also compared our method with\n",
      "similar approaches for handling long texts with\n",
      "small input windows, such as ReadAgent (Lee\n",
      "et al., 2024). ReadAgent is a method that segments\n",
      "long texts and generates gist memories, which are\n",
      "then looked up to search for information in order to\n",
      "answer questions. In the experiments, for datasets\n",
      "from LongBench, we adopted the default hyperpa-\n",
      "rameters declared in the ReadAgent paper, specifi-\n",
      "cally a max_words of 600 and min_words of 280\n",
      "when splitting pages. For HotpotWikiQA-mixup\n",
      "from LV-Eval, we scaled these two hyperparam-\n",
      "eters using the same approach as in the ReadA-\n",
      "gent paper. Specifically, for datasets with lengths\n",
      "of 256k and 128k, we used max_words=10000\n",
      "and min_words=2000; for those with lengths of\n",
      "64k, 32k and 16k, we used max_words=5000 and\n",
      "min_words=1000. At the same time, we employed\n",
      "the ReadAgent-S method, which ReadAgent claims\n",
      "to be the most effective, reading the pages in se-\n",
      "quence. Additionally, we allowed reading up to 5\n",
      "pages (Look up 1-5 pages).\n",
      "EEvaluation Recall for Supporting Facts\n",
      "We evaluate the recall rate of supporting facts for\n",
      "different methods using GPT-4-128k, with the tem-\n",
      "perature set to 0.1. Figure 19 shows the specific\n",
      "evaluation prompt.\n",
      "For GraphReader, we evaluate the memory\n",
      "recorded in the final notebook. For ReadAgent,\n",
      "the evaluation focused on the final text segments\n",
      "reviewed. In the case of Chunk Read with Notes,\n",
      "we evaluate both the memory and the chunk read at\n",
      "the time of the final answer; for the RAG methods,\n",
      "we assess the retrieved chunks.\n",
      "F The Analysis of Function Calls\n",
      "To verify the rationality and utility of agent actions\n",
      "under various circumstances of GraphReader, we\n",
      "made statistics on its function calls at each stage\n",
      "across two datasets. From the statistical results in\n",
      "Table 7, it can be observed that each piece of datawill perform an average of 3 to 4 actions, corre-\n",
      "sponding to the average number of function calls\n",
      "in the table. This indicates the effectiveness of\n",
      "the graph we constructed, with GraphReader being\n",
      "able to swiftly locate key information while mini-\n",
      "mizing resource usage. Furthermore, each action\n",
      "has a certain probability of being chosen, justifying\n",
      "the rationality of the action set. Among them, the\n",
      "most commonly used action on multi-hop QA tasks\n",
      "is to read neighbor nodes, and the most common\n",
      "action on single-hop QA tasks is to read chunks.\n",
      "This difference is caused by the fact that multi-hop\n",
      "questions need to gather information contained by\n",
      "multiple nodes to answer questions, while single-\n",
      "hop data sets often require only one atomic fact.\n",
      "G Statistics of Graph\n",
      "The statistics of graphs from various datasets are\n",
      "presented in Table 8. For longer texts, there tends\n",
      "to be a higher average number of nodes and atomic\n",
      "facts. After normalization, each node has an aver-\n",
      "age of about 10 neighbor nodes. This is because\n",
      "the number of key elements occurring simultane-\n",
      "ously in each atomic fact is generally of this mag-\n",
      "nitude. Furthermore, the aggregation of similar\n",
      "nodes caused by normalization results in a slight\n",
      "increase in the number of neighboring nodes.\n",
      "On average, each node is associated with about\n",
      "2 atomic facts, and the average number of atomic\n",
      "facts in the node with the most atomic facts in\n",
      "each graph ranges from 15 to 50, indicating a rela-\n",
      "tively even distribution of atomic facts. The max-\n",
      "imum average number of atomic facts is found\n",
      "in NarrativeQA, a possible explanation being that\n",
      "NarrativeQA is mainly derived from movie scripts,\n",
      "where characters, such as the protagonist, appear\n",
      "frequently throughout the text, thus including a\n",
      "larger number of atomic facts.\n",
      "H GraphReader Example\n",
      "This section presents a case study of the\n",
      "GraphReader workflow. Figure 20 displays the\n",
      "posed question alongside the answer and pertinent\n",
      "supporting passages. Subsequently, Figure 21 de-\n",
      "lineates the methodology for constructing the graph.\n",
      "Figure 22 further elaborates on the initialization of\n",
      "a pre-planned rational path by GraphReader and\n",
      "the selection of initial nodes. Figure 23 illustrates\n",
      "the sequence of function invocations during the ex-\n",
      "ploration phase. Finally, Figure 24 showcases how\n",
      "GraphReader formulates the answer by leveraging\n",
      "the insights obtained through exploration.Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%)\n",
      "HotpotQA 3.0Exploring Atomic Facts 42.0read_chunk 46.5\n",
      "stop_and_read_neighbor 53.5\n",
      "Exploring Chunks 31.9search_more 12.1\n",
      "read_previous_chunk 21.1\n",
      "read_subsequent_chunk 22.9\n",
      "termination 43.9\n",
      "Exploring Neighbors 26.1read_neighbor_node 35.5\n",
      "termination 65.5\n",
      "2WikiMultihopQA 3.2Exploring Atomic Facts 40.4read_chunk 48.6\n",
      "stop_and_read_neighbor 51.4\n",
      "Exploring Chunks 34.5search_more 14.5\n",
      "read_previous_chunk 25.1\n",
      "read_subsequent_chunk 23.3\n",
      "termination 37.1\n",
      "Exploring Neighbors 25.1read_neighbor_node 37.3\n",
      "termination 62.7\n",
      "MuSiQue 3.5Exploring Atomic Facts 40.0read_chunk 41.3\n",
      "stop_and_read_neighbor 58.7\n",
      "Exploring Chunks 31.2search_more 19.1\n",
      "read_previous_chunk 26.6\n",
      "read_subsequent_chunk 25.7\n",
      "termination 28.6\n",
      "Exploring Neighbors 28.8read_neighbor_node 40.1\n",
      "termination 59.9\n",
      "NarrativeQA 3.9Exploring Atomic Facts 32.5read_chunk 64.5\n",
      "stop_and_read_neighbor 35.5\n",
      "Exploring Chunks 54.3search_more 4.1\n",
      "read_previous_chunk 35.3\n",
      "read_subsequent_chunk 32.6\n",
      "termination 28.0\n",
      "Exploring Neighbors 13.2read_neighbor_node 51.4\n",
      "termination 48.6\n",
      "Table 7: Statistics of function calls on MuSiQue and NarrativeQA.\n",
      "datasetSample Dimension Sample & Node Dimension\n",
      "node num atomic facts num neighbor node num atomic facts num\n",
      "avg. max avg. max avg. avg. avg. max avg. avg. avg. max\n",
      "HotpotQA 583.8 1945.0 244.0 645.0 10.1 263.1 2.1 17.8\n",
      "2WikiMultihopQA 515.8 1691.0 217.7 545.0 9.2 215.7 2.1 17.0\n",
      "MusiQue 1029.4 2142.0 419.9 586.0 9.3 253.4 2.1 15.6\n",
      "NarrativeQA 966.0 3110.0 515.5 1296.0 10.3 652.6 2.3 50.0\n",
      "HotpotWikiQA-mixup16k 1741.6 3822.0 749.7 1043.0 9.4 231.0 2.2 17.1\n",
      "32k 2827.3 5086.0 1257.4 1694.0 9.8 263.3 2.2 29.3\n",
      "64k 5054.1 8918.0 2360.0 3015.0 10.4 227.2 2.3 17.1\n",
      "128k 8828.5 14592.0 4437.9 5182.0 11.1 302.0 2.4 19.2\n",
      "256k 14853.3 24981.0 8632.8 9478.0 12.2 427.6 2.5 27.8\n",
      "Table 8: Graph statistical data. Under the Sample dimension, “avg.” indicates the average number of nodes in each\n",
      "graph, and “max” refers to the largest node count across all graphs. The same logic applies to atomic facts num. In\n",
      "the Sample & Node dimensions, “avg. avg.” denotes the average of the average neighbor node counts per graph,\n",
      "and“avg. max” means the average of the maximum neighbor node counts per graph. This approach is also used for\n",
      "counting atomic facts num.You are now an intelligent assistant tasked with meticulously extracting both key elements and\n",
      "atomic facts from a long text.\n",
      "1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g.,\n",
      "actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative.\n",
      "2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include\n",
      "propositions, theories, existences, concepts, and implicit elements like logic, causality, event\n",
      "sequences, interpersonal relationships, timelines, etc.\n",
      "Requirements:\n",
      "#####\n",
      "1. Ensure that all identified key elements are reflected within the corresponding atomic facts.\n",
      "2. You should extract key elements and atomic facts comprehensively, especially those that are\n",
      "important and potentially query-worthy and do not leave out details.\n",
      "3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He,\n",
      "She to actual names).\n",
      "4. Ensure that the key elements and atomic facts you extract are presented in the same language as\n",
      "the original text (e.g., English or Chinese).\n",
      "5. You should output a total of key elements and atomic facts that do not exceed 1024 tokens.\n",
      "6. Your answer format for each line should be: [Serial Number ],[Atomic Facts ],[List of Key\n",
      "Elements, separated with ‘|’ ]\n",
      "#####\n",
      "Example:\n",
      "#####\n",
      "User:\n",
      "One day, a father and his little son ......\n",
      "Assistant:\n",
      "1. One day, a father and his little son were going home. | father | little son | going home\n",
      "2. ......\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 6: The prompt for key elements and atomic facts extraction.As an intelligent assistant, your primary objective is to answer the question by gathering\n",
      "supporting facts from a given article. To facilitate this objective, the first step is to make\n",
      "a rational plan based on the question. This plan should outline the step-by-step process to\n",
      "resolve the question and specify the key information required to formulate a comprehensive answer.\n",
      "Example:\n",
      "#####\n",
      "User: Who had a longer tennis career, Danny or Alice?\n",
      "Assistant: In order to answer this question, we first need to find the length of Danny’s\n",
      "and Alice’s tennis careers, such as the start and retirement of their careers, and then compare the\n",
      "two.\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 7: The prompt for rational plan.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
      "comprising the following elements:\n",
      "1. Text Chunks: Chunks of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "Your current task is to check a list of nodes, with the objective of selecting the most rele-\n",
      "vant initial nodes from the graph to efficiently answer the question. You are given the question, the\n",
      "rational plan, and a list of node key elements. These initial nodes are crucial because they are the\n",
      "starting point for searching for relevant information.\n",
      "Requirements:\n",
      "#####\n",
      "1. Once you have selected a starting node, assess its relevance to the potential answer by assigning\n",
      "a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer,\n",
      "whereas a score of 0 suggests minimal relevance.\n",
      "2. Present each chosen starting node in a separate line, accompanied by its relevance score. Format\n",
      "each line as follows: Node: [Key Element of Node], Score: [Relevance Score].\n",
      "3. Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.\n",
      "4. In the user’s input, each line constitutes a node. When selecting the starting node, please make\n",
      "your choice from those provided, and refrain from fabricating your own. The nodes you output\n",
      "must correspond exactly to the nodes given by the user, with identical wording.\n",
      "#####\n",
      "Example:\n",
      "#####\n",
      "User:\n",
      "Question: {QUESTION}\n",
      "Plan: {RATIONAL PLAN}\n",
      "Nodes: {LIST OF KEY ELEMENTS}\n",
      "Assistant:{LIST OF SELECTED NODES}\n",
      "#####\n",
      "Finally, I emphasize again that you need to select the starting node from the given Nodes, and\n",
      "it must be consistent with the words of the node you selected. Please strictly follow the above\n",
      "format. Let’s begin.\n",
      "Figure 8: The prompt for initial node selection.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
      "comprising the following elements:\n",
      "1. Text Chunks: Chunks of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "Your current task is to check a node and its associated atomic facts, with the objective of\n",
      "determining whether to proceed with reviewing the text chunk corresponding to these atomic facts.\n",
      "Given the question, the rational plan, previous actions, notebook content, and the current node’s\n",
      "atomic facts and their corresponding chunk IDs, you have the following Action Options:\n",
      "#####\n",
      "1. read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic\n",
      "fact may hold the necessary information to answer the question. This will allow you to access\n",
      "more complete and detailed information.\n",
      "2. stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable\n",
      "information.\n",
      "#####\n",
      "Strategy:\n",
      "#####\n",
      "1. Reflect on previous actions and prevent redundant revisiting nodes or chunks.\n",
      "2. You can choose to read multiple text chunks at the same time.\n",
      "3. Atomic facts only cover part of the information in the text chunk, so even if you feel that the\n",
      "atomic facts are slightly relevant to the question, please try to read the text chunk to get more\n",
      "complete information.\n",
      "#####\n",
      "Response format:\n",
      "#####\n",
      "*Updated Notebook*: First, combine your current notebook with new insights and findings about\n",
      "the question from current atomic facts, creating a more complete version of the notebook that\n",
      "contains more valid information.\n",
      "*Rationale for Next Action*: Based on the given question, the rational plan, previous actions, and\n",
      "notebook content, analyze how to choose the next action.\n",
      "*Chosen Action*: read_chunk(List[ID]) or stop_and_read_neighbor(). (Here is the Action you\n",
      "selected from Action Options, which is in the form of a function call as mentioned before. The\n",
      "formal parameter in parentheses should be replaced with the actual parameter.)\n",
      "#####\n",
      "Finally, it is emphasized again that even if the atomic fact is only slightly relevant to the\n",
      "question, you should still look at the text chunk to avoid missing information. You should only\n",
      "choose stop_and_read_neighbor() when you are very sure that the given text chunk is irrelevant to\n",
      "the question. Please strictly follow the above format. Let’s begin.\n",
      "Figure 9: The prompt for exploring atomic facts.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
      "following elements:\n",
      "1. Text Chunks: Segments of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "Your current task is to assess a specific text chunk and determine whether the available information\n",
      "suffices to answer the question. Given the question, rational plan, previous actions, notebook\n",
      "content, and the current text chunk, you have the following Action Options:\n",
      "#####\n",
      "1. search_more(): Choose this action if you think that the essential information necessary to\n",
      "answer the question is still lacking.\n",
      "2. read_previous_chunk(): Choose this action if you feel that the previous text chunk contains\n",
      "valuable information for answering the question.\n",
      "3. read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains\n",
      "valuable information for answering the question.\n",
      "4. termination(): Choose this action if you believe that the information you have currently obtained\n",
      "is enough to answer the question. This will allow you to summarize the gathered information and\n",
      "provide a final answer.\n",
      "#####\n",
      "Strategy:\n",
      "#####\n",
      "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
      "2. You can only choose one action.\n",
      "#####\n",
      "Response format:\n",
      "#####\n",
      "*Updated Notebook*: First, combine your previous notes with new insights and findings about the\n",
      "question from current text chunks, creating a more complete version of the notebook that contains\n",
      "more valid information.\n",
      "*Rationale for Next Action*: Based on the given question, rational plan, previous actions, and\n",
      "notebook content, analyze how to choose the next action.\n",
      "*Chosen Action*: search_more() or read_previous_chunk() or read_subsequent_chunk() or\n",
      "termination(). (Here is the Action you selected from Action Options, which is in the form of a\n",
      "function call as mentioned before. The formal parameter in parentheses should be replaced with\n",
      "the actual parameter.)\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 10: The prompt for exploring chunks.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
      "following elements:\n",
      "1. Text Chunks: Segments of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "Your current task is to assess all neighboring nodes of the current node, with the objec-\n",
      "tive of determining whether to proceed to the next neighboring node. Given the question, rational\n",
      "plan, previous actions, notebook content, and the neighbors of the current node, you have the\n",
      "following Action Options:\n",
      "#####\n",
      "1. read_neighbor_node(key element of node): Choose this action if you believe that any of the\n",
      "neighboring nodes may contain information relevant to the question. Note that you should focus\n",
      "on one neighbor node at a time.\n",
      "2. termination(): Choose this action if you believe that none of the neighboring nodes possess\n",
      "information that could answer the question.\n",
      "#####\n",
      "Strategy:\n",
      "#####\n",
      "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
      "2. You can only choose one action. This means that you can choose to read only one neighbor\n",
      "node or choose to terminate.\n",
      "#####\n",
      "Response format:\n",
      "#####\n",
      "*Rationale for Next Action*: Based on the given question, rational plan, previous actions, and\n",
      "notebook content, analyze how to choose the next action.\n",
      "*Chosen Action*: read_neighbor_node(neighbor_node) or termination(). (Here is the Action you\n",
      "selected from Action Options, which is in the form of a function call as mentioned before. The\n",
      "formal parameter in parentheses should be replaced with the actual parameter.)\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 11: The prompt for exploring neighbors.As an intelligent assistant, your primary objective is to answer questions based on information\n",
      "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
      "following elements:\n",
      "1. Text Chunks: Segments of the original text.\n",
      "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
      "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
      "facts derived from different text chunks.\n",
      "You have now explored multiple paths from various starting nodes on this graph, record-\n",
      "ing key information for each path in a notebook.\n",
      "Your task now is to analyze these memories and reason to answer the question.\n",
      "Strategy:\n",
      "#####\n",
      "1. You should first analyze each notebook content before providing a final answer.\n",
      "2. During the analysis, consider complementary information from other notes and employ a\n",
      "majority voting strategy to resolve any inconsistencies.\n",
      "3. When generating the final answer, ensure that you take into account all available information.\n",
      "#####\n",
      "Example:\n",
      "#####\n",
      "User:\n",
      "Question: Who had a longer tennis career, Danny or Alice?\n",
      "Notebook of different exploration paths:\n",
      "1. We only know that Danny’s tennis career started in 1972 and ended in 1990, but we don’t know\n",
      "the length of Alice’s career.\n",
      "2. ......\n",
      "Assistant:\n",
      "Analyze:\n",
      "The summary of search path 1 points out that Danny’s tennis career is 1990-1972=18 years.\n",
      "Although it does not indicate the length of Alice’s career, the summary of search path 2 finds this\n",
      "information, that is, the length of Alice’s tennis career is 15 years. Then we can get the final\n",
      "answer, that is, Danny’s tennis career is longer than Alice’s.\n",
      "Final answer:\n",
      "Danny’s tennis career is longer than Alice’s.\n",
      "#####\n",
      "Please strictly follow the above format. Let’s begin.\n",
      "Figure 12: The prompt for answer reasoning.After reading some text, John was given the following question about the text:\n",
      "{QUESTION TEXT}\n",
      "John’s answer to the question was:\n",
      "{MODEL RESPONSE TEXT}\n",
      "The ground truth answer was:\n",
      "{REFERENCE RESPONSE TEXT}\n",
      "Does John’s answer agree with the ground truth answer?\n",
      "Please answer \"Yes\" or \"No\".\n",
      "Figure 13: The prompt for LLM-Rating-1.\n",
      "After reading some text, John was given the following question about the text:\n",
      "{QUESTION TEXT}\n",
      "John’s answer to the question was:\n",
      "{MODEL RESPONSE TEXT}\n",
      "The ground truth answer was:\n",
      "{REFERENCE RESPONSE TEXT}\n",
      "Does John’s answer agree with the ground truth answer?\n",
      "Please answer “Yes”, “Yes, partially”, or “No”. If John’s response has any overlap with the ground\n",
      "truth answer, answer “Yes, partially”. If John’s response contains the ground truth answer, answer\n",
      "“Yes”. If John’s response is more specific than the ground truth answer, answer “Yes”.\n",
      "Figure 14: The prompt for LLM-Rating-2.\n",
      "Please read the passage below and answer the question based on the passage.\n",
      "Passage:\n",
      "{PASSAGE TEXT}\n",
      "Question:\n",
      "{QUESTION TEXT}\n",
      "Now please answer this question based on the passage content.\n",
      "Figure 15: The prompt for Full Text Read.\n",
      "Please read the text chunks below and answer the question.\n",
      "Text chunks:\n",
      "{CHUNKED PASSAGE TEXT}\n",
      "Question:\n",
      "{QUESTION TEXT}\n",
      "If you think you can answer the question based on the above text chunks please output\n",
      "[answerable] and then output your answer.\n",
      "Otherwise, if there is not enough information to answer the question, please output:\n",
      "[unanswerable]\n",
      "Figure 16: The prompt for Chunk Read.Please read the text chunk below and answer the questions based on your previous summary.\n",
      "Text chunk:\n",
      "{CHUNKED PASSAGE TEXT}\n",
      "Your previous summary:\n",
      "{SUMMARY TEXT}\n",
      "Question:\n",
      "{QUESTION TEXT}\n",
      "If the above text chunk has information that can help answer the question, please extract\n",
      "the effective information, output [summary], and then output the refined information. Please note\n",
      "that it must be brief.\n",
      "If you can answer the question based on the above information, please output [answerable] and\n",
      "then output your answer.\n",
      "Otherwise, if there is insufficient information to answer the question, please output [unanswerable].\n",
      "Figure 17: The prompt for Chunk Read with Note.\n",
      "Please read the text chunk below and answer the question.\n",
      "Text chunks:\n",
      "{RETRIEVED PASSAGE TEXT}\n",
      "Question:\n",
      "{QUESTION TEXT}\n",
      "Now please answer this question based on the text chunks.\n",
      "Figure 18: The prompt for RAG.Now you are an intelligent assistant. Given a text, a question, and xsupporting facts that can\n",
      "answer the question, please determine how many supporting facts the text covers.\n",
      "Requirements:\n",
      "#####\n",
      "1. It’s possible that not all supporting facts are needed to answer the question, so you’ll need to\n",
      "analyze the supporting facts to determine which supporting facts are actually needed, and then\n",
      "determine whether those supporting facts are covered. Supporting facts that are not really needed\n",
      "are discarded, and you do not need to judge whether they are covered. So the number of real\n",
      "supporting facts is t (0 < t <= x).\n",
      "2. A supporting fact has some valid information that helps answer the question. When the text\n",
      "provides this part of the valid information, it is considered to have covered the supporting fact,\n",
      "even if the text does not provide all the information supporting the fact.\n",
      "3. The number of covered items in your output should be between 0 and t (including 0 and t).\n",
      "4. Please analyze and reason first, and then output the final result.\n",
      "#####\n",
      "Example:\n",
      "#####\n",
      "{EXAMPLE}\n",
      "#####\n",
      "Please note that you should follow: 0 <= Number of recalls <= True number of support-\n",
      "ing facts <= Number of supporting facts.\n",
      "Please output according to the example format. Now let’s start.\n",
      "Figure 19: The prompt for evaluating recall.\n",
      "Question &Answer\n",
      "Question What is the name of the castle in the city where the performer of Never Too Loud was\n",
      "formed?\n",
      "Answer Casa Loma\n",
      "Supporting Passages\n",
      "1.Never Too Loud is the fourth studio album by Canadian hard rock band Danko Jones.\n",
      "It was recorded at Studio 606 in Los Angeles, with the producer Nick Raskulinecz.\n",
      "2.Danko Jones is a Canadian hard rock trio from Toronto. The band consists of Danko\n",
      "Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums). The band’s music\n",
      "includes elements of hard rock and punk and they are known for their energetic live shows.\n",
      "3.Casa Loma (improper Spanish for \"Hill House\") is a Gothic Revival castle-style mansion\n",
      "and garden in midtown Toronto, Ontario, Canada, that is now a historic house museum\n",
      "and landmark. It was constructed from 1911 to 1914 as a residence for financier Sir Henry\n",
      "Pellatt. The architect was E. J. Lennox, who designed several other city landmarks.\n",
      "Figure 20: GraphReader Example(Question and Annotations) . We provide an example question with its answer,\n",
      "along with the supporting passages for this question. This is a typical 3-hop question where we need to gather\n",
      "information and reason step-by-step to arrive at the answer.Graph Construction: Extract Atomic Facts And Key Elements\n",
      "Atomic Facts\n",
      "1. \"Never Too Loud\" is the fourth studio album by Canadian hard rock band Danko Jones.\n",
      "2. Danko Jones is a Canadian hard rock trio from Toronto.\n",
      "3. Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario,\n",
      "Canada.\n",
      "......\n",
      "Key Elements\n",
      "1. [Never Too Loud, studio album, Canadian, hard rock band, Danko Jones]\n",
      "2. [Danko Jones, Canadian, hard rock trio, Toronto]\n",
      "3. [Casa Loma, Gothic Revival, castle-style mansion, Toronto, Canada]\n",
      "......\n",
      "Graph Construction: Normalize And Link Nodes\n",
      "Never Too Loudstudio albumcastle-style mansionTorontoDankoJonesGothic RevivalCasa LomaCanadianCanadaCanadahard rock bandhard rock triohard rock band\n",
      "Canadahard rock bandAtomicFacts1AtomicFacts2AtomicFacts3NormalizeLink\n",
      "Figure 21: GraphReader Example(Graph Construction) . The atomic facts and key elements extracted from the\n",
      "passage correspond to each other, after which the latter are normalized to serve as nodes. Finally, links are formed\n",
      "based on the co-occurrence relationships of the nodes within the atomic facts.Agent Initialization: Pre-plan And Select Initial Nodes\n",
      "Rational Plan To answer the question, we need to identify the performer or band associated\n",
      "with \"Never Too Loud\", determine the city where they were formed, and then find out the name of\n",
      "any notable castle in that city.\n",
      "Initial Node Never Too Loud\n",
      "Agent Initialization: Visual Representation\n",
      "Never Too Loudstudio albumcastle-style mansionTorontoDankoJonesGothic RevivalCasa Loma\n",
      "Canadahard rock bandAtomicFacts1AtomicFacts2AtomicFacts3InitialNodeNever Too LoudNotebook\n",
      "RationalPlan\n",
      "Figure 22: GraphReader Example(Agent Initialization) . Initially, a rational plan is formulated in response to the\n",
      "question, guiding further exploration; subsequently, the plan dictates the selection of the initial node from all nodes.Exploration: Function Call Process\n",
      "Exploring Atomic Facts Node: Never Too Loud; [Atomic Fact 1 from Chunk ID-6]\n",
      "Call Function read_chunk(ID-6) .\n",
      "Exploring Chunks Realized the performer of Never Too Loud is Danko Jones.\n",
      "Call Function: search_more\n",
      "Exploring Neighbors Node: Never Too Loud; Neighbor Nodes: [hard rock band, Danko Jones,\n",
      "studio album, Canada]\n",
      "Call Function read_neighbor_node(Danko Jones)\n",
      "Exploring Atomic Facts Node: Danko Jones; [Atomic Fact 1 from Chunk ID-6, Atomic Fact 2\n",
      "from Chunk ID-9].\n",
      "Call Function read_chunk(ID-9) .\n",
      "Exploring Chunks Realized Danko Jones band is a band from Toronto, Canada.\n",
      "Call Function: search_more\n",
      "Exploring Neighbors Node: Danko Jones; Neighbor Nodes: [hard rock band, Never Too Lou,\n",
      "studio album, Canada, Toronto]\n",
      "Call Function read_neighbor_node(Toronto)\n",
      "Exploring Atomic Facts Node: Toronto; [Atomic Fact 2 from Chunk ID-9, Atomic Fact 3 from\n",
      "Chunk ID-13].\n",
      "Call Function read_chunk(ID-13) .\n",
      "Exploring Chunks Realized the castle mentioned in the text in Toronto is Casa Loma.\n",
      "Call Function: termination\n",
      "Exploration: Visual Representation\n",
      "Never Too Loudstudio albumcastle-style mansionTorontoDankoJonesGothic RevivalCasa Loma\n",
      "Canadahard rock bandAtomicFacts1AtomicFacts2AtomicFacts3InitialNodeNever Too LoudNotebook\n",
      "RationalPlan\n",
      "Figure 23: GraphReader Example(Exploration) . GraphReader begins from the initial node, guided by the rational\n",
      "plan, carrying a notebook that records memory, gradually collecting information to answer the question.Answer Reasoning: Response Based on the Notebook\n",
      "Question What is the name of the castle in the city where the performer of Never Too Loud was\n",
      "formed?\n",
      "Memory from the notebook The performer of Never Too Loud is Danko Jones, which is a band\n",
      "from Toronto, Canada. The text mentions that the castle in Toronto is Casa Loma.\n",
      "GraphReader answer Casa Loma\n",
      "Figure 24: GraphReader Example(Answer Reasoning) . Ultimately, GraphReader answers the question based on\n",
      "the notebook recorded during the exploration process.\n"
     ]
    }
   ],
   "source": [
    "from autogen import retrieve_utils\n",
    "\n",
    "paper = retrieve_utils.extract_text_from_pdf(\"data/paper.pdf\")\n",
    "print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 09-10 23:55:24] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    \"chatbot\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4o-mini\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
    "    function_map=None,  # No registered functions, by default it is None.\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_atom_prompt = \"\"\"\n",
    "You are now an intelligent assistant tasked with meticulously extracting both key elements and atomic facts from a long text. 1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g., actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative. 2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include propositions, theories, existences, concepts, and implicit elements like logic, causality, event sequences, interpersonal relationships, timelines, etc.  Requirements: ##### 1. Ensure that all identified key elements are reflected within the corresponding atomic facts. 2. You should extract key elements and atomic facts comprehensively, especially those that are important and potentially query-worthy and do not leave out details. 3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He, She to actual names). 4. Ensure that the key elements and atomic facts you extract are presented in the same language as the original text (e.g., English or Chinese). 5. You should output a total of key elements and atomic facts that do not exceed 1024 tokens. 6. Your answer format for each line should be: [Serial Number], [Atomic Facts], [List of Key Elements, separated with ‘|’] #####  Example: ##### User: One day, a father and his little son ......  Assistant: 1. One day, a father and his little son were going home. | father | little son | going home 2. ...... #####  Please strictly follow the above format. Let’s begin.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are several potential flaws in the methods of the paper \"GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models\":\n",
      "\n",
      "1. **Dependence on Graph Construction**: The effectiveness of GraphReader relies heavily on the quality and completeness of the graph constructed from the text. If key elements or atomic facts are missed during the extraction, the subsequent exploration and reasoning may lead to incomplete or incorrect answers. The method describes normalization and linking processes, but these may not always guarantee that the most relevant information is represented in the graph.\n",
      "\n",
      "2. **Chunking Limitations**: The authors divide long texts into chunks while preserving paragraph structures. However, this approach may not always capture the necessary context for complex multi-hop questions. Key information might be divided across adjacent chunks, leading to incomplete understanding or loss of critical relationships between facts. Additionally, the choice of maximum chunk size (2k tokens) may not be optimal and could affect the extraction of atomic facts.\n",
      "\n",
      "3. **Scalability Concerns**: While the method claims to improve performance for long-context tasks, there is no discussion on how the performance scales with extremely large inputs (e.g., 1M tokens or more), which may still overwhelm the graph's structure or cause inefficiencies in exploration.\n",
      "\n",
      "4. **Rational Plan and Initial Node Selection**: The reliance on a rational plan for node selection and exploration may introduce bias in the exploration process. If the initial nodes selected are not the most relevant, the exploration could deviate from critical information. The process relies on effective reasoning capabilities of the agent, and any deficiencies here may hinder performance.\n",
      "\n",
      "5. **Limited Exploration**: The method proposes a coarse-to-fine exploration strategy. However, if the agent fails to access relevant neighboring nodes or does not explore all potential paths due to misjudgment of relevance, it may miss critical supporting facts. The decision-making process during exploration may not consider all relevant factors, leading to suboptimal information retrieval.\n",
      "\n",
      "6. **Evaluation Metrics and Bias**: The results depend on various evaluation metrics (F1, EM, etc.), but the paper does not thoroughly discuss the potential biases or limitations of these metrics in the context of long-context question answering. The choice of benchmarks may not fully capture the complexity of real-world scenarios that GraphReader aims to address.\n",
      "\n",
      "7. **Ablation Studies**: Although ablation studies are presented, they may not cover all possible variations of the method. For instance, exploring the impact of different graph structures, node connections, or alternative exploration strategies could provide deeper insights into how different factors influence performance. Limited ablation studies may lead to overgeneralization of the findings.\n",
      "\n",
      "8. **Dependence on External Models**: The paper uses GPT-4 for implementing GraphReader. Any limitations or biases inherent to GPT-4 could impact the performance of the GraphReader system as a whole. Moreover, reliance on off-the-shelf API limits opportunities for more nuanced improvements and could impose constraints on handling queries in specific domains.\n",
      "\n",
      "9. **Robustness Across Domains**: The experimental results primarily focus on specific datasets, and the paper does not address how well the method performs across varied domains or types of long-context tasks beyond those tested. The transferability of the approach to other datasets or different types of input data may be limited.\n",
      "\n",
      "10. **Computational Efficiency**: While the paper addresses performance in terms of accuracy, computational efficiency (in terms of time and resource usage) is not thoroughly examined. As the method relies on exploring numerous nodes and functions, attention to resource management and efficiency would be critical, especially for larger texts.\n",
      "\n",
      "These points represent considerations that could be further scrutinized to enhance the robustness, scalability, and applicability of the GraphReader method in real-world long-context applications.\n"
     ]
    }
   ],
   "source": [
    "reply = agent.generate_reply(messages=[{\"content\": paper, \"role\": \"system\"},{\"content\": \"Find flaws in this paper's methods.\", \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_tokens is too small to fit a single line of text. Breaking this line:\n",
      "\tGraphReader: Building Graph-based Agent to Enhance ...\n",
      "Failed to split docs with must_break_at_empty_line being True, set to False.\n"
     ]
    }
   ],
   "source": [
    "chunks = retrieve_utils.split_text_to_chunks(paper, 300, \"multi_lines\")\n",
    "\n",
    "chunk_dict = {i: chunk for i, chunk in enumerate(chunks)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. | Long-context capabilities | large language models | complex | long-input tasks  \n",
      "2. Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs. | Numerous efforts | optimize | LLMs | long contexts | challenges | robustly processing | long inputs  \n",
      "3. This paper introduces GraphReader, a graph-based agent system designed to handle long texts. | GraphReader | graph-based agent system | handle | long texts  \n",
      "4. GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously. | GraphReader | structures | long texts | graph | agent | explore | autonomously  \n",
      "5. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. | agent | receiving | question | undertakes | step-by-step analysis | devises | rational plan  \n",
      "6. The agent invokes a set of predefined functions to read node content and neighbors. | agent | invokes | predefined functions | read | node content | neighbors  \n",
      "7. The graph-based approach facilitates a coarse-to-fine reading process of long texts. | graph-based approach | facilitates | coarse-to-fine reading process | long texts  \n"
     ]
    }
   ],
   "source": [
    "reply = agent.generate_reply(\n",
    "    messages=[\n",
    "        {\"content\": key_atom_prompt, \"role\": \"system\"},\n",
    "        {\"content\": chunks[0], \"role\": \"user\"},\n",
    "    ]\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(text):\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    result = {}\n",
    "\n",
    "    for line in lines:\n",
    "        try:\n",
    "            parts = line.split(\"|\")\n",
    "            sentence = parts[0].split(\".\", 1)[1].strip()\n",
    "            keys = [key.strip() for key in parts[1:]]\n",
    "\n",
    "            for key in keys:\n",
    "                if key in result:\n",
    "                    result[key].append(sentence)\n",
    "                else:\n",
    "                    result[key] = [sentence]\n",
    "        except IndexError:\n",
    "            # If split fails, skip this line and continue with the next\n",
    "            continue\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Long-context capabilities': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'large language models': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'complex': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'long-input tasks': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'Numerous efforts': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'optimize': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'LLMs': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'long contexts': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'challenges': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'robustly processing': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'long inputs': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'GraphReader': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'graph-based agent system': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.'],\n",
       " 'handle': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.'],\n",
       " 'long texts': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.',\n",
       "  'The graph-based approach facilitates a coarse-to-fine reading process of long texts.'],\n",
       " 'structures': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'graph': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'agent': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.',\n",
       "  'Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.',\n",
       "  'The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'explore': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'autonomously': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'receiving': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'question': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'undertakes': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'step-by-step analysis': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'devises': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'rational plan': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'invokes': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'predefined functions': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'read': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'node content': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'neighbors': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'graph-based approach': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.'],\n",
       " 'facilitates': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.'],\n",
       " 'coarse-to-fine reading process': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dict = convert_to_dict(reply)\n",
    "converted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\12700K\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460c76ad50e545339e87c4ec17d43c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae685c4b9efb4a6bbe295c6aa7a4b13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a4ee52f59447698c63700cf46a9bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9f3fc97a1c44cfa5d19983f0c4a737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0313f3d43c477a9e62cb01d2895da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe163c369bc423a884e7fe8e01eb756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8c04d2cbc94e4495769ddcc6f02dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ef85b97bbf4b7ca83896eee16703b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e352f675d9c0469c88196cadab726b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379ac3534dd749d38b1cef8349e5eab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941c2064fe5d430b970d217b02aeb870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 87\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     77\u001b[0m tags \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArtificial Intelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     86\u001b[0m ]\n\u001b[1;32m---> 87\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[13], line 67\u001b[0m, in \u001b[0;36maggregate_tags\u001b[1;34m(tags, alpha, semantic_threshold, association_min_support)\u001b[0m\n\u001b[0;32m     65\u001b[0m filtered_tags \u001b[38;5;241m=\u001b[39m frequency_filtering(tags, alpha)\n\u001b[0;32m     66\u001b[0m aggregated_tags \u001b[38;5;241m=\u001b[39m rule_aggregation(filtered_tags)\n\u001b[1;32m---> 67\u001b[0m semantically_aggregated_tags \u001b[38;5;241m=\u001b[39m \u001b[43msemantic_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregated_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msemantic_threshold\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m final_tags \u001b[38;5;241m=\u001b[39m association_aggregation(\n\u001b[0;32m     71\u001b[0m     semantically_aggregated_tags, association_min_support\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_tags\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36msemantic_aggregation\u001b[1;34m(tags, threshold)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Cluster tags based on semantic similarity using DBSCAN.\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/paraphrase-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m clustering \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39mthreshold, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(embeddings)\n\u001b[0;32m     32\u001b[0m clusters \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:565\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 565\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mall_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[0;32m    566\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: ['Python is a programming language', 'pythons are cool snakes']\n",
      "machine learn: ['ML is a subset of AI', 'machine-learning algorithms learn from data']\n",
      "data sci: ['data_science uses statistical methods', 'Data Science involves analyzing data']\n",
      "ai: ['AI stands for Artificial Intelligence']\n",
      "artificial intellig: ['Artificial Intelligence mimics human intelligence']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def frequency_filtering(tag_dict, alpha):\n",
    "    \"\"\"Filter out tags appearing less than alpha times.\"\"\"\n",
    "    tag_counts = Counter(tag_dict.keys())\n",
    "    return {\n",
    "        tag: list(set(values))\n",
    "        for tag, values in tag_dict.items()\n",
    "        if tag_counts[tag] >= alpha\n",
    "    }\n",
    "\n",
    "\n",
    "def rule_aggregation(tag_dict):\n",
    "    ps = PorterStemmer()\n",
    "    new_dict = {}\n",
    "    for tag, values in tag_dict.items():\n",
    "        new_tag = ps.stem(re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", tag.lower()))\n",
    "        if new_tag in new_dict:\n",
    "            new_dict[new_tag].extend(values)\n",
    "            new_dict[new_tag] = list(set(new_dict[new_tag]))\n",
    "        else:\n",
    "            new_dict[new_tag] = values.copy()\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def semantic_aggregation(tag_dict, threshold):\n",
    "    \"\"\"Cluster tags based on semantic similarity using DBSCAN.\"\"\"\n",
    "    model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "    tags = list(tag_dict.keys())\n",
    "    embeddings = model.encode(tags)\n",
    "    clustering = DBSCAN(eps=threshold, min_samples=1).fit(embeddings)\n",
    "    new_dict = {}\n",
    "    for i, label in enumerate(clustering.labels_):\n",
    "        if label == -1:\n",
    "            new_dict[tags[i]] = tag_dict[tags[i]]\n",
    "        else:\n",
    "            cluster_tags = [\n",
    "                tags[j] for j, l in enumerate(clustering.labels_) if l == label\n",
    "            ]\n",
    "            new_key = min(cluster_tags, key=len)\n",
    "            if new_key not in new_dict:\n",
    "                new_dict[new_key] = []\n",
    "            for tag in cluster_tags:\n",
    "                new_dict[new_key].extend(tag_dict[tag])\n",
    "            new_dict[new_key] = list(set(new_dict[new_key]))\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def association_aggregation(tag_dict, min_support):\n",
    "    \"\"\"Merge associated tags using FP-Growth algorithm.\"\"\"\n",
    "    tags = [list(tag_dict.keys())]\n",
    "    te = pd.get_dummies(pd.DataFrame(tags)).astype(bool)\n",
    "    frequent_itemsets = pd.DataFrame(\n",
    "        fpgrowth(te, min_support=min_support, use_colnames=True)\n",
    "    )\n",
    "\n",
    "    new_dict = {}\n",
    "    for _, row in frequent_itemsets.iterrows():\n",
    "        if len(row[\"itemsets\"]) > 1:\n",
    "            new_key = \" \".join(sorted(row[\"itemsets\"]))\n",
    "            new_dict[new_key] = []\n",
    "            for tag in row[\"itemsets\"]:\n",
    "                tag = tag.split('_', 1)[1]\n",
    "                new_dict[new_key].extend(tag_dict[tag])\n",
    "                tag_dict.pop(tag, None)\n",
    "            new_dict[new_key] = list(set(new_dict[new_key]))\n",
    "    new_dict.update(tag_dict)  # Add remaining unmerged tags\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def aggregate_tags(\n",
    "    tag_dict, alpha=1, semantic_threshold=5, association_min_support=1.2\n",
    "):\n",
    "    \"\"\"Aggregate tags using all methods.\"\"\"\n",
    "    filtered_dict = frequency_filtering(tag_dict, alpha)\n",
    "    aggregated_dict = rule_aggregation(filtered_dict)\n",
    "    semantically_aggregated_dict = semantic_aggregation(\n",
    "        aggregated_dict, semantic_threshold\n",
    "    )\n",
    "    final_dict = association_aggregation(\n",
    "        semantically_aggregated_dict, association_min_support\n",
    "    )\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "original_dict = {\n",
    "    \"Python\": [\"Python is a programming language\"],\n",
    "    \"python\": [\"pythons are cool snakes\"],\n",
    "    \"Machine Learning\": [\"ML is a subset of AI\"],\n",
    "    \"machine-learning\": [\"machine-learning algorithms learn from data\"],\n",
    "    \"Data Science\": [\"Data Science involves analyzing data\"],\n",
    "    \"data_science\": [\"data_science uses statistical methods\"],\n",
    "    \"AI\": [\"AI stands for Artificial Intelligence\"],\n",
    "    \"Artificial Intelligence\": [\"Artificial Intelligence mimics human intelligence\"],\n",
    "}\n",
    "result = aggregate_tags(original_dict)\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'long context cap': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'large language model': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'complex': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'long input task': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'numerous effort': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'optim': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'llm': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'long context': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'challeng': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'robustly process': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'long input': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'graphread': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'graph based agent system': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.'],\n",
       " 'handl': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.'],\n",
       " 'long text': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.',\n",
       "  'The graph-based approach facilitates a coarse-to-fine reading process of long texts.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'structur': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'graph': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'agent': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.',\n",
       "  'Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.',\n",
       "  'The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'explor': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'autonom': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'receiv': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'question': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'undertak': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'step by step analysi': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'devis': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'rational plan': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'invok': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'predefined funct': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'read': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'node cont': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'neighbor': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'graph based approach': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.'],\n",
       " 'facilit': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.'],\n",
       " 'coarse to fine reading process': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_tags(converted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: ['is popular', 'is a programming language', 'is used for data science']\n",
      "AI: ['stands for Artificial Intelligence']\n",
      "Machine Learning: ['is a subset of AI']\n"
     ]
    }
   ],
   "source": [
    "def extend_and_merge_dict(original_dict, new_dict):\n",
    "    \"\"\"\n",
    "    Extends the original dictionary with a new dictionary,\n",
    "    merging and deduplicating value lists for common keys.\n",
    "\n",
    "    :param original_dict: The dictionary to be extended\n",
    "    :param new_dict: The dictionary to extend with\n",
    "    :return: The extended and merged dictionary\n",
    "    \"\"\"\n",
    "    for key, new_values in new_dict.items():\n",
    "        if key in original_dict:\n",
    "            # Merge lists and remove duplicates\n",
    "            original_dict[key] = list(set(original_dict[key] + new_values))\n",
    "        else:\n",
    "            # Add new key-value pair\n",
    "            original_dict[key] = new_values\n",
    "\n",
    "    return original_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dict1 = {\n",
    "    \"Python\": [\"is a programming language\", \"is used for data science\"],\n",
    "    \"AI\": [\"stands for Artificial Intelligence\"],\n",
    "}\n",
    "\n",
    "dict2 = {\n",
    "    \"Python\": [\"is popular\", \"is used for data science\"],\n",
    "    \"Machine Learning\": [\"is a subset of AI\"],\n",
    "}\n",
    "\n",
    "result = extend_and_merge_dict(dict1, dict2)\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\autogen\\logger\\logger_utils.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n"
     ]
    }
   ],
   "source": [
    "greader_dict = {}\n",
    "for chunk in chunks:\n",
    "    reply = agent.generate_reply(\n",
    "        messages=[\n",
    "            {\"content\": key_atom_prompt, \"role\": \"system\"},\n",
    "            {\"content\": chunk, \"role\": \"user\"},\n",
    "        ]\n",
    "    )\n",
    "    greader_dict = extend_and_merge_dict(greader_dict, convert_to_dict(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1774"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(greader_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12700K\\Documents\\Projects\\VLOG-GraphReader\\vlog_greader\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'long context cap': ['GraphReader establishes a scalable long-context capability based on a 4k context window.',\n",
       "  'This paper introduces GraphReader, a graph-based agent designed to enhance the long-context capabilities of large language models.',\n",
       "  'The main contributions are threefold: the introduction of GraphReader, the establishment of a scalable long-context capability, and the demonstration of performance.',\n",
       "  'Recent efforts by Chen et al., Ding et al., and Peng have focused on positional interpolation to enhance long-context capabilities.',\n",
       "  'Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.'],\n",
       " 'language model': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".',\n",
       "  'Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian published a paper in 2023 titled \"Extending context window of large language models via positional interpolation\".',\n",
       "  'Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.',\n",
       "  'Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"',\n",
       "  'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.',\n",
       "  'Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"',\n",
       "  'Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.',\n",
       "  'The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.',\n",
       "  'Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.',\n",
       "  'Recent work has increasingly leveraged Large Language Models (LLMs) as agents to tackle complex problems.',\n",
       "  'Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"',\n",
       "  'Jerry Liu published \"Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources\" in 2023.',\n",
       "  'Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer authored a paper in 2021, questioning whether \"long-range language models actually use long-range context.\"',\n",
       "  'Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.',\n",
       "  'Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023).',\n",
       "  'Model-level methods train large language models (LLMs) with target length texts.',\n",
       "  'This paper introduces GraphReader, a graph-based agent designed to enhance the long-context capabilities of large language models.',\n",
       "  'Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.',\n",
       "  'Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole authored \"Yarn: Efficient context window extension of large language models\" in 2023.',\n",
       "  'Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'complex': ['Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.',\n",
       "  'The rational plan is aimed at addressing complex real-world multi-hop questions.'],\n",
       " 'long input': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.',\n",
       "  'Experiments demonstrate that GraphReader outperforms GPT-4 with a 128k input length across various long-context single-hop and multi-hop question-answering benchmarks.',\n",
       "  'Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks.',\n",
       "  'Average scores were recorded across various input lengths (16k, 32k, 64k, 128k, 256k).'],\n",
       " 'effort': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.',\n",
       "  'Many efforts investigate various levels of retrieval granularity.'],\n",
       " 'optim': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.',\n",
       "  'The agent optimizes the process until it has gathered sufficient information to generate an answer.'],\n",
       " 'llm': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.',\n",
       "  'LLMs utilize strong planning and reflection abilities.',\n",
       "  'The performance of GraphReader closely matches that achieved by directly supplying supporting facts to the LLM.',\n",
       "  'In the Chunk Read approach, the LLM only sees the current chunk during each reading.',\n",
       "  'The LLM analyzes each note by considering complementary information from other memories and using a majority voting strategy to resolve inconsistencies.',\n",
       "  'The LLM extracts atomic facts from text chunks and simplifies them.',\n",
       "  'Current techniques for solving long-context tasks of LLMs can be divided into two perspectives: model-level and fine-tuning.',\n",
       "  \"The Full Text Read method cannot be used for texts that exceed the LLM's input window tokens.\",\n",
       "  \"The Full Text Read method can be used for texts that are fewer than the LLM's input window tokens.\",\n",
       "  'The LLM also extracts key elements, including essential nouns, verbs, and adjectives from each atomic fact.',\n",
       "  'Golden denotes the settings in which question and its supporting facts are added to LLM directly.',\n",
       "  'Model-level methods limit the ability of LLMs to address complex tasks, such as multi-hop questions.',\n",
       "  'Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023).',\n",
       "  'Lee et al. truncated the text to fit it into the LLM when it exceeded token limits.',\n",
       "  'The top-n chunks with the highest relevance scores are input for the LLM to answer.',\n",
       "  'In the Chunk Read with Notes approach, the LLM can summarize useful information.',\n",
       "  'The B LLM Rater evaluates given a question, a golden answer, and an answer to be evaluated using an LLM.',\n",
       "  'The LLM summarizes each chunk into atomic facts to extract nodes.',\n",
       "  'The LLM generates the final answer after considering all available information.',\n",
       "  'The LLM reads these chunks sequentially according to the text order.'],\n",
       " 'long context': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".',\n",
       "  'BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.',\n",
       "  'The context length increasing to 128k resulted in a performance gain of 75.00% over GPT-4-128k.',\n",
       "  'The excessive length of the dataset leads to selection of the first 50 entries from each context length for experimentation to control costs.',\n",
       "  'GPT-4-128k outperforms ReadAgent on three long-context benchmarks.',\n",
       "  'The impact of extremely long context on the GraphReader is studied.',\n",
       "  'Employing GPT-4-128k to directly answer questions with long contexts significantly outperforms RAG methods.',\n",
       "  'The strategy significantly boosts the agent’s capability in multi-hop reasoning and capturing long-range dependencies of key information.',\n",
       "  'Long-context LLMs optimized with model-level methods overlook crucial details in long contexts.',\n",
       "  'The results indicate that RAG methods based on BM25 and Ada-002 exhibit the worst performance compared to long-context LLM and agent-based methods.',\n",
       "  'The single-hop long-context QA benchmark is NarrativeQA.',\n",
       "  'Retrieval-augmented LLMs or agents were employed to process long contexts.',\n",
       "  'Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Agent-level techniques were mentioned regarding the processing of long contexts.',\n",
       "  'The effectiveness of the rational plan is verified by removing it during agent initialization and conducting experiments on four long-context QA benchmarks.',\n",
       "  'Current techniques for solving long-context tasks of LLMs can be divided into two perspectives: model-level and fine-tuning.',\n",
       "  'The phenomenon of overlooking details in long contexts is known as “lost in the middle.”',\n",
       "  'Long-context benchmarks are shown in Table 2 and Table 3.',\n",
       "  'Long-context LLM uses GPT-4-128k for reading full text or for segmenting the text into chunks.',\n",
       "  'Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz authored the paper titled \"Walking down the memory maze: Beyond context limit through interactive methods.\"',\n",
       "  'Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.',\n",
       "  'ReadAgent significantly underperforms the method in handling extremely long contexts.',\n",
       "  'The context benchmark consists of five length levels up to 256k.',\n",
       "  'Experiments demonstrate that GraphReader outperforms GPT-4 with a 128k input length across various long-context single-hop and multi-hop question-answering benchmarks.',\n",
       "  'An agent-based system is employed for the execution of retrieval and reading processes for long-context QA.',\n",
       "  'Transformer variants with modified attention mechanisms have been proposed to address limitations in long-context LLMs.',\n",
       "  'Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer authored a paper in 2021, questioning whether \"long-range language models actually use long-range context.\"',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'Other models exhibited a significant decrease in performance as context length increases.',\n",
       "  'The multi-hop long-context QA benchmarks include HotpotQA, 2WikiMulti-hopQA, and MuSiQue.',\n",
       "  'The approach is compared with baseline methods: retrieval augmented generation (RAG), long-context LLM, and agent-based methods.',\n",
       "  'GraphReader demonstrated scalable performance in very long contexts.',\n",
       "  'GraphReader demonstrates performance that is comparable to or surpasses GPT-4 with a 128k context window across varying context lengths.',\n",
       "  'Our approach demonstrates superior performance in multi-hop long-context tasks.',\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'As context length increases from 16k to 256k, recall performance is analyzed.',\n",
       "  'The effectiveness of agent-level approaches is limited on very long contexts.',\n",
       "  'Results are obtained from three types of methods on four multi-hop long-context benchmarks and one single-hop task.',\n",
       "  'Our approach consistently performs better than all baselines on four long-context benchmarks.',\n",
       "  'Transformer-based LLMs struggle to handle long contexts due to limitations of context window and memory usage.',\n",
       "  'The effectiveness of employing a limited context window LLM for long-context tasks with the GraphReader is demonstrated in previous experiments.',\n",
       "  'The approach of GraphReader unlocks the capabilities of constrained context window LLMs more efficiently in processing long context.',\n",
       "  'The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'GraphReader demonstrates scalability and effectiveness in processing long contexts.',\n",
       "  'Results of long-context LLMs are discussed later in the text.',\n",
       "  'The experiments are conducted on two types of long-context QA benchmarks.',\n",
       "  'As the context length increases, the impact of the “lost in the middle” effect on GPT-4-128k becomes progressively more severe.',\n",
       "  'The method can effectively address challenges of processing extremely long context with limited context window LLMs by exploring graphs containing fine-grained information.',\n",
       "  'The GraphReader achieves a performance gain of 10.53% relatively on LR-1 over GPT-4-128k full-text reading under 16k context length.',\n",
       "  'The GraphReader exhibits robustness with the expansion of context length.'],\n",
       " 'challeng': ['Despite its capabilities, Retrieval Augmented Generation (RAG) faces challenges in addressing complex questions due to difficulties in developing robust decision-making mechanisms.',\n",
       "  'Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'robustly process': ['Numerous efforts have been made to optimize LLMs for long contexts, yet challenges persist in robustly processing long inputs.'],\n",
       " 'graphread': ['GraphReader begins from the initial node and follows a rational plan.',\n",
       "  'GraphReader establishes a scalable long-context capability based on a 4k context window.',\n",
       "  'The GraphReader captures global information using a graph structure.',\n",
       "  'GraphReader is built on a graph G={V,E}.',\n",
       "  'The impact of extremely long context on the GraphReader is studied.',\n",
       "  'The Effect of Node Selection is demonstrated through experiments on randomly selecting initial nodes and neighbor nodes.',\n",
       "  'For GraphReader, the memory is recorded in the final notebook.',\n",
       "  'The results shown in Table 3 indicate that the GraphReader outperforms all baseline methods across text lengths ranging from 16k to 256k tokens.',\n",
       "  'GraphReader maintains around 60% recall at 256k context length.',\n",
       "  'This section presents a case study of the GraphReader workflow.',\n",
       "  'GraphReader achieves superior performance in complex single-hop and multi-hop QA tasks.',\n",
       "  'GraphReader has an average context of 358.3k tokens and an average cost of 52.8k tokens.',\n",
       "  'The process of GraphReader includes three phases: graph construction, graph exploration, and answer reasoning.',\n",
       "  'The performance of GraphReader closely matches that achieved by directly supplying supporting facts to the LLM.',\n",
       "  \"Compared to GraphReader, ReadAgent's strategy may restrict the agent's ability to identify specific details.\",\n",
       "  'Table 5 refers to the comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k.',\n",
       "  'GraphReader organizes long texts into graph structures and employs an autonomous agent to explore the graph.',\n",
       "  'GraphReader is constructed using an off-the-shelf GPT-4 API.',\n",
       "  'The input window size for GraphReader is configured to 4k tokens unless stated otherwise.',\n",
       "  'Experiments were conducted with different initial node counts on multi-hop and single-hop QA datasets to assess the effect of initial nodes on GraphReader’s performance.',\n",
       "  'The proposed graph-based agent is named GraphReader.',\n",
       "  'GraphReader autonomously explores this graph using predefined functions, guided by a step-by-step rational plan.',\n",
       "  'GraphReader demonstrates robustness towards different initial node numbers.',\n",
       "  'The average tokens for ReadAgent and GraphReader are shown in Table 5.',\n",
       "  'Figure 22 elaborates on the initialization of a pre-planned rational path by GraphReader and the selection of initial nodes.',\n",
       "  'The GraphReader approach consists of graph construction, exploration, and answer reasoning.',\n",
       "  'GraphReader segments long texts into discrete chunks.',\n",
       "  'GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'Experiments demonstrate that GraphReader outperforms GPT-4 with a 128k input length across various long-context single-hop and multi-hop question-answering benchmarks.',\n",
       "  'GraphReader is close-sourced, which may lead to potential restrictions such as limits on Queries Per Second (QPS) and regional constraints.',\n",
       "  'GraphReader has a token capacity of 4k, with recorded values of 42.0, 42.0, 38.2, 32.0, 38.0, 36.4, 30.0, 36.0, 32.9, 28.0, 34.0, 30.6, 30.0, 38.0, and 33.0.',\n",
       "  'The same chunking method as GraphReader is used in the proposed method.',\n",
       "  'GraphReader outperformed existing open-sourced and closed-source models in performance on the LV-Eval dataset.',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'GraphReader terminates exploration once sufficient information is gathered to answer a question.',\n",
       "  'ReadAgent with a 128k context window setup underperforms GraphReader with a 4k context window.',\n",
       "  'GraphReader 4k achieved scores of 84.3, 89.7, 55.0, 70.0, 83.7, 87.0, 59.3, 70.1, 59.0, 63.5, 38.0, 47.4, 65.0, 80.0, 15.5, and 29.8.',\n",
       "  \"The findings indirectly reflect GraphReader's intelligence and effectiveness in exploration.\",\n",
       "  'The text is split into chunks using the method from GraphReader.',\n",
       "  'GraphReader collects information to answer the question.',\n",
       "  'GraphReader demonstrated scalable performance in very long contexts.',\n",
       "  'GraphReader demonstrates performance that is comparable to or surpasses GPT-4 with a 128k context window across varying context lengths.',\n",
       "  'GraphReader successfully establishes long-range dependencies within a relatively small 4k context window.',\n",
       "  'Statistics on function calls at each stage across two datasets are made to verify the actions of GraphReader.',\n",
       "  'Figure 3 shows the performance of GraphReader with different initial node numbers on 2WikiMultihopQA and NarrativeQA.',\n",
       "  'The graph was constructed with GraphReader being able to swiftly locate key information while minimizing resource usage.',\n",
       "  'This paper introduces GraphReader, a graph-based agent system designed to handle long texts.',\n",
       "  'GraphReader carries a notebook that records memory.',\n",
       "  'The rationality and utility of agent actions are verified under various circumstances of GraphReader.',\n",
       "  'GraphReader formulates the answer by leveraging insights obtained through exploration.',\n",
       "  'The analysis compares the average token consumption of ReadAgent and GraphReader for each question.',\n",
       "  'The effectiveness of employing a limited context window LLM for long-context tasks with the GraphReader is demonstrated in previous experiments.',\n",
       "  'The approach of GraphReader unlocks the capabilities of constrained context window LLMs more efficiently in processing long context.',\n",
       "  'The recall rate of GraphReader is calculated at different granularities.',\n",
       "  'GraphReader demonstrates scalability and effectiveness in processing long contexts.',\n",
       "  'This paper introduces GraphReader, a graph-based agent designed to enhance the long-context capabilities of large language models.',\n",
       "  'The main contributions are threefold: the introduction of GraphReader, the establishment of a scalable long-context capability, and the demonstration of performance.',\n",
       "  'The method of GraphReader incorporates various elements.',\n",
       "  'GraphReader answers the question based on memory from the notebook.',\n",
       "  \"The impact of chunk size on GraphReader's performance was investigated.\",\n",
       "  'The experiment divides the chunks in the same way as GraphReader.',\n",
       "  'These statistics indicate the effectiveness of GraphReader.',\n",
       "  'GraphReader uses 1.08 times more tokens than ReadAgent.',\n",
       "  'GraphReader extracts essential information from text segments.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.',\n",
       "  'GraphReader collects supporting facts during graph exploration.',\n",
       "  'The GraphReader achieves a performance gain of 10.53% relatively on LR-1 over GPT-4-128k full-text reading under 16k context length.',\n",
       "  'GraphReader achieves over twice the performance improvement compared to ReadAgent.',\n",
       "  'GraphReader shows a recall performance of 76.4% for atomic facts and 90.5% for the final notebook at SF-wise granularity.',\n",
       "  'The GraphReader exhibits robustness with the expansion of context length.',\n",
       "  'Figure 24 showcases how GraphReader operates.',\n",
       "  'GraphReader compresses key elements and atomic facts into a graph.'],\n",
       " 'graph based agent system': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.'],\n",
       " 'handl': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.',\n",
       "  'The method enables handling overly long texts with a limited input window.'],\n",
       " 'long text': ['This paper introduces GraphReader, a graph-based agent system designed to handle long texts.',\n",
       "  'The graph structure effectively captures long-range dependencies and multi-hop relationships within long text.',\n",
       "  'GraphReader segments long texts into discrete chunks.',\n",
       "  'HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'The method enables handling overly long texts with a limited input window.',\n",
       "  'Model-level methods train large language models (LLMs) with target length texts.',\n",
       "  'The method is compared with similar approaches for handling long texts with small input windows.',\n",
       "  'The method is effective in handling extremely long texts by graph exploration with limited context window LLMs.',\n",
       "  'GraphReader organizes long texts into graph structures and employs an autonomous agent to explore the graph.',\n",
       "  'The superior performance of GPT-4-128k lies in its ability to process long texts and execute multi-hop reasoning tasks.',\n",
       "  'Retrieval-Augmented Generation (RAG) addresses long-text problems.',\n",
       "  'The results shown in Table 3 indicate that the GraphReader outperforms all baseline methods across text lengths ranging from 16k to 256k tokens.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.',\n",
       "  'The graph-based approach facilitates a coarse-to-fine reading process of long texts.',\n",
       "  'For longer texts, there tends to be a higher average number of nodes and atomic facts.',\n",
       "  'PoSE and SkipAlign investigate data skip strategy but tend to neglect detailed information in long texts.'],\n",
       " 'structur': ['GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'graph': ['GraphReader is built on a graph G={V,E}.',\n",
       "  'Pre-planning, reflection, and various actions are important for using a graph that contains key information.',\n",
       "  'Each graph has associated neighbor node counts.',\n",
       "  'The average number of atomic facts in the node with the most atomic facts in each graph ranges from 15 to 50.',\n",
       "  'The process of GraphReader includes three phases: graph construction, graph exploration, and answer reasoning.',\n",
       "  'The term “avg. avg.” denotes the average of the average neighbor node counts per graph.',\n",
       "  'GraphReader organizes long texts into graph structures and employs an autonomous agent to explore the graph.',\n",
       "  'Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"',\n",
       "  'Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.',\n",
       "  'GraphReader autonomously explores this graph using predefined functions, guided by a step-by-step rational plan.',\n",
       "  'The GraphReader approach consists of graph construction, exploration, and answer reasoning.',\n",
       "  'Figure 21 delineates the methodology for constructing the graph.',\n",
       "  'The term “avg. max” means the average of the maximum neighbor node counts per graph.',\n",
       "  'A graph comprises text chunks, atomic facts, and nodes, which are key elements in the text.',\n",
       "  'The construction of the graph involves nodes representing key information.',\n",
       "  'An exploration of the graph was conducted by an agent that continuously records new insights and reflects on current circumstances.',\n",
       "  'The goal is to design an agent that autonomously explores the graph using predefined functions.',\n",
       "  'KGP organizes documents into graphs.',\n",
       "  'The graph was constructed with GraphReader being able to swiftly locate key information while minimizing resource usage.',\n",
       "  'Figure 6 illustrates the prompt used for Graph Construction.',\n",
       "  'A graph has been created from the text, comprising text chunks, atomic facts, and nodes.',\n",
       "  'In Table 8, “avg.” indicates the average number of nodes in each graph.',\n",
       "  'In Table 8, “max” refers to the largest node count across all graphs.',\n",
       "  'Statistics of graphs from various datasets are presented in Table 8.',\n",
       "  'The average scores for different metrics are illustrated in the graph, with LR-1 and LR-2 as comparisons.',\n",
       "  'The method can effectively address challenges of processing extremely long context with limited context window LLMs by exploring graphs containing fine-grained information.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.',\n",
       "  \"Our method benefits from the graph's ability to capture relationships between detailed information.\",\n",
       "  'GraphReader compresses key elements and atomic facts into a graph.'],\n",
       " 'agent': ['The agent explores chunks of the node after exploring atomic facts.',\n",
       "  'Chunks are grouped by corresponding chunk IDs and fed to the agent.',\n",
       "  'The agent explores each initial node by first exploring atomic facts.',\n",
       "  'The agent may insert chunk IDs into the queue.',\n",
       "  'If the agent identifies certain chunks as valuable for further reading, it completes the function parameters with the chunk IDs.',\n",
       "  '“w/o Rational Plan” indicates the removal of the rational plan in the agent initialization stage.',\n",
       "  'The agent appends these IDs to a chunk queue.',\n",
       "  'The agent thinks about what can be added to the current notebook.',\n",
       "  'If the agent deems that none of the chunks are worth further reading, it finishes reading the node.',\n",
       "  'The agent checks all neighboring nodes and performs one of two functions: reading a neighboring node or terminating.',\n",
       "  'In the graph exploration stage, a rational plan is introduced to help the agent analyze complex input questions step by step.',\n",
       "  'The strategy significantly boosts the agent’s capability in multi-hop reasoning and capturing long-range dependencies of key information.',\n",
       "  'While reading chunks, the agent will consider the question and plan.',\n",
       "  'The agent needs to access the next node after processing.',\n",
       "  'The agent continuously updates the notebook with relevant information during the exploration process.',\n",
       "  'Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration.',\n",
       "  'Retrieval-augmented LLMs or agents were employed to process long contexts.',\n",
       "  'The agent explores neighboring nodes, guided by the question and rational plan.',\n",
       "  'The agent is provided with two functions: read_chunk and stop_and_read_neighbor.',\n",
       "  \"Compared to GraphReader, ReadAgent's strategy may restrict the agent's ability to identify specific details.\",\n",
       "  'The agent determines which chunk is likely to contain useful information.',\n",
       "  'The agent will select one of four functions based on the updated notebook.',\n",
       "  'The agent identifies the key information needed.',\n",
       "  'The agent will terminate exploration if sufficient information has been gathered.',\n",
       "  'Recent work has increasingly leveraged Large Language Models (LLMs) as agents to tackle complex problems.',\n",
       "  'When the chunk queue is non-empty, it indicates the agent has identified multiple text chunks of interest.',\n",
       "  'The agent starts by maintaining a notebook to record supporting facts.',\n",
       "  'The agent proceeds to explore neighboring nodes.',\n",
       "  'Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.',\n",
       "  \"The document discusses the procedures involved in an agent's exploration and the reasoning process following information gathering.\",\n",
       "  'The agent evaluates the key elements of all nodes V and selects Ninitial nodes.',\n",
       "  'An exploration of the graph was conducted by an agent that continuously records new insights and reflects on current circumstances.',\n",
       "  'The agent performs two initializations: defining a rational plan and selecting the initial node.',\n",
       "  'The goal is to design an agent that autonomously explores the graph using predefined functions.',\n",
       "  'Based on a given question, the agent progressively accesses information from coarse key elements and atomic facts to detailed original text chunks.',\n",
       "  'The agent traverses the queue, reading each chunk.',\n",
       "  'The agent will read_previous_chunk and read_subsequent_chunk due to truncation issues.',\n",
       "  'The agent captures an overview of each chunk by reading all groups of atomic facts.',\n",
       "  'The agent will use search_more if supporting facts are insufficient.',\n",
       "  'The agent breaks down the original question step-by-step.',\n",
       "  'The rationality and utility of agent actions are verified under various circumstances of GraphReader.',\n",
       "  'The agent takes notes and reflects until it gathers sufficient information to generate an answer.',\n",
       "  'The agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks.',\n",
       "  'In contrast, we employ agents that use planning and reflection to gather essential information.',\n",
       "  'KGP primarily uses agents to generate queries.',\n",
       "  'The agent will summarize key information and provide brief clues.',\n",
       "  'The agent invokes a set of predefined functions to read node content and neighbors.',\n",
       "  'The agent forms a rational plan.',\n",
       "  'Agents can retrieve unstructured information.',\n",
       "  'The agent employs a coarse-to-fine strategy, progressing from reading atomic facts to the original text.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.',\n",
       "  'The agent utilizes the question, rational plan, and notes in its notebook to reflect on required clues.',\n",
       "  'The agent determines that none of the neighboring nodes contain useful information and finishes the exploration.',\n",
       "  \"The agent's functions include assessing neighboring nodes to provide answers or denote completion of information gathering.\"],\n",
       " 'explor': ['The agent explores chunks of the node after exploring atomic facts.',\n",
       "  'The document preprocessing affects token consumption in subsequent exploration.',\n",
       "  'Fully processed atomic facts and chunk queue indicate thorough exploration of the current node.',\n",
       "  'The agent explores each initial node by first exploring atomic facts.',\n",
       "  'The average cost tokens comprise both input tokens and output tokens during exploration.',\n",
       "  'The agent checks all neighboring nodes and performs one of two functions: reading a neighboring node or terminating.',\n",
       "  'Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration.',\n",
       "  'The agent explores neighboring nodes, guided by the question and rational plan.',\n",
       "  'GraphReader autonomously explores this graph using predefined functions, guided by a step-by-step rational plan.',\n",
       "  'The agent will terminate exploration if sufficient information has been gathered.',\n",
       "  'It is emphasized that even slightly relevant atomic facts should be explored.',\n",
       "  'The exploration process includes the function call to read a chunk of information.',\n",
       "  'GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'An exploration of the graph was conducted by an agent that continuously records new insights and reflects on current circumstances.',\n",
       "  'The goal is to design an agent that autonomously explores the graph using predefined functions.',\n",
       "  \"The findings indirectly reflect GraphReader's intelligence and effectiveness in exploration.\",\n",
       "  'GraphReader formulates the answer by leveraging insights obtained through exploration.',\n",
       "  'The method is capable of extracting more valid information from chunks during exploration.',\n",
       "  'The agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks.',\n",
       "  'A rational plan is initially formulated in response to a question, guiding further exploration.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.',\n",
       "  'GraphReader terminates exploration once sufficient information is gathered to answer a question.',\n",
       "  'The agent determines that none of the neighboring nodes contain useful information and finishes the exploration.'],\n",
       " 'autonom': ['The goal is to design an agent that autonomously explores the graph using predefined functions.',\n",
       "  'GraphReader structures long texts into a graph and employs an agent to explore this graph autonomously.'],\n",
       " 'receiv': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'answer': ['If able to answer, the response should be [answerable] followed by the answer.',\n",
       "  'An example question is: Who had a longer tennis career, Danny or Alice?',\n",
       "  'Figure 12 shows the prompt used for Answer Reasoning.',\n",
       "  'While reading chunks, the agent will consider the question and plan.',\n",
       "  'The agent optimizes the process until it has gathered sufficient information to generate an answer.',\n",
       "  'A possible reason for the poor performance of RAG methods is difficulty in recalling all chunks that contain supporting facts.',\n",
       "  'The analysis requires reviewing all notebooks before providing an answer.',\n",
       "  'The process of GraphReader includes three phases: graph construction, graph exploration, and answer reasoning.',\n",
       "  'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.',\n",
       "  'The agent explores neighboring nodes, guided by the question and rational plan.',\n",
       "  'The accuracy of the latter is based on the question and correct answer.',\n",
       "  'Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"',\n",
       "  'Choose to read a neighboring node if it may contain relevant information to the question.',\n",
       "  'The question is still lacking clarity.',\n",
       "  'The GraphReader approach consists of graph construction, exploration, and answer reasoning.',\n",
       "  'Another prompt requires reading a passage and answering a question based on that passage.',\n",
       "  'Relevance scores are calculated between the question and the chunks.',\n",
       "  'Each question in the original datasets is supported by 2-4 paragraphs providing evidence for simple reasoning and additional decoy paragraphs.',\n",
       "  'If neither scorer finds the answer correct, it is adjudged incorrect.',\n",
       "  'The necessity of selecting which nodes to visit is based on reasoning about required information.',\n",
       "  'If the strict scorer finds an answer incorrect while the lenient scorer deems it partially correct, the answer is classified as partially correct.',\n",
       "  'Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.',\n",
       "  'The first step to resolve a question is to create a rational plan based on the question.',\n",
       "  'Golden denotes the settings in which question and its supporting facts are added to LLM directly.',\n",
       "  'Annotators rephrase questions to avoid shortcut answers and maintain natural linguistic quality.',\n",
       "  'GraphReader collects information to answer the question.',\n",
       "  'The question is based on the passage content.',\n",
       "  'If either LLM Rater deems an answer correct, it is considered correct.',\n",
       "  'After multiple agents have independently gathered information and stopped their exploration, notes from each agent will be compiled for reasoning.',\n",
       "  'The answer to the agreement question could be \"Yes\" or \"No\".',\n",
       "  'The year of publication is 2023, and it discusses how \"questions are all you need to train a dense passage retriever.\"',\n",
       "  'GraphReader formulates the answer by leveraging insights obtained through exploration.',\n",
       "  'GraphReader answers the question based on memory from the notebook.',\n",
       "  'Figure 20 displays the posed question alongside the answer and pertinent supporting passages.',\n",
       "  'John was given a question about the text after reading some text.',\n",
       "  'The B LLM Rater evaluates given a question, a golden answer, and an answer to be evaluated using an LLM.',\n",
       "  'A rational plan is initially formulated in response to a question, guiding further exploration.',\n",
       "  'LLM Raters are implemented for answer correctness evaluation.',\n",
       "  'Questions in 2WikiMultihopQA are constructed through carefully designed templates to prevent shortcut solutions.',\n",
       "  'The text asks if John’s answer agrees with the ground truth answer.',\n",
       "  'In the MuSiQue dataset, questions are intricately crafted starting from straightforward scenarios.',\n",
       "  'The agent utilizes the question, rational plan, and notes in its notebook to reflect on required clues.',\n",
       "  'John’s answer to the question was a model response.'],\n",
       " 'undertak': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'analysi': ['The experiments section denotes ongoing analysis and findings related to the outlined methodology.',\n",
       "  'The analysis evaluates the inference cost of the approach.',\n",
       "  'Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.',\n",
       "  'The section title is \"Further Analysis.\"',\n",
       "  'The analysis requires reviewing all notebooks before providing an answer.'],\n",
       " 'devis': ['Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.'],\n",
       " 'rational plan': ['GraphReader begins from the initial node and follows a rational plan.',\n",
       "  'NarrativeQAGraphReader without Rational Plan scored 63.0, 78.5, and 26.6.',\n",
       "  '“w/o Rational Plan” indicates the removal of the rational plan in the agent initialization stage.',\n",
       "  'In the graph exploration stage, a rational plan is introduced to help the agent analyze complex input questions step by step.',\n",
       "  'The rational plan should outline a step-by-step process to resolve the question.',\n",
       "  'Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration.',\n",
       "  'The agent explores neighboring nodes, guided by the question and rational plan.',\n",
       "  'The effectiveness of the rational plan is verified by removing it during agent initialization and conducting experiments on four long-context QA benchmarks.',\n",
       "  'GraphReader autonomously explores this graph using predefined functions, guided by a step-by-step rational plan.',\n",
       "  'The initial node is selected from all nodes based on the rational plan.',\n",
       "  'The rational plan is aimed at addressing complex real-world multi-hop questions.',\n",
       "  'Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan.',\n",
       "  'The first step to resolve a question is to create a rational plan based on the question.',\n",
       "  'The agent performs two initializations: defining a rational plan and selecting the initial node.',\n",
       "  'HotpotQAGraphReader without Rational Plan scored 81.7, 87.7, and 63.8.',\n",
       "  'MuSiQueGraphReader without Rational Plan scored 56.0, 61.0, and 42.4.',\n",
       "  '2WikiMultihopQAGraphReader without Rational Plan scored 81.3, 86.0, and 65.4.',\n",
       "  'The agent forms a rational plan.',\n",
       "  'A rational plan is initially formulated in response to a question, guiding further exploration.',\n",
       "  'The agent utilizes the question, rational plan, and notes in its notebook to reflect on required clues.'],\n",
       " 'invok': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'predefined funct': ['GraphReader autonomously explores this graph using predefined functions, guided by a step-by-step rational plan.',\n",
       "  'The goal is to design an agent that autonomously explores the graph using predefined functions.',\n",
       "  'GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'read': ['Another prompt requires reading a passage and answering a question based on that passage.',\n",
       "  'Up to 5 pages can be read in the evaluation.',\n",
       "  'John was given a question about the text after reading some text.',\n",
       "  'In the Chunk Read approach, the LLM only sees the current chunk during each reading.',\n",
       "  'The reading of the pages should be in sequence to be the most effective.',\n",
       "  'The agent invokes a set of predefined functions to read node content and neighbors.',\n",
       "  'The agent employs a coarse-to-fine strategy, progressing from reading atomic facts to the original text.',\n",
       "  'While reading chunks, the agent will consider the question and plan.',\n",
       "  'GPT-4-128k is employed to read retrieved chunks and answer questions in RAG.'],\n",
       " 'node cont': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'neighbor': ['The agent invokes a set of predefined functions to read node content and neighbors.'],\n",
       " 'graph based approach': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.'],\n",
       " 'facilit': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.'],\n",
       " 'coarse to fine reading process': ['The graph-based approach facilitates a coarse-to-fine reading process of long texts.'],\n",
       " 'record': ['An exploration of the graph was conducted by an agent that continuously records new insights and reflects on current circumstances.',\n",
       "  'The agent starts by maintaining a notebook to record supporting facts.',\n",
       "  'Any supporting facts discovered will be recorded in the notebook.',\n",
       "  '\"Never Too Loud\" was recorded at Studio 606 in Los Angeles, with producer Nick Raskulinecz.'],\n",
       " 'insight': ['An exploration of the graph was conducted by an agent that continuously records new insights and reflects on current circumstances.',\n",
       "  'The action \"read_subsequent_chunk()\" may also provide insights.',\n",
       "  'GraphReader formulates the answer by leveraging insights obtained through exploration.'],\n",
       " 'reflect': ['GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'Strategy involves reflecting on previous actions to prevent redundant revisiting of nodes or chunks.',\n",
       "  'In contrast, we employ agents that use planning and reflection to gather essential information.',\n",
       "  'The agent takes notes and reflects until it gathers sufficient information to generate an answer.',\n",
       "  'An exploration of the graph was conducted by an agent that continuously records new insights and reflects on current circumstances.',\n",
       "  'Pre-planning, reflection, and various actions are important for using a graph that contains key information.',\n",
       "  'KGP does not fully exploit the agent’s capabilities for planning and reflection.',\n",
       "  \"The findings indirectly reflect GraphReader's intelligence and effectiveness in exploration.\"],\n",
       " 'circumst': ['An exploration of the graph was conducted by an agent that continuously records new insights and reflects on current circumstances.',\n",
       "  'The rationality and utility of agent actions are verified under various circumstances of GraphReader.'],\n",
       " '': ['2WikiMulti-hopQA was developed by Ho et al. in 2020.',\n",
       "  'The lack of detailed information about the content of each page makes page selection very difficult for ReadAgent.',\n",
       "  'This step is essential because atomic facts merely sum up the overall text.',\n",
       "  'Gist memories are looked up to search for information to answer questions.',\n",
       "  'The agent appends these IDs to a chunk queue.',\n",
       "  'NarrativeQA was developed by Kociský et al. in 2018.',\n",
       "  'Employing GPT-4-128k to directly answer questions with long contexts significantly outperforms RAG methods.',\n",
       "  'Each graph has associated neighbor node counts.',\n",
       "  'The agent continuously updates the notebook with relevant information during the exploration process.',\n",
       "  'The agent is provided with two functions: read_chunk and stop_and_read_neighbor.',\n",
       "  \"Compared to GraphReader, ReadAgent's strategy may restrict the agent's ability to identify specific details.\",\n",
       "  'F1 score is calculated if the recall exceeds a certain threshold.',\n",
       "  'Choose to read a neighboring node if it may contain relevant information to the question.',\n",
       "  'The Chunk Read approach is suitable for single-hop QA tasks.',\n",
       "  'The necessity of selecting which nodes to visit is based on reasoning about required information.',\n",
       "  'The dataset includes various models such as NarrativeQA and HotpotWikiQA-mixup16k.',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'Our approach demonstrates superior performance in multi-hop long-context tasks.',\n",
       "  'The lengths of four benchmarks are significantly shorter than the 128k context window.',\n",
       "  'The agent takes notes and reflects until it gathers sufficient information to generate an answer.',\n",
       "  'The approach demonstrated superior performance on four challenging single-hop and multi-hop benchmarks.',\n",
       "  'The action \"termination()\" indicates that sufficient information may have already been gathered.',\n",
       "  'GraphReader extracts essential information from text segments.',\n",
       "  'The LLM generates the final answer after considering all available information.',\n",
       "  'The LLM reads these chunks sequentially according to the text order.',\n",
       "  'Chunks are grouped by corresponding chunk IDs and fed to the agent.',\n",
       "  'GPT-4-128k outperforms ReadAgent on three long-context benchmarks.',\n",
       "  'There are two main strategies during the reading process: Chunk Read and Chunk Read with Notes.',\n",
       "  'Table 1 provides statistics about these benchmarks.',\n",
       "  'The evaluation metrics employed include F1 score, Exact Match (EM) score, and optimized F1* score.',\n",
       "  'In a single-hop QA dataset, the information needed to answer questions appears at a single location within the text.',\n",
       "  'The assistant must assess whether the available information in a text chunk is sufficient to answer a question.',\n",
       "  'The single-hop long-context QA benchmark is NarrativeQA.',\n",
       "  'Multi-hop questions need to gather information contained by multiple nodes to answer questions.',\n",
       "  'The 128k model has an average of 8828.5 nodes and a maximum of 14592.0 nodes.',\n",
       "  'HotpotQA was developed by Yang et al. in 2018.',\n",
       "  'The final answer must consider all available information collected from the notebooks.',\n",
       "  'The LLM analyzes each note by considering complementary information from other memories and using a majority voting strategy to resolve inconsistencies.',\n",
       "  'The agent determines which chunk is likely to contain useful information.',\n",
       "  'The 64k model shows an average of 5054.1 nodes and a maximum of 8918.0 nodes.',\n",
       "  'PoSE and SkipAlign investigate data skip strategy but tend to neglect detailed information in long texts.',\n",
       "  'A new method is proposed that does not lose information and allows for better comparison.',\n",
       "  \"The Full Text Read method cannot be used for texts that exceed the LLM's input window tokens.\",\n",
       "  'HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'When the chunk queue is non-empty, it indicates the agent has identified multiple text chunks of interest.',\n",
       "  'The same chunking method as GraphReader is used in the proposed method.',\n",
       "  'Transformer variants were applied in the research.',\n",
       "  'If the recall does not exceed the threshold, the score defaults to zero.',\n",
       "  'GraphReader collects information to answer the question.',\n",
       "  'MuSiQue was developed by Trivedi et al. in 2022.',\n",
       "  'The agent captures an overview of each chunk by reading all groups of atomic facts.',\n",
       "  'Choose termination if no neighboring nodes possess information to answer the question.',\n",
       "  'Our method can identify crucial information and search for supporting facts for input questions efficiently.',\n",
       "  'In Table 8, “max” refers to the largest node count across all graphs.',\n",
       "  'The superior performance of GPT-4-128k lies in its ability to process long texts and execute multi-hop reasoning tasks.',\n",
       "  'The agent utilizes the question, rational plan, and notes in its notebook to reflect on required clues.',\n",
       "  'Adjacent chunks might contain relevant and useful information.',\n",
       "  'LongBench is authored by Bai et al. in 2023.',\n",
       "  'If the agent identifies certain chunks as valuable for further reading, it completes the function parameters with the chunk IDs.',\n",
       "  'Detailed information is available in Appendix C.',\n",
       "  'The agent optimizes the process until it has gathered sufficient information to generate an answer.',\n",
       "  'The 256k model features an average of 14853.3 nodes and a maximum of 24981.0 nodes.',\n",
       "  'For ReadAgent, the evaluation focuses on the final text segments reviewed.',\n",
       "  'Shorter benchmark lengths mitigate the impact of “lost in the middle” on model performance.',\n",
       "  'In the Chunk Read approach, the LLM only sees the current chunk during each reading.',\n",
       "  'Current techniques for solving long-context tasks of LLMs can be divided into two perspectives: model-level and fine-tuning.',\n",
       "  'The proposed method involves dividing the entire text into chunks.',\n",
       "  'The approach is also used for counting atomic facts.',\n",
       "  'The agent proceeds to explore neighboring nodes.',\n",
       "  'Future research will explore enhancements of planning and reasoning features to improve the effectiveness of the method.',\n",
       "  'Lee et al. truncated the text to fit it into the LLM when it exceeded token limits.',\n",
       "  'Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023).',\n",
       "  'The multi-hop long-context QA benchmarks include HotpotQA, 2WikiMulti-hopQA, and MuSiQue.',\n",
       "  'In the Chunk Read with Notes approach, the LLM can summarize useful information.',\n",
       "  'F1* computes the recall of golden answer keywords first.',\n",
       "  'After multiple agents have independently gathered information and stopped their exploration, notes from each agent will be compiled for reasoning.',\n",
       "  'The values represent performance metrics including statistical figures like averages and maximums for each model.',\n",
       "  'Transformer-based LLMs struggle to handle long contexts due to limitations of context window and memory usage.',\n",
       "  'Specific details are best obtained directly from the original text chunks.',\n",
       "  'The experiments are conducted on two types of long-context QA benchmarks.',\n",
       "  \"Our method benefits from the graph's ability to capture relationships between detailed information.\",\n",
       "  'The 32k model has an average of 2827.3 nodes and a maximum of 5086.0 nodes.',\n",
       "  \"An intelligent assistant's primary objective is to answer questions based on information within the text.\",\n",
       "  'The action \"read_previous_chunk()\" may provide useful information.',\n",
       "  'If the agent deems that none of the chunks are worth further reading, it finishes reading the node.',\n",
       "  'HotpotWikiQA-mixup is included in the evaluation from LV-Eval.',\n",
       "  'Long-context LLMs optimized with model-level methods overlook crucial details in long contexts.',\n",
       "  'The method enables handling overly long texts with a limited input window.',\n",
       "  'For HotpotWikiQA-mixup16k, the average nodes is 1741.6, and the maximum node count is 3822.0.',\n",
       "  'The agent will terminate exploration if sufficient information has been gathered.',\n",
       "  'The evaluation metrics are introduced by LV-Eval in 2024.',\n",
       "  \"The Full Text Read method can be used for texts that are fewer than the LLM's input window tokens.\",\n",
       "  'Truncating texts results in information loss.',\n",
       "  'The average is calculated for neighbor node counts.',\n",
       "  'An exploration of the graph was conducted by an agent that continuously records new insights and reflects on current circumstances.',\n",
       "  'It is important to incorporate complementary information from other notes.',\n",
       "  'Based on a given question, the agent progressively accesses information from coarse key elements and atomic facts to detailed original text chunks.',\n",
       "  'The agent traverses the queue, reading each chunk.',\n",
       "  'Our approach consistently performs better than all baselines on four long-context benchmarks.',\n",
       "  'The method is capable of extracting more valid information from chunks during exploration.',\n",
       "  'The evaluation metrics are cost-effective.',\n",
       "  'In contrast, we employ agents that use planning and reflection to gather essential information.',\n",
       "  'In Table 8, “avg.” indicates the average number of nodes in each graph.',\n",
       "  'For NarrativeQA, the average nodes is 966.0, while the maximum node count is 3110.0.',\n",
       "  'Agents can retrieve unstructured information.',\n",
       "  'Further details and evaluation prompts can be found in Appendix E.',\n",
       "  'GraphReader terminates exploration once sufficient information is gathered to answer a question.',\n",
       "  'The agent determines that none of the neighboring nodes contain useful information and finishes the exploration.'],\n",
       " 'process': ['The process of GraphReader includes three phases: graph construction, graph exploration, and answer reasoning.',\n",
       "  'Fully processed atomic facts and chunk queue indicate thorough exploration of the current node.',\n",
       "  'Retrieval-augmented LLMs or agents were employed to process long contexts.',\n",
       "  'GraphReader demonstrates scalability and effectiveness in processing long contexts.',\n",
       "  'The superior performance of GPT-4-128k lies in its ability to process long texts and execute multi-hop reasoning tasks.',\n",
       "  'Agent-level techniques were mentioned regarding the processing of long contexts.',\n",
       "  'The entire process is illustrated in Figure 2.',\n",
       "  'The agent optimizes the process until it has gathered sufficient information to generate an answer.',\n",
       "  'The agent needs to access the next node after processing.'],\n",
       " 'experimental result': ['Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.'],\n",
       " 'lv eval': ['The evaluation metrics are introduced by LV-Eval in 2024.',\n",
       "  'GraphReader outperformed existing open-sourced and closed-source models in performance on the LV-Eval dataset.',\n",
       "  'Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'HotpotWikiQA-mixup is included in the evaluation from LV-Eval.',\n",
       "  'For HotpotWikiQA-mixup from LV-Eval, two hyperparameters are scaled using the same approach as in the ReadAgent paper.',\n",
       "  'The title of the paper \"Lv-eval\" suggests a focus on evaluation in a balanced manner.',\n",
       "  \"Table 3 presents performance percentages of different baselines on datasets from LV-Eval, with F1* indicating LV-Eval's optimized F1.\",\n",
       "  'HotpotWikiQA-mixup is derived from LV-Eval and uses a mixup construction method.'],\n",
       " '128k context': ['GraphReader maintains around 60% recall at 256k context length.',\n",
       "  'GraphReader establishes a scalable long-context capability based on a 4k context window.',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'ReadAgent with a 128k context window setup underperforms GraphReader with a 4k context window.',\n",
       "  'Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.',\n",
       "  'GraphReader successfully establishes long-range dependencies within a relatively small 4k context window.',\n",
       "  'GraphReader demonstrates performance that is comparable to or surpasses GPT-4 with a 128k context window across varying context lengths.',\n",
       "  'The lengths of four benchmarks are significantly shorter than the 128k context window.'],\n",
       " 'outperform': ['GPT-4-128k outperforms ReadAgent on three long-context benchmarks.',\n",
       "  'GraphReader outperformed existing open-sourced and closed-source models in performance on the LV-Eval dataset.',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'Employing GPT-4-128k to directly answer questions with long contexts significantly outperforms RAG methods.',\n",
       "  'The model consistently outperforms other baseline methods.'],\n",
       " 'gpt 4 128k': ['The context length increasing to 128k resulted in a performance gain of 75.00% over GPT-4-128k.',\n",
       "  'The recall rate of supporting facts is evaluated for different methods using GPT-4-128k with a temperature of 0.1.',\n",
       "  'GPT-4-128k (chunk w/ notes) achieved scores of 72.3, 76.7, 45.7, 59.5, 65.7, 68.7, 46.3, 56.6, 39.5, 43.0, 25.0, 32.5, 56.5, 65.0, 8.5, and 24.3.',\n",
       "  'GPT-4-128k outperforms ReadAgent on three long-context benchmarks.',\n",
       "  'Employing GPT-4-128k to directly answer questions with long contexts significantly outperforms RAG methods.',\n",
       "  'We utilize GPT-4-128k as the LLM Rater with the temperature set to 0.1.',\n",
       "  'GPT-4-128k achieved scores of 83.3, 88.3, 53.0, 68.4, 77.3, 80.0, 58.7, 70.0, 52.0, 59.5, 33.5, 42.7, 63.5, 77.0, 11.5, and 29.4 in various metrics.',\n",
       "  'ReadAgent performs worse than GPT-4-128k full-text reading.',\n",
       "  'The input window for GPT-4-128k is 128k and 256k.',\n",
       "  'GPT-4-128k achieves a score with a collection size of 128k with values 38.0, 38.0, 35.7, 26.0, 30.0, 26.0, 22.0, 24.0, 20.6, 16.0, 16.0, 14.6, 14.0, 16.0, and 10.3.',\n",
       "  'GPT-4-128k (chunk) achieved scores of 71.3, 74.7, 45.7, 59.5, 59.3, 62.3, 40.7, 50.5, 41.0, 43.0, 23.0, 32.1, 58.0, 69.5, 9.50, and 25.5.',\n",
       "  'Long-context LLM uses GPT-4-128k for reading full text or for segmenting the text into chunks.',\n",
       "  'GPT-4-128k is employed to read retrieved chunks and answer questions in RAG.',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'GPT-4-128k (chunk with notes) also has a maximum token count of 4,000, showing performance metrics of 22.0, 32.0, 24.2, 26.0, 30.0, 21.3, 28.0, 32.0, 22.0, 24.0, 26.0, 17.4, 26.0, 26.0, and 14.8.',\n",
       "  'GPT-4-128k is used for both the method and baseline approaches in the experiments.',\n",
       "  'The superior performance of GPT-4-128k lies in its ability to process long texts and execute multi-hop reasoning tasks.',\n",
       "  'The performance of GPT-4-128k full-text reading degrades gradually with an increase in the length of the input context.',\n",
       "  'As the context length increases, the impact of the “lost in the middle” effect on GPT-4-128k becomes progressively more severe.',\n",
       "  'The GraphReader achieves a performance gain of 10.53% relatively on LR-1 over GPT-4-128k full-text reading under 16k context length.',\n",
       "  'GPT-4-128k (chunk) has a maximum token count of 4,000 with varying data points across different categories: 18.0, 22.0, 24.6, 16.0, 20.0, 17.7, 20.0, 24.0, 17.0, 20.0, 24.0, 14.7, 28.0, 30.0, and 10.7.'],\n",
       " '16k': ['HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Various input window sizes are evaluated: 4k, 10k, 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Average scores were recorded across various input lengths (16k, 32k, 64k, 128k, 256k).',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.',\n",
       "  'The results shown in Table 3 indicate that the GraphReader outperforms all baseline methods across text lengths ranging from 16k to 256k tokens.',\n",
       "  'The GraphReader achieves a performance gain of 10.53% relatively on LR-1 over GPT-4-128k full-text reading under 16k context length.',\n",
       "  'The performance metrics for LR-1 show scores like 10.0, 16.0, 12.0 for 4k, 10k, and 16k inputs respectively.',\n",
       "  'As context length increases from 16k to 256k, recall performance is analyzed.'],\n",
       " '256k': ['For datasets with lengths of 256k and 128k, the max_words is 10000 and min_words is 2000.',\n",
       "  'The context benchmark consists of five length levels up to 256k.',\n",
       "  'HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The input window for GPT-4-128k is 128k and 256k.',\n",
       "  'The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Various input window sizes are evaluated: 4k, 10k, 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Average scores were recorded across various input lengths (16k, 32k, 64k, 128k, 256k).',\n",
       "  'Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.',\n",
       "  'The results shown in Table 3 indicate that the GraphReader outperforms all baseline methods across text lengths ranging from 16k to 256k tokens.',\n",
       "  'As context length increases from 16k to 256k, recall performance is analyzed.'],\n",
       " 'large margin': ['Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin.'],\n",
       " 'method': ['The analysis evaluates the inference cost of the approach.',\n",
       "  'The recall rate of supporting facts is evaluated for different methods using GPT-4-128k with a temperature of 0.1.',\n",
       "  'The method is compared with similar approaches for handling long texts with small input windows.',\n",
       "  'The methods are detailed in Appendix D.',\n",
       "  'The method enables handling overly long texts with a limited input window.',\n",
       "  'The comparison includes methods and their performance metrics: LR-1, LR-2 and F1 scores are measured.',\n",
       "  'The proposed method involves dividing the entire text into chunks.',\n",
       "  'A new method is proposed that does not lose information and allows for better comparison.',\n",
       "  'ReadAgent significantly underperforms the method in handling extremely long contexts.',\n",
       "  'The approach is also used for counting atomic facts.',\n",
       "  'Existing methods struggle with multi-hop questions.',\n",
       "  'Our approach demonstrates superior performance in multi-hop long-context tasks.',\n",
       "  'Results are obtained from three types of methods on four multi-hop long-context benchmarks and one single-hop task.',\n",
       "  'Our approach consistently performs better than all baselines on four long-context benchmarks.',\n",
       "  'GPT-4-128k is used for both the method and baseline approaches in the experiments.',\n",
       "  'The method is capable of extracting more valid information from chunks during exploration.',\n",
       "  'Our method can identify crucial information and search for supporting facts for input questions efficiently.',\n",
       "  'The approach of GraphReader unlocks the capabilities of constrained context window LLMs more efficiently in processing long context.',\n",
       "  'The method of GraphReader incorporates various elements.',\n",
       "  'The method is effective in handling extremely long texts by graph exploration with limited context window LLMs.',\n",
       "  'The approach demonstrated superior performance on four challenging single-hop and multi-hop benchmarks.',\n",
       "  'The method can effectively address challenges of processing extremely long context with limited context window LLMs by exploring graphs containing fine-grained information.',\n",
       "  \"Our method benefits from the graph's ability to capture relationships between detailed information.\"],\n",
       " 'superior perform': ['The superior performance of GPT-4-128k lies in its ability to process long texts and execute multi-hop reasoning tasks.',\n",
       "  'GraphReader achieves superior performance in complex single-hop and multi-hop QA tasks.',\n",
       "  'Our approach demonstrates superior performance in multi-hop long-context tasks.',\n",
       "  'The approach demonstrated superior performance on four challenging single-hop and multi-hop benchmarks.'],\n",
       " 'benchmark': ['Extensive experiments were conducted on four challenging benchmarks.',\n",
       "  'Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.',\n",
       "  'Table 1 provides statistics about these benchmarks.',\n",
       "  'The approach demonstrated superior performance on four challenging single-hop and multi-hop benchmarks.',\n",
       "  'The total number of benchmarks is denoted by #Samples.',\n",
       "  'The statistics of benchmarks employed in the evaluation are presented in Table 1.',\n",
       "  'The lengths of four benchmarks are significantly shorter than the 128k context window.'],\n",
       " 'single hop': ['Experiments demonstrate that GraphReader outperforms GPT-4 with a 128k input length across various long-context single-hop and multi-hop question-answering benchmarks.',\n",
       "  'NarrativeQA is a single-hop QA dataset that tests comprehension abilities for long documents sourced from movie scripts.',\n",
       "  'Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.',\n",
       "  'The Chunk Read approach is suitable for single-hop QA tasks.',\n",
       "  'The approach demonstrated superior performance on four challenging single-hop and multi-hop benchmarks.',\n",
       "  'The most commonly used action on multi-hop QA tasks is to read neighbor nodes.',\n",
       "  'In a single-hop QA dataset, the information needed to answer questions appears at a single location within the text.',\n",
       "  'Beyond the threshold of 5 initial nodes, performance declines, especially in single-hop scenarios.',\n",
       "  'The most common action on single-hop QA tasks is to read chunks.',\n",
       "  'Experiments were conducted with different initial node counts on multi-hop and single-hop QA datasets to assess the effect of initial nodes on GraphReader’s performance.',\n",
       "  'Single-hop data sets often require only one atomic fact.',\n",
       "  'The Multi-hop QA Datasets include HotpotQA, which features a collection of 2-hop questions directly authored by native speakers.',\n",
       "  'GraphReader achieves superior performance in complex single-hop and multi-hop QA tasks.',\n",
       "  'The current chunk is suitable for multi-hop QA tasks.',\n",
       "  'Results are obtained from three types of methods on four multi-hop long-context benchmarks and one single-hop task.'],\n",
       " 'multi hop': ['Agent-level approaches fail to capture multi-hop and long-range dependencies.',\n",
       "  'Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'The graph structure effectively captures long-range dependencies and multi-hop relationships within long text.',\n",
       "  'Experiments demonstrate that GraphReader outperforms GPT-4 with a 128k input length across various long-context single-hop and multi-hop question-answering benchmarks.',\n",
       "  'The rational plan is aimed at addressing complex real-world multi-hop questions.',\n",
       "  'Multi-hop questions need to gather information contained by multiple nodes to answer questions.',\n",
       "  'The approach demonstrated superior performance on four challenging single-hop and multi-hop benchmarks.',\n",
       "  'Model-level methods limit the ability of LLMs to address complex tasks, such as multi-hop questions.',\n",
       "  'Existing methods struggle with multi-hop questions.',\n",
       "  'The strategy significantly boosts the agent’s capability in multi-hop reasoning and capturing long-range dependencies of key information.',\n",
       "  'The Multi-hop QA Datasets include HotpotQA, which features a collection of 2-hop questions directly authored by native speakers.'],\n",
       " 'progress': ['Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023).'],\n",
       " 'natural language understand': ['Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023).'],\n",
       " 'natural language gener': ['Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023).'],\n",
       " 'zhao et al   2023': ['Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023).'],\n",
       " 'transformer based llm': ['Transformer-based LLMs struggle to handle long contexts due to limitations of context window and memory usage.'],\n",
       " 'struggl': ['Transformer-based LLMs struggle to handle long contexts due to limitations of context window and memory usage.',\n",
       "  'Existing methods struggle with multi-hop questions.'],\n",
       " 'limit': ['Transformer variants with modified attention mechanisms have been proposed to address limitations in long-context LLMs.',\n",
       "  'Transformer-based LLMs struggle to handle long contexts due to limitations of context window and memory usage.'],\n",
       " 'context window': ['The context window remains constrained by a predefined fixed length despite extensive expansion.',\n",
       "  'Transformer-based LLMs struggle to handle long contexts due to limitations of context window and memory usage.',\n",
       "  'It is impractical to include all original text chunks related to a node within the context window.',\n",
       "  'Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian published a paper in 2023 titled \"Extending context window of large language models via positional interpolation\".',\n",
       "  'The context window will limit the effectiveness of RAG methods.',\n",
       "  'Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole authored \"Yarn: Efficient context window extension of large language models\" in 2023.',\n",
       "  'All atomic facts associated with a node can fit within the context window.'],\n",
       " 'memory usag': ['Transformer-based LLMs struggle to handle long contexts due to limitations of context window and memory usage.'],\n",
       " 'current techniqu': ['Current techniques for solving long-context tasks of LLMs can be divided into two perspectives: model-level and fine-tuning.'],\n",
       " 'divid': ['Current techniques for solving long-context tasks of LLMs can be divided into two perspectives: model-level and fine-tuning.'],\n",
       " 'two perspect': ['Current techniques for solving long-context tasks of LLMs can be divided into two perspectives: model-level and fine-tuning.'],\n",
       " 'model level': ['Long-context LLMs optimized with model-level methods overlook crucial details in long contexts.',\n",
       "  'Model-level methods train large language models (LLMs) with target length texts.',\n",
       "  'Model-level methods limit the ability of LLMs to address complex tasks, such as multi-hop questions.',\n",
       "  'Current techniques for solving long-context tasks of LLMs can be divided into two perspectives: model-level and fine-tuning.'],\n",
       " 'fine tun': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".',\n",
       "  'Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.',\n",
       "  'Current techniques for solving long-context tasks of LLMs can be divided into two perspectives: model-level and fine-tuning.'],\n",
       " 'modified positional embed': ['Modified positional embeddings were studied by Chen et al. (2023b), Zhu et al. (2023), Peng et al. (2023), and Ding et al. (2024).'],\n",
       " 'zhu et al ': ['Recent efforts by Chen et al., Ding et al., and Peng have focused on positional interpolation to enhance long-context capabilities.',\n",
       "  'Modified positional embeddings were studied by Chen et al. (2023b), Zhu et al. (2023), Peng et al. (2023), and Ding et al. (2024).'],\n",
       " 'peng et al ': ['Modified positional embeddings were studied by Chen et al. (2023b), Zhu et al. (2023), Peng et al. (2023), and Ding et al. (2024).'],\n",
       " 'ding et al ': ['Recent efforts by Chen et al., Ding et al., and Peng have focused on positional interpolation to enhance long-context capabilities.',\n",
       "  'Modified positional embeddings were studied by Chen et al. (2023b), Zhu et al. (2023), Peng et al. (2023), and Ding et al. (2024).'],\n",
       " '2023': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".',\n",
       "  'LongBench is authored by Bai et al. in 2023.',\n",
       "  'Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian published a paper in 2023 titled \"Extending context window of large language models via positional interpolation\".',\n",
       "  'Chunks were investigated by Liu in 2024 and the LangChain-team in 2024.',\n",
       "  'Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.',\n",
       "  'ReadAgent is authored by Lee et al., 2024.',\n",
       "  'Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.',\n",
       "  'Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer authored the paper titled \"A human-inspired reading agent with gist memory of very long contexts\" in 2024.',\n",
       "  'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"',\n",
       "  'LangChain-team published a paper titled \"LangChain\" in 2024.',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"',\n",
       "  'Learning-based strategies were explored by Khattab and Zaharia in 2020, Sachan et al. in 2023, and Sun et al. in 2021.',\n",
       "  'Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.',\n",
       "  'The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.',\n",
       "  'The study by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li in 2024a addressed \"Long context alignment with short instructions and synthesized positions.\"',\n",
       "  'Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz authored the paper titled \"Walking down the memory maze: Beyond context limit through interactive methods.\"',\n",
       "  'Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.',\n",
       "  'The evaluation metrics are introduced by LV-Eval in 2024.',\n",
       "  'The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.',\n",
       "  'Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"',\n",
       "  'Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.',\n",
       "  'Jerry Liu published \"Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources\" in 2023.',\n",
       "  'The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.',\n",
       "  'Modified positional embeddings were studied by Chen et al. (2023b), Zhu et al. (2023), Peng et al. (2023), and Ding et al. (2024).',\n",
       "  'MuSiQue was developed by Trivedi et al. in 2022.',\n",
       "  'The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.',\n",
       "  'The paper titled \"Musique: Multi-hop questions via single-hop question composition\" was authored by Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal in 2022.',\n",
       "  'The year of publication is 2023, and it discusses how \"questions are all you need to train a dense passage retriever.\"',\n",
       "  'Albert Gu and Tri Dao introduced \"Mamba: Linear-time sequence modeling with selective state spaces\" in 2023.',\n",
       "  'Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal published \"Leave no context behind: Efficient infinite context transformers with infinite\" in 2024.',\n",
       "  'Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.',\n",
       "  'Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole authored \"Yarn: Efficient context window extension of large language models\" in 2023.',\n",
       "  'The Twelfth International Conference on Learning Representations took place in 2023.',\n",
       "  'Modified attention mechanisms were studied by Dai et al. (2019), Munkhdalai et al. (2024), and Gu and Dao (2023).',\n",
       "  'Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'transformer vari': ['Transformer variants with modified attention mechanisms have been proposed to address limitations in long-context LLMs.',\n",
       "  'Transformer variants were applied in the research.'],\n",
       " 'appli': ['Transformer variants were applied in the research.'],\n",
       " 'first four author': ['The first four authors contributed equally to the project.'],\n",
       " 'contributed equ': ['The first four authors contributed equally to the project.'],\n",
       " 'project': ['The first four authors contributed equally to the project.'],\n",
       " 'corresponding author': ['The corresponding author was designated for the study.'],\n",
       " 'design': ['The goal is to design an agent that autonomously explores the graph using predefined functions.',\n",
       "  'GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'The corresponding author was designated for the study.',\n",
       "  'E. J. Lennox designed several other city landmarks.'],\n",
       " 'studi': ['Modified attention mechanisms were studied by Dai et al. (2019), Munkhdalai et al. (2024), and Gu and Dao (2023).',\n",
       "  'The project leader was mentioned in the study.',\n",
       "  'The corresponding author was designated for the study.'],\n",
       " 'project lead': ['The project leader was mentioned in the study.'],\n",
       " 'mention': ['The project leader was mentioned in the study.',\n",
       "  'Agent-level techniques were mentioned regarding the processing of long contexts.'],\n",
       " 'average scor': ['The average scores for different metrics are illustrated in the graph, with LR-1 and LR-2 as comparisons.',\n",
       "  'Average scores were recorded across various input lengths (16k, 32k, 64k, 128k, 256k).'],\n",
       " '32k': ['HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Various input window sizes are evaluated: 4k, 10k, 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Average scores were recorded across various input lengths (16k, 32k, 64k, 128k, 256k).',\n",
       "  'For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.'],\n",
       " '64k': ['HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Various input window sizes are evaluated: 4k, 10k, 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Average scores were recorded across various input lengths (16k, 32k, 64k, 128k, 256k).',\n",
       "  'For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.',\n",
       "  'Recall performance at sample-wise granularity is 64.7% for atomic facts and 85.3% for the final notebook.'],\n",
       " '128k': ['For datasets with lengths of 256k and 128k, the max_words is 10000 and min_words is 2000.',\n",
       "  'HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The context length increasing to 128k resulted in a performance gain of 75.00% over GPT-4-128k.',\n",
       "  'The input window for GPT-4-128k is 128k and 256k.',\n",
       "  'The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'ReadAgent has a capacity of 128k, displaying efficiency measures of 24.0, 26.0, 29.2, 20.0, 22.0, 16.9, 24.0, 30.0, 15.3, 14.0, 18.0, 13.6, 20.0, 22.0, and 10.4.',\n",
       "  'Various input window sizes are evaluated: 4k, 10k, 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'Average scores were recorded across various input lengths (16k, 32k, 64k, 128k, 256k).'],\n",
       " 'existing open sourced model': ['GraphReader outperformed existing open-sourced and closed-source models in performance on the LV-Eval dataset.'],\n",
       " 'existing closed source model': ['GraphReader outperformed existing open-sourced and closed-source models in performance on the LV-Eval dataset.'],\n",
       " 'demonstr': ['GraphReader demonstrated scalable performance in very long contexts.',\n",
       "  'Our approach demonstrates superior performance in multi-hop long-context tasks.',\n",
       "  'GraphReader demonstrates scalability and effectiveness in processing long contexts.'],\n",
       " 'scalable perform': ['GraphReader demonstrated scalable performance in very long contexts.'],\n",
       " 'model': ['ReadAgent feeds all mixed memories to the model for page number selection.',\n",
       "  'Shorter benchmark lengths mitigate the impact of “lost in the middle” on model performance.',\n",
       "  'The dataset includes various models such as NarrativeQA and HotpotWikiQA-mixup16k.',\n",
       "  'Other models exhibited a significant decrease in performance as context length increases.',\n",
       "  'These models are prone to losing earlier information.',\n",
       "  'The model consistently outperforms other baseline methods.'],\n",
       " 'exhibit': ['Other models exhibited a significant decrease in performance as context length increases.'],\n",
       " 'significant decreas': ['Other models exhibited a significant decrease in performance as context length increases.'],\n",
       " 'perform': ['The atomic fact regarding the performer of \"Never Too Loud\" is derived from Chunk ID-6.',\n",
       "  'The performer or band associated with \"Never Too Loud\" is Danko Jones.',\n",
       "  'Increasing the number of recalled chunks could improve the performance of text retrieval.',\n",
       "  'The main contributions are threefold: the introduction of GraphReader, the establishment of a scalable long-context capability, and the demonstration of performance.',\n",
       "  'The performance of GraphReader closely matches that achieved by directly supplying supporting facts to the LLM.',\n",
       "  'The name of the castle in the city is Casa Loma.',\n",
       "  'The performance of GPT-4-128k full-text reading degrades gradually with an increase in the length of the input context.',\n",
       "  'Other models exhibited a significant decrease in performance as context length increases.',\n",
       "  'The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'Casa Loma is located in the city where the performer of Never Too Loud was formed.',\n",
       "  'The city where the performer of \"Never Too Loud\" was formed is Toronto, Canada.',\n",
       "  'The agent performs two initializations: defining a rational plan and selecting the initial node.',\n",
       "  'The performer of the album Never Too Loud is Danko Jones.',\n",
       "  'The results indicate that RAG methods based on BM25 and Ada-002 exhibit the worst performance compared to long-context LLM and agent-based methods.',\n",
       "  'Figure 3 shows the performance of GraphReader with different initial node numbers on 2WikiMultihopQA and NarrativeQA.'],\n",
       " 'modified attention mechan': ['Modified attention mechanisms were studied by Dai et al. (2019), Munkhdalai et al. (2024), and Gu and Dao (2023).',\n",
       "  'Transformer variants with modified attention mechanisms have been proposed to address limitations in long-context LLMs.'],\n",
       " 'dai et al ': ['Modified attention mechanisms were studied by Dai et al. (2019), Munkhdalai et al. (2024), and Gu and Dao (2023).'],\n",
       " 'munkhdalai et al ': ['Modified attention mechanisms were studied by Dai et al. (2019), Munkhdalai et al. (2024), and Gu and Dao (2023).'],\n",
       " 'gu and dao': ['Modified attention mechanisms were studied by Dai et al. (2019), Munkhdalai et al. (2024), and Gu and Dao (2023).'],\n",
       " '2019': ['Modified attention mechanisms were studied by Dai et al. (2019), Munkhdalai et al. (2024), and Gu and Dao (2023).',\n",
       "  'Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis authored the paper titled \"Generalization through memorization: Nearest neighbor language models\" in 2019.',\n",
       "  'Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".',\n",
       "  'Tokens were investigated by Khandelwal et al. in 2019.'],\n",
       " 'agent level techniqu': ['Agent-level techniques were mentioned regarding the processing of long contexts.'],\n",
       " 'retrieval augmented llm': ['Retrieval-augmented LLMs or agents were employed to process long contexts.'],\n",
       " 'employ': ['The method employed for reasoning is Chain-of-Thought, as mentioned by Wei et al., 2022.',\n",
       "  'The agent employs a coarse-to-fine strategy, progressing from reading atomic facts to the original text.',\n",
       "  'The ReadAgent-S method is employed.',\n",
       "  'Retrieval-augmented LLMs or agents were employed to process long contexts.'],\n",
       " 'training dataset': ['Model-level methods train large language models (LLMs) with target length texts.'],\n",
       " 'training cost': ['Model-level methods train large language models (LLMs) with target length texts.',\n",
       "  'Positional interpolation methods require training on full-length texts, leading to significant increases in data and training costs.'],\n",
       " 'overlooking detail': ['The phenomenon of overlooking details in long contexts is known as “lost in the middle.”',\n",
       "  'When the chunk size L exceeds a certain threshold, performance declines because larger chunks cause the model to overlook essential details.'],\n",
       " 'lost in the middl': ['The phenomenon of overlooking details in long contexts is known as “lost in the middle.”',\n",
       "  'Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.',\n",
       "  'Shorter benchmark lengths mitigate the impact of “lost in the middle” on model performance.'],\n",
       " 'task': ['Abilities related to reasoning and decision-making have been utilized in complex tasks like function call and KGQA.',\n",
       "  'The task requires determining if the text chunk can help answer the previous question.',\n",
       "  'The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'Model-level methods limit the ability of LLMs to address complex tasks, such as multi-hop questions.',\n",
       "  'The task is to assess all neighboring nodes of the current node.'],\n",
       " 'agent based method': ['Agent-based Method selects ReadAgent for its evaluation.',\n",
       "  'Agent-level approaches fail to capture multi-hop and long-range dependencies.',\n",
       "  'Agent-level approaches transform input text into a tree or paginated pages.',\n",
       "  'An agent-based system is employed for the execution of retrieval and reading processes for long-context QA.',\n",
       "  'The approach is compared with baseline methods: retrieval augmented generation (RAG), long-context LLM, and agent-based methods.',\n",
       "  'The results indicate that RAG methods based on BM25 and Ada-002 exhibit the worst performance compared to long-context LLM and agent-based methods.',\n",
       "  'The effectiveness of agent-level approaches is limited on very long contexts.'],\n",
       " 'input': ['The performance metrics for LR-2 correspond to scores like 18.0, 11.9, and 6.0 for increasing input sizes.',\n",
       "  'Agent-level approaches transform input text into a tree or paginated pages.',\n",
       "  'The performance metrics for LR-1 show scores like 10.0, 16.0, 12.0 for 4k, 10k, and 16k inputs respectively.'],\n",
       " 'tree': ['Agent-level approaches transform input text into a tree or paginated pages.'],\n",
       " 'paginated pag': ['Agent-level approaches transform input text into a tree or paginated pages.'],\n",
       " 'long range depend': ['The strategy significantly boosts the agent’s capability in multi-hop reasoning and capturing long-range dependencies of key information.',\n",
       "  'Agent-level approaches fail to capture multi-hop and long-range dependencies.',\n",
       "  'GraphReader successfully establishes long-range dependencies within a relatively small 4k context window.',\n",
       "  'The graph structure effectively captures long-range dependencies and multi-hop relationships within long text.'],\n",
       " 'effect': ['Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration.',\n",
       "  'The effectiveness of employing a limited context window LLM for long-context tasks with the GraphReader is demonstrated in previous experiments.',\n",
       "  'GraphReader demonstrates scalability and effectiveness in processing long contexts.',\n",
       "  'The context window will limit the effectiveness of RAG methods.',\n",
       "  'Pre-planning the solution helps tackle these complex questions effectively.',\n",
       "  'The method is effective in handling extremely long texts by graph exploration with limited context window LLMs.',\n",
       "  'The overall performance reflects different methodologies and their effectiveness in handling the datasets.',\n",
       "  'The reading of the pages should be in sequence to be the most effective.',\n",
       "  'These statistics indicate the effectiveness of GraphReader.',\n",
       "  'The effectiveness of the rational plan is verified by removing it during agent initialization and conducting experiments on four long-context QA benchmarks.',\n",
       "  \"The findings indirectly reflect GraphReader's intelligence and effectiveness in exploration.\",\n",
       "  'Experiments were conducted with different initial node counts on multi-hop and single-hop QA datasets to assess the effect of initial nodes on GraphReader’s performance.',\n",
       "  'The effectiveness of agent-level approaches is limited on very long contexts.'],\n",
       " 'graph based ag': ['The proposed graph-based agent is named GraphReader.',\n",
       "  'This paper introduces GraphReader, a graph-based agent designed to enhance the long-context capabilities of large language models.'],\n",
       " 'discrete chunk': ['GraphReader segments long texts into discrete chunks.'],\n",
       " 'text seg': ['For ReadAgent, the evaluation focuses on the final text segments reviewed.',\n",
       "  'GraphReader extracts essential information from text segments.'],\n",
       " 'key el': ['The LLM also extracts key elements, including essential nouns, verbs, and adjectives from each atomic fact.',\n",
       "  'A graph comprises text chunks, atomic facts, and nodes, which are key elements in the text.',\n",
       "  'Nodes represent key elements (nouns, verbs, adjectives) that correspond with various atomic facts.',\n",
       "  'The agent evaluates the key elements of all nodes V and selects Ninitial nodes.',\n",
       "  'A final set of key elements is created post-normalization.',\n",
       "  'GraphReader compresses key elements and atomic facts into a graph.',\n",
       "  'The process involves normalizing key elements to manage lexical noise and granularity issues, following Lu et al. (2023).',\n",
       "  'The number of key elements occurring simultaneously in each atomic fact is generally of this magnitude.',\n",
       "  'Each node vi=(ki,Ai) is constructed, where ki is a key element and Ai is the set of atomic facts corresponding to ki.',\n",
       "  'Nodes are key elements in the text that correlate with several atomic facts derived from different text chunks.'],\n",
       " 'atomic fact': ['There is a relatively even distribution of atomic facts in the nodes.',\n",
       "  'Fully processed atomic facts and chunk queue indicate thorough exploration of the current node.',\n",
       "  'The agent explores each initial node by first exploring atomic facts.',\n",
       "  'In HotpotQA, the Exploring Atomic Facts stage has a ratio of 42.0%.',\n",
       "  'This step is essential because atomic facts merely sum up the overall text.',\n",
       "  'Nodes represent key elements (nouns, verbs, adjectives) that correspond with various atomic facts.',\n",
       "  'GraphReader compresses key elements and atomic facts into a graph.',\n",
       "  'The agent checks all neighboring nodes and performs one of two functions: reading a neighboring node or terminating.',\n",
       "  'In MuSiQue, the Exploring Atomic Facts stage has a ratio of 40.0%.',\n",
       "  'Single-hop data sets often require only one atomic fact.',\n",
       "  'Text chunks are segments of the original text, while atomic facts are the smallest truths extracted from those chunks.',\n",
       "  'The number of atomic facts for WikiMultihopQA has an average of 217.7 and a maximum of 545.0.',\n",
       "  'Atomic facts are the smallest, indivisible truths extracted from text chunks.',\n",
       "  'The average number of atomic facts in the node with the most atomic facts in each graph ranges from 15 to 50.',\n",
       "  'The average of atomic facts per neighbor node in WikiMultihopQA is 2.1 with a maximum of 17.0.',\n",
       "  'In WikiMultihopQA, the Exploring Atomic Facts stage has a ratio of 40.4%.',\n",
       "  'The LLM extracts atomic facts from text chunks and simplifies them.',\n",
       "  'For longer texts, there tends to be a higher average number of nodes and atomic facts.',\n",
       "  'Each node vi=(ki,Ai) is constructed, where ki is a key element and Ai is the set of atomic facts corresponding to ki.',\n",
       "  'It is emphasized that even slightly relevant atomic facts should be explored.',\n",
       "  'The number of atomic facts for MusiQue has an average of 419.9 and a maximum of 586.0.',\n",
       "  'The number of atomic facts for HotpotQA has an average of 244.0 and a maximum of 645.0.',\n",
       "  'The approach is also used for counting atomic facts.',\n",
       "  'A graph comprises text chunks, atomic facts, and nodes, which are key elements in the text.',\n",
       "  'Smaller chunks lead to more semantic truncation, hindering comprehension and accuracy in extracting atomic facts.',\n",
       "  'The average of atomic facts per neighbor node in MusiQue is 2.1 with a maximum of 15.6.',\n",
       "  'The average of atomic facts per neighbor node in HotpotQA is 2.1 with a maximum of 17.8.',\n",
       "  'Based on a given question, the agent progressively accesses information from coarse key elements and atomic facts to detailed original text chunks.',\n",
       "  'The number of key elements occurring simultaneously in each atomic fact is generally of this magnitude.',\n",
       "  'On average, each node is associated with about 2 atomic facts.',\n",
       "  'The agent captures an overview of each chunk by reading all groups of atomic facts.',\n",
       "  'Nodes are key elements in the text that correlate with several atomic facts derived from different text chunks.',\n",
       "  'A graph has been created from the text, comprising text chunks, atomic facts, and nodes.',\n",
       "  'The agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks.',\n",
       "  'The LLM summarizes each chunk into atomic facts to extract nodes.',\n",
       "  'The agent employs a coarse-to-fine strategy, progressing from reading atomic facts to the original text.',\n",
       "  'The maximum average number of atomic facts is found in NarrativeQA.',\n",
       "  'All atomic facts associated with a node can fit within the context window.',\n",
       "  'Recall performance at sample-wise granularity is 64.7% for atomic facts and 85.3% for the final notebook.',\n",
       "  'The recall for the final notebook is slightly higher than the recall of atomic facts.',\n",
       "  'GraphReader shows a recall performance of 76.4% for atomic facts and 90.5% for the final notebook at SF-wise granularity.'],\n",
       " 'node': ['There is a relatively even distribution of atomic facts in the nodes.',\n",
       "  'A graph has been created from the text, comprising text chunks, atomic facts, and nodes.',\n",
       "  'It is impractical to include all original text chunks related to a node within the context window.',\n",
       "  'Strategy involves reflecting on previous actions to prevent redundant revisiting of nodes or chunks.',\n",
       "  'The necessity of selecting which nodes to visit is based on reasoning about required information.',\n",
       "  'A graph comprises text chunks, atomic facts, and nodes, which are key elements in the text.',\n",
       "  'The average number of atomic facts in the node with the most atomic facts in each graph ranges from 15 to 50.',\n",
       "  'Nodes represent key elements (nouns, verbs, adjectives) that correspond with various atomic facts.',\n",
       "  'The construction of the graph involves nodes representing key information.',\n",
       "  'The agent evaluates the key elements of all nodes V and selects Ninitial nodes.',\n",
       "  'The edges in the graph represent relationships between nodes.',\n",
       "  'If the agent deems that none of the chunks are worth further reading, it finishes reading the node.',\n",
       "  'On average, each node is associated with about 2 atomic facts.',\n",
       "  'All atomic facts associated with a node can fit within the context window.',\n",
       "  'For longer texts, there tends to be a higher average number of nodes and atomic facts.',\n",
       "  'The initial node is selected from all nodes based on the rational plan.',\n",
       "  'Nodes are key elements in the text that correlate with several atomic facts derived from different text chunks.'],\n",
       " 'key inform': ['The agent will summarize key information and provide brief clues.',\n",
       "  'The construction of the graph involves nodes representing key information.',\n",
       "  'Pre-planning, reflection, and various actions are important for using a graph that contains key information.',\n",
       "  'The plan specifies the key information required to formulate a comprehensive answer.',\n",
       "  'The strategy significantly boosts the agent’s capability in multi-hop reasoning and capturing long-range dependencies of key information.',\n",
       "  'The agent identifies the key information needed.',\n",
       "  'The graph was constructed with GraphReader being able to swiftly locate key information while minimizing resource usage.'],\n",
       " 'graph structur': ['The graph structure effectively captures long-range dependencies and multi-hop relationships within long text.',\n",
       "  'GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'The GraphReader captures global information using a graph structure.',\n",
       "  'GraphReader organizes long texts into graph structures and employs an autonomous agent to explore the graph.'],\n",
       " 'access': ['The agent needs to access the next node after processing.',\n",
       "  'Based on a given question, the agent progressively accesses information from coarse key elements and atomic facts to detailed original text chunks.'],\n",
       " 'coarse key el': ['Based on a given question, the agent progressively accesses information from coarse key elements and atomic facts to detailed original text chunks.'],\n",
       " 'chunk': ['Chunks are grouped by corresponding chunk IDs and fed to the agent.',\n",
       "  'Chunks were investigated by Liu in 2024 and the LangChain-team in 2024.',\n",
       "  'If the agent deems that none of the chunks are worth further reading, it finishes reading the node.',\n",
       "  'Text chunks are segments of the original text, while atomic facts are the smallest truths extracted from those chunks.',\n",
       "  'The process involves selecting the top-1 chunk for answering questions.',\n",
       "  'The assistant must assess whether the available information in a text chunk is sufficient to answer a question.',\n",
       "  'Atomic facts are the smallest, indivisible truths extracted from text chunks.',\n",
       "  'The task requires determining if the text chunk can help answer the previous question.',\n",
       "  'The agent determines which chunk is likely to contain useful information.',\n",
       "  'When opting for the top-3 chunks, the maximum length of each chunk is set to 1k.',\n",
       "  'The LLM extracts atomic facts from text chunks and simplifies them.',\n",
       "  'Text chunks are segments of the original text.',\n",
       "  'The proposed method involves dividing the entire text into chunks.',\n",
       "  'The instruction is to read text chunks and answer the question.',\n",
       "  'Long-context LLM uses GPT-4-128k for reading full text or for segmenting the text into chunks.',\n",
       "  'GPT-4-128k is employed to read retrieved chunks and answer questions in RAG.',\n",
       "  'Relevance scores are calculated between the question and the chunks.',\n",
       "  'It is impractical to include all original text chunks related to a node within the context window.',\n",
       "  'Strategy involves reflecting on previous actions to prevent redundant revisiting of nodes or chunks.',\n",
       "  'The maximum length of the chunk is set to fill the input window as much as possible.',\n",
       "  'When the chunk queue is non-empty, it indicates the agent has identified multiple text chunks of interest.',\n",
       "  'A graph comprises text chunks, atomic facts, and nodes, which are key elements in the text.',\n",
       "  'Smaller chunks lead to more semantic truncation, hindering comprehension and accuracy in extracting atomic facts.',\n",
       "  'The text is split into chunks using the method from GraphReader.',\n",
       "  'Based on a given question, the agent progressively accesses information from coarse key elements and atomic facts to detailed original text chunks.',\n",
       "  'The agent traverses the queue, reading each chunk.',\n",
       "  'The top-n chunks with the highest relevance scores are input for the LLM to answer.',\n",
       "  'Nodes are key elements in the text that correlate with several atomic facts derived from different text chunks.',\n",
       "  'A graph has been created from the text, comprising text chunks, atomic facts, and nodes.',\n",
       "  'The method is capable of extracting more valid information from chunks during exploration.',\n",
       "  'The agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks.',\n",
       "  'The LLM summarizes each chunk into atomic facts to extract nodes.',\n",
       "  'Specific details are best obtained directly from the original text chunks.',\n",
       "  'When the chunk size L exceeds a certain threshold, performance declines because larger chunks cause the model to overlook essential details.',\n",
       "  'The experiment divides the chunks in the same way as GraphReader.',\n",
       "  'The document D is split into chunks while preserving paragraph structure.',\n",
       "  'Various levels of retrieval granularity include tokens, entities, and chunks.',\n",
       "  'The maximum length of the chunk is set to 2k.',\n",
       "  'Adjacent chunks might contain relevant and useful information.',\n",
       "  'The LLM reads these chunks sequentially according to the text order.'],\n",
       " 'takes not': ['The agent takes notes and reflects until it gathers sufficient information to generate an answer.'],\n",
       " 'help answ': ['The task requires determining if the text chunk can help answer the previous question.',\n",
       "  'The agent takes notes and reflects until it gathers sufficient information to generate an answer.',\n",
       "  'The accuracy of the latter is based on the question and correct answer.',\n",
       "  'If able to answer, the response should be [answerable] followed by the answer.',\n",
       "  'The plan specifies the key information required to formulate a comprehensive answer.',\n",
       "  'WebGPT simulates human actions to search the internet for specific answers.',\n",
       "  \"The agent's functions include assessing neighboring nodes to provide answers or denote completion of information gathering.\"],\n",
       " 'main contribut': ['The main contributions are threefold: the introduction of GraphReader, the establishment of a scalable long-context capability, and the demonstration of performance.'],\n",
       " 'organ': ['PEARL organizes documents into a tree structure.',\n",
       "  'KGP organizes documents into graphs.',\n",
       "  'MemWalker organizes documents into a tree structure.',\n",
       "  'GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.'],\n",
       " 'notebook': ['The agent continuously updates the notebook with relevant information during the exploration process.',\n",
       "  'GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'GraphReader answers the question based on memory from the notebook.',\n",
       "  'The agent starts by maintaining a notebook to record supporting facts.',\n",
       "  'The final answer must consider all available information collected from the notebooks.',\n",
       "  'Any supporting facts discovered will be recorded in the notebook.',\n",
       "  'The agent will select one of four functions based on the updated notebook.',\n",
       "  'The agent thinks about what can be added to the current notebook.',\n",
       "  'The agent utilizes the question, rational plan, and notes in its notebook to reflect on required clues.',\n",
       "  'GraphReader carries a notebook that records memory.',\n",
       "  'The analysis requires reviewing all notebooks before providing an answer.'],\n",
       " 'plan': ['GraphReader is designed to organize long texts into a graph structure, leveraging predefined functions and a notebook to facilitate planning and reflection during exploration.',\n",
       "  'In contrast, we employ agents that use planning and reflection to gather essential information.',\n",
       "  'Pre-planning the solution helps tackle these complex questions effectively.',\n",
       "  'KGP does not fully exploit the agent’s capabilities for planning and reflection.',\n",
       "  'Pre-planning, reflection, and various actions are important for using a graph that contains key information.',\n",
       "  'The plan specifies the key information required to formulate a comprehensive answer.',\n",
       "  'While reading chunks, the agent will consider the question and plan.'],\n",
       " 'establish': ['GraphReader establishes a scalable long-context capability based on a 4k context window.'],\n",
       " 'demonstrates perform': ['GraphReader demonstrates performance that is comparable to or surpasses GPT-4 with a 128k context window across varying context lengths.'],\n",
       " 'compar': ['GraphReader demonstrates performance that is comparable to or surpasses GPT-4 with a 128k context window across varying context lengths.'],\n",
       " 'surpass': ['GraphReader demonstrates performance that is comparable to or surpasses GPT-4 with a 128k context window across varying context lengths.'],\n",
       " 'gpt 4': ['Experiments demonstrate that GraphReader outperforms GPT-4 with a 128k input length across various long-context single-hop and multi-hop question-answering benchmarks.',\n",
       "  'The token number is calculated using the GPT-4 tokenizer from TikToken.',\n",
       "  'GraphReader is constructed using an off-the-shelf GPT-4 API.',\n",
       "  'The recall of supporting facts is evaluated using GPT-4 on the HotpotWikiQA-mixup dataset.',\n",
       "  'GraphReader demonstrates performance that is comparable to or surpasses GPT-4 with a 128k context window across varying context lengths.'],\n",
       " 'experi': ['Extensive experiments were conducted on four challenging benchmarks.',\n",
       "  'GPT-4-128k is used for both the method and baseline approaches in the experiments.',\n",
       "  'The experiments section denotes ongoing analysis and findings related to the outlined methodology.',\n",
       "  'Experiments demonstrate that GraphReader outperforms GPT-4 with a 128k input length across various long-context single-hop and multi-hop question-answering benchmarks.',\n",
       "  'The Effect of Node Selection is demonstrated through experiments on randomly selecting initial nodes and neighbor nodes.',\n",
       "  'The experiments are conducted on two types of long-context QA benchmarks.',\n",
       "  'The experiment divides the chunks in the same way as GraphReader.',\n",
       "  'In the experiments, datasets from LongBench are used.',\n",
       "  'The effectiveness of the rational plan is verified by removing it during agent initialization and conducting experiments on four long-context QA benchmarks.',\n",
       "  'Experiments were conducted with different initial node counts on multi-hop and single-hop QA datasets to assess the effect of initial nodes on GraphReader’s performance.',\n",
       "  'The input window is controlled to 4k in the experiments.',\n",
       "  'Baseline methods are referenced, indicating methods against which experiments may be compared.'],\n",
       " 'peng': ['Recent efforts by Chen et al., Ding et al., and Peng have focused on positional interpolation to enhance long-context capabilities.',\n",
       "  'Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.'],\n",
       " 'positional interpol': ['Recent efforts by Chen et al., Ding et al., and Peng have focused on positional interpolation to enhance long-context capabilities.',\n",
       "  'Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian published a paper in 2023 titled \"Extending context window of large language models via positional interpolation\".'],\n",
       " 'training on full length text': ['Positional interpolation methods require training on full-length texts, leading to significant increases in data and training costs.'],\n",
       " 'significant increas': ['Positional interpolation methods require training on full-length texts, leading to significant increases in data and training costs.'],\n",
       " 'data': ['Positional interpolation methods require training on full-length texts, leading to significant increases in data and training costs.'],\n",
       " 'pose': ['PoSE and SkipAlign investigate data skip strategy but tend to neglect detailed information in long texts.'],\n",
       " 'skipalign': ['PoSE and SkipAlign investigate data skip strategy but tend to neglect detailed information in long texts.'],\n",
       " 'data skip strategi': ['PoSE and SkipAlign investigate data skip strategy but tend to neglect detailed information in long texts.'],\n",
       " 'constrain': ['The context window remains constrained by a predefined fixed length despite extensive expansion.'],\n",
       " 'predefined fixed length': ['The context window remains constrained by a predefined fixed length despite extensive expansion.'],\n",
       " 'extensive expans': ['The context window remains constrained by a predefined fixed length despite extensive expansion.'],\n",
       " 'losing earlier inform': ['These models are prone to losing earlier information.'],\n",
       " 'retrieval augmented generation  rag ': ['Retrieval Augmented Generation (RAG) leverages an extensive database of documents to extract task-related information that aids in response generation.',\n",
       "  'Retrieval-Augmented Generation (RAG) addresses long-text problems.',\n",
       "  'Despite its capabilities, Retrieval Augmented Generation (RAG) faces challenges in addressing complex questions due to difficulties in developing robust decision-making mechanisms.',\n",
       "  'The approach is compared with baseline methods: retrieval augmented generation (RAG), long-context LLM, and agent-based methods.'],\n",
       " 'extensive database of docu': ['Retrieval Augmented Generation (RAG) leverages an extensive database of documents to extract task-related information that aids in response generation.'],\n",
       " 'task related inform': ['Retrieval Augmented Generation (RAG) leverages an extensive database of documents to extract task-related information that aids in response generation.'],\n",
       " 'gener': ['The LLM generates the final answer after considering all available information.',\n",
       "  'Retrieval Augmented Generation (RAG) leverages an extensive database of documents to extract task-related information that aids in response generation.',\n",
       "  'KGP primarily uses agents to generate queries.'],\n",
       " 'various level': ['Many efforts investigate various levels of retrieval granularity.'],\n",
       " 'retrieval granular': ['Many efforts investigate various levels of retrieval granularity.'],\n",
       " 'token': [\"The Full Text Read method cannot be used for texts that exceed the LLM's input window tokens.\",\n",
       "  \"The Full Text Read method can be used for texts that are fewer than the LLM's input window tokens.\",\n",
       "  'Tokens were investigated by Khandelwal et al. in 2019.',\n",
       "  'GraphReader uses 1.08 times more tokens than ReadAgent.',\n",
       "  'The results shown in Table 3 indicate that the GraphReader outperforms all baseline methods across text lengths ranging from 16k to 256k tokens.',\n",
       "  'Various levels of retrieval granularity include tokens, entities, and chunks.'],\n",
       " 'entiti': ['Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.',\n",
       "  'Various levels of retrieval granularity include tokens, entities, and chunks.',\n",
       "  'Entities were investigated by Févry et al. in 2020 and De Jong et al. in 2021.'],\n",
       " 'khandelwal et al ': ['Tokens were investigated by Khandelwal et al. in 2019.'],\n",
       " 'f vry et al ': ['Entities were investigated by Févry et al. in 2020 and De Jong et al. in 2021.'],\n",
       " '2020': ['2WikiMulti-hopQA was developed by Ho et al. in 2020.',\n",
       "  'Omar Khattab and Matei Zaharia authored the paper titled \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\" in 2020.',\n",
       "  'Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.',\n",
       "  'Entities were investigated by Févry et al. in 2020 and De Jong et al. in 2021.',\n",
       "  'Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.',\n",
       "  'Learning-based strategies were explored by Khattab and Zaharia in 2020, Sachan et al. in 2023, and Sun et al. in 2021.'],\n",
       " 'de jong et al ': ['Entities were investigated by Févry et al. in 2020 and De Jong et al. in 2021.'],\n",
       " '2021': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.',\n",
       "  'Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".',\n",
       "  'Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer authored a paper in 2021, questioning whether \"long-range language models actually use long-range context.\"',\n",
       "  'Entities were investigated by Févry et al. in 2020 and De Jong et al. in 2021.',\n",
       "  'Learning-based strategies were explored by Khattab and Zaharia in 2020, Sachan et al. in 2023, and Sun et al. in 2021.'],\n",
       " 'liu': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".',\n",
       "  'Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian published a paper in 2023 titled \"Extending context window of large language models via positional interpolation\".',\n",
       "  'Chunks were investigated by Liu in 2024 and the LangChain-team in 2024.',\n",
       "  'Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.',\n",
       "  'Yanan Wu, Jie Liu, Xingyuan Bu, and Jiaheng Liu are also relevant authors mentioned in the context of these works, though specific details about their contributions are not provided.',\n",
       "  'Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.',\n",
       "  'Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer authored the paper titled \"A human-inspired reading agent with gist memory of very long contexts\" in 2024.',\n",
       "  'Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"',\n",
       "  'The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.',\n",
       "  'The study by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li in 2024a addressed \"Long context alignment with short instructions and synthesized positions.\"',\n",
       "  'Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.',\n",
       "  'The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.',\n",
       "  'Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"',\n",
       "  'Yiran Ding, Li Lyna Zhang, Chengruidong Zhang are mentioned in the provided text without a corresponding publication title or year.',\n",
       "  'Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.',\n",
       "  'Jerry Liu published \"Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources\" in 2023.',\n",
       "  'The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.',\n",
       "  'The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.',\n",
       "  'Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.'],\n",
       " 'langchain': ['Chunks were investigated by Liu in 2024 and the LangChain-team in 2024.',\n",
       "  'LangChain-team published a paper titled \"LangChain\" in 2024.'],\n",
       " 'retrieval method': ['Retrieval methods based on Okapi BM25 and OpenAI API embedding model are included in the comparison.',\n",
       "  'Other approaches have explored diverse retrieval methods, such as BM25 and learning-based strategies.'],\n",
       " 'bm25': ['BM25 achieves a top-3 score of 4k with values 16.0, 22.0, 13.9, 18.0, 28.0, 13.3, 16.0, 18.0, 11.8, 12.0, 16.0, 11.8, 12.0, 22.0, and 9.3.',\n",
       "  'BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'The performance metrics for BM25 (top-1) and BM25 (top-3) vary across different evaluation metrics such as precision for multiple datasets.',\n",
       "  'BM25 (top-1) shows performance metrics for various input window sizes.',\n",
       "  'BM25 was explored by Rasooli and Tetreault in 2015.',\n",
       "  'Other approaches have explored diverse retrieval methods, such as BM25 and learning-based strategies.',\n",
       "  'The results indicate that RAG methods based on BM25 and Ada-002 exhibit the worst performance compared to long-context LLM and agent-based methods.',\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'The paper discusses the \"probabilistic relevance framework,\" specifically focusing on \"BM25 and beyond.\"'],\n",
       " 'strategi': ['Strategy involves reflecting on previous actions to prevent redundant revisiting of nodes or chunks.',\n",
       "  'There are two main strategies during the reading process: Chunk Read and Chunk Read with Notes.',\n",
       "  \"Compared to GraphReader, ReadAgent's strategy may restrict the agent's ability to identify specific details.\",\n",
       "  'The strategy of ReadAgent excessively compresses original texts into gist memories.',\n",
       "  'Other approaches have explored diverse retrieval methods, such as BM25 and learning-based strategies.',\n",
       "  'Learning-based strategies were explored by Khattab and Zaharia in 2020, Sachan et al. in 2023, and Sun et al. in 2021.',\n",
       "  'The strategy significantly boosts the agent’s capability in multi-hop reasoning and capturing long-range dependencies of key information.',\n",
       "  \"ReadAgent's strategy affects its overall performance.\"],\n",
       " 'rasooli': ['BM25 was explored by Rasooli and Tetreault in 2015.'],\n",
       " 'tetreault': ['Mohammad Sadegh Rasooli and Joel R. Tetreault published \"Yara parser: A fast and accurate dependency parser\" in 2015.',\n",
       "  'BM25 was explored by Rasooli and Tetreault in 2015.'],\n",
       " '2015': ['Mohammad Sadegh Rasooli and Joel R. Tetreault published \"Yara parser: A fast and accurate dependency parser\" in 2015.',\n",
       "  'BM25 was explored by Rasooli and Tetreault in 2015.'],\n",
       " 'khattab': ['Omar Khattab and Matei Zaharia authored the paper titled \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\" in 2020.',\n",
       "  'Learning-based strategies were explored by Khattab and Zaharia in 2020, Sachan et al. in 2023, and Sun et al. in 2021.'],\n",
       " 'zaharia': ['Omar Khattab and Matei Zaharia authored the paper titled \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\" in 2020.',\n",
       "  'Learning-based strategies were explored by Khattab and Zaharia in 2020, Sachan et al. in 2023, and Sun et al. in 2021.'],\n",
       " 'sachan et al ': ['Learning-based strategies were explored by Khattab and Zaharia in 2020, Sachan et al. in 2023, and Sun et al. in 2021.'],\n",
       " 'sun et al ': ['Learning-based strategies were explored by Khattab and Zaharia in 2020, Sachan et al. in 2023, and Sun et al. in 2021.'],\n",
       " 'complex quest': ['Despite its capabilities, Retrieval Augmented Generation (RAG) faces challenges in addressing complex questions due to difficulties in developing robust decision-making mechanisms.',\n",
       "  'Pre-planning the solution helps tackle these complex questions effectively.',\n",
       "  '2WikiMultihopQA is comprised of complex questions up to 5-hops in length.'],\n",
       " 'difficulti': ['Despite its capabilities, Retrieval Augmented Generation (RAG) faces challenges in addressing complex questions due to difficulties in developing robust decision-making mechanisms.'],\n",
       " 'robust decision making mechan': ['Despite its capabilities, Retrieval Augmented Generation (RAG) faces challenges in addressing complex questions due to difficulties in developing robust decision-making mechanisms.'],\n",
       " 'complex problem': ['Recent work has increasingly leveraged Large Language Models (LLMs) as agents to tackle complex problems.'],\n",
       " 'strong plan': ['LLMs utilize strong planning and reflection abilities.'],\n",
       " 'reflection ': ['LLMs utilize strong planning and reflection abilities.'],\n",
       " 'reasoning ': ['Abilities related to reasoning and decision-making have been utilized in complex tasks like function call and KGQA.',\n",
       "  'The method employed for reasoning is Chain-of-Thought, as mentioned by Wei et al., 2022.',\n",
       "  'Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.',\n",
       "  \"The document discusses the procedures involved in an agent's exploration and the reasoning process following information gathering.\",\n",
       "  'Future research will explore enhancements of planning and reasoning features to improve the effectiveness of the method.'],\n",
       " 'function cal': ['Abilities related to reasoning and decision-making have been utilized in complex tasks like function call and KGQA.',\n",
       "  'The exploration process includes the function call to read a chunk of information.',\n",
       "  'Each piece of data performs an average of 3 to 4 actions corresponding to the average number of function calls.',\n",
       "  'The document includes statistics of function calls on MuSiQue and NarrativeQA.',\n",
       "  'Statistics on function calls at each stage across two datasets are made to verify the actions of GraphReader.'],\n",
       " 'kgqa': ['Abilities related to reasoning and decision-making have been utilized in complex tasks like function call and KGQA.'],\n",
       " 'retriev': ['RAG uses Okapi BM25 or OpenAI API embedding model Ada-002 for retrieval.',\n",
       "  'Agents can retrieve unstructured information.',\n",
       "  'An agent-based system is employed for the execution of retrieval and reading processes for long-context QA.'],\n",
       " 'webgpt': ['WebGPT simulates human actions to search the internet for specific answers.',\n",
       "  'Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.',\n",
       "  'The \"Webgpt\" paper is available as an arXiv preprint under the identifier arXiv:2112.09332.'],\n",
       " 'human act': ['WebGPT simulates human actions to search the internet for specific answers.'],\n",
       " 'find': ['Searches are initiated from 5 initial nodes.',\n",
       "  'The experiments section denotes ongoing analysis and findings related to the outlined methodology.',\n",
       "  'Our method can identify crucial information and search for supporting facts for input questions efficiently.',\n",
       "  \"The findings indirectly reflect GraphReader's intelligence and effectiveness in exploration.\",\n",
       "  'WebGPT simulates human actions to search the internet for specific answers.'],\n",
       " 'internet': ['WebGPT simulates human actions to search the internet for specific answers.'],\n",
       " 'memwalk': ['MemWalker organizes documents into a tree structure.'],\n",
       " 'document': ['KGP organizes documents into graphs.',\n",
       "  'MemWalker organizes documents into a tree structure.',\n",
       "  'ReadAgent condenses documents into a gist memory directory.',\n",
       "  'PEARL organizes documents into a tree structure.'],\n",
       " 'tree structur': ['MemWalker organizes documents into a tree structure.',\n",
       "  'PEARL organizes documents into a tree structure.'],\n",
       " 'pearl': ['PEARL organizes documents into a tree structure.'],\n",
       " 'readag': ['ReadAgent has a capacity of 128k, displaying efficiency measures of 24.0, 26.0, 29.2, 20.0, 22.0, 16.9, 24.0, 30.0, 15.3, 14.0, 18.0, 13.6, 20.0, 22.0, and 10.4.',\n",
       "  'GPT-4-128k outperforms ReadAgent on three long-context benchmarks.',\n",
       "  'ReadAgent is authored by Lee et al., 2024.',\n",
       "  'ReadAgent performs worse than GPT-4-128k full-text reading.',\n",
       "  'Agent-based Method selects ReadAgent for its evaluation.',\n",
       "  'For ReadAgent, the evaluation focuses on the final text segments reviewed.',\n",
       "  'ReadAgent feeds all mixed memories to the model for page number selection.',\n",
       "  'Table 5 refers to the comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k.',\n",
       "  \"Compared to GraphReader, ReadAgent's strategy may restrict the agent's ability to identify specific details.\",\n",
       "  'The strategy of ReadAgent excessively compresses original texts into gist memories.',\n",
       "  'ReadAgent is mentioned as a method that segments long texts and generates gist memories.',\n",
       "  'ReadAgent condenses documents into a gist memory directory.',\n",
       "  'The average tokens for ReadAgent and GraphReader are shown in Table 5.',\n",
       "  \"ReadAgent's strategy affects its overall performance.\",\n",
       "  'ReadAgent significantly underperforms the method in handling extremely long contexts.',\n",
       "  'ReadAgent with a 128k context window setup underperforms GraphReader with a 4k context window.',\n",
       "  'The analysis compares the average token consumption of ReadAgent and GraphReader for each question.',\n",
       "  'ReadAgent has an average context of 358.3k tokens and an average cost of 48.7k tokens.',\n",
       "  'GraphReader uses 1.08 times more tokens than ReadAgent.',\n",
       "  'GraphReader achieves over twice the performance improvement compared to ReadAgent.'],\n",
       " 'gist memory directori': ['ReadAgent condenses documents into a gist memory directory.'],\n",
       " 'condens': ['ReadAgent condenses documents into a gist memory directory.'],\n",
       " 'kgp': ['KGP does not fully exploit the agent’s capabilities for planning and reflection.',\n",
       "  'KGP organizes documents into graphs.',\n",
       "  'KGP primarily uses agents to generate queries.'],\n",
       " 'queri': ['KGP primarily uses agents to generate queries.'],\n",
       " 'agent s cap': ['KGP does not fully exploit the agent’s capabilities for planning and reflection.'],\n",
       " 'not fully exploit': ['KGP does not fully exploit the agent’s capabilities for planning and reflection.'],\n",
       " 'built on': ['GraphReader is built on a graph G={V,E}.'],\n",
       " 'node vi': ['Each node vi contains a set of summarized content.',\n",
       "  'Nodes vi and vj are linked if key element ki appears in Aj and vice versa.',\n",
       "  'Each node vi=(ki,Ai) is constructed, where ki is a key element and Ai is the set of atomic facts corresponding to ki.',\n",
       "  'Each node vi∈ V contains a key element ki.'],\n",
       " 'set v': ['Each node vi∈ V contains a key element ki.'],\n",
       " 'key element ki': ['Nodes vi and vj are linked if key element ki appears in Aj and vice versa.',\n",
       "  'Each node vi∈ V contains a key element ki.'],\n",
       " 'contain': ['Each node vi contains a set of summarized content.',\n",
       "  'If John\\'s response contains the ground truth answer it is rated as \"Yes\".',\n",
       "  'Each node vi∈ V contains a key element ki.'],\n",
       " 'set': ['Each node vi contains a set of summarized content.'],\n",
       " 'summarized cont': ['Each node vi contains a set of summarized content.'],\n",
       " 'captur': [\"Our method benefits from the graph's ability to capture relationships between detailed information.\",\n",
       "  'The GraphReader captures global information using a graph structure.'],\n",
       " 'global inform': ['The GraphReader captures global information using a graph structure.'],\n",
       " 'edg': ['The edges in the graph represent relationships between nodes.'],\n",
       " 'repres': ['The edges in the graph represent relationships between nodes.'],\n",
       " 'relationship': [\"Our method benefits from the graph's ability to capture relationships between detailed information.\",\n",
       "  'The edges in the graph represent relationships between nodes.'],\n",
       " 'policeman': ['There is often a policeman presence at the intersection next to the school.',\n",
       "  'Policemen are often present at the intersection next to the school.'],\n",
       " 'presenc': ['There is often a policeman presence at the intersection next to the school.',\n",
       "  'Policemen are often present at the intersection next to the school.'],\n",
       " 'intersect': ['There is often a policeman presence at the intersection next to the school.',\n",
       "  'Policemen are often present at the intersection next to the school.'],\n",
       " 'school': ['There is often a policeman presence at the intersection next to the school.',\n",
       "  'Policemen are often present at the intersection next to the school.'],\n",
       " 'citi': ['The name of the castle in the city is Casa Loma.',\n",
       "  'The city where the performer of \"Never Too Loud\" was formed is Toronto, Canada.',\n",
       "  'Casa Loma is located in the city where the performer of Never Too Loud was formed.',\n",
       "  'The castle in the city where Danko Jones was formed is Casa Loma.'],\n",
       " 'toronto': ['\"Never Too Loud\" is associated with the Gothic Revival castle-style mansion, Casa Loma, located in Toronto, Canada.',\n",
       "  'Danko Jones is a hard rock trio based in Toronto, Canada.',\n",
       "  'The name of the castle in Toronto is Casa Loma.',\n",
       "  'Danko Jones is a hard rock band from Toronto, Canada.',\n",
       "  'The castle mentioned in the text in Toronto is Casa Loma.',\n",
       "  'The exploration identifies Danko Jones as a Toronto-based band.',\n",
       "  'Danko Jones is a Canadian hard rock trio from Toronto.',\n",
       "  'The city where the performer of \"Never Too Loud\" was formed is Toronto, Canada.'],\n",
       " 'never too loud': ['The atomic fact regarding the performer of \"Never Too Loud\" is derived from Chunk ID-6.',\n",
       "  'The performer or band associated with \"Never Too Loud\" is Danko Jones.',\n",
       "  'The name of the castle in the city is Casa Loma.',\n",
       "  'Neighbor nodes related to \"Never Too Loud\" include hard rock band, Danko Jones, studio album, and Canada.',\n",
       "  '\"Never Too Loud\" is the fourth studio album by Canadian hard rock band Danko Jones.',\n",
       "  'The city where the performer of \"Never Too Loud\" was formed is Toronto, Canada.',\n",
       "  'Casa Loma is located in the city where the performer of Never Too Loud was formed.',\n",
       "  'The studio album \"Never Too Loud\" is performed by the hard rock band Danko Jones.',\n",
       "  'The performer of the album Never Too Loud is Danko Jones.',\n",
       "  '\"Never Too Loud\" is a studio album by the Canadian hard rock band Danko Jones.',\n",
       "  '\"Never Too Loud\" was recorded at Studio 606 in Los Angeles, with producer Nick Raskulinecz.'],\n",
       " 'castl': ['The name of the castle in the city is Casa Loma.',\n",
       "  'The name of the castle in Toronto is Casa Loma.',\n",
       "  'The castle mentioned in the text in Toronto is Casa Loma.'],\n",
       " 'casa loma': ['\"Never Too Loud\" is associated with the Gothic Revival castle-style mansion, Casa Loma, located in Toronto, Canada.',\n",
       "  'Casa Loma is characterized by its Gothic Revival architecture.',\n",
       "  'The name of the castle in Toronto is Casa Loma.',\n",
       "  'The castle in the city where Danko Jones was formed is Casa Loma.',\n",
       "  'Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario, Canada.',\n",
       "  'The name of the castle in the city is Casa Loma.',\n",
       "  'Casa Loma is now a historic house museum and landmark.',\n",
       "  'The castle mentioned in the text in Toronto is Casa Loma.',\n",
       "  'Casa Loma is located in the city where the performer of Never Too Loud was formed.',\n",
       "  'The architect of Casa Loma was E. J. Lennox.',\n",
       "  'Casa Loma is located in midtown Toronto, Ontario, Canada.',\n",
       "  'Casa Loma was constructed from 1911 to 1914 as a residence for financier Sir Henry Pellatt.'],\n",
       " 'band': ['The band consists of Danko Jones, John \"JC\" Calabrese, and Rich Knox.',\n",
       "  'The performer or band associated with \"Never Too Loud\" is Danko Jones.',\n",
       "  'The exploration identifies Danko Jones as a Toronto-based band.'],\n",
       " 'danko jon': ['Danko Jones is a hard rock trio based in Toronto, Canada.',\n",
       "  'The castle in the city where Danko Jones was formed is Casa Loma.',\n",
       "  'The band consists of Danko Jones, John \"JC\" Calabrese, and Rich Knox.',\n",
       "  'The performer or band associated with \"Never Too Loud\" is Danko Jones.',\n",
       "  'Danko Jones is known for their energetic live shows.',\n",
       "  'Danko Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums) make up the ensemble.',\n",
       "  'Danko Jones is a hard rock band from Toronto, Canada.',\n",
       "  'Neighbor nodes related to \"Never Too Loud\" include hard rock band, Danko Jones, studio album, and Canada.',\n",
       "  'Danko Jones is a Canadian hard rock trio from Toronto.',\n",
       "  'The exploration identifies Danko Jones as a Toronto-based band.',\n",
       "  'Another atomic fact about Danko Jones is obtained from Chunk ID-9.',\n",
       "  '\"Never Too Loud\" is the fourth studio album by Canadian hard rock band Danko Jones.',\n",
       "  'The studio album \"Never Too Loud\" is performed by the hard rock band Danko Jones.',\n",
       "  'The performer of the album Never Too Loud is Danko Jones.',\n",
       "  '\"Never Too Loud\" is a studio album by the Canadian hard rock band Danko Jones.'],\n",
       " 'studio album': ['\"Never Too Loud\" is the fourth studio album by Canadian hard rock band Danko Jones.',\n",
       "  '\"Never Too Loud\" is a studio album by the Canadian hard rock band Danko Jones.',\n",
       "  'Neighbor nodes related to \"Never Too Loud\" include hard rock band, Danko Jones, studio album, and Canada.',\n",
       "  'The studio album \"Never Too Loud\" is performed by the hard rock band Danko Jones.'],\n",
       " 'canada': ['\"Never Too Loud\" is associated with the Gothic Revival castle-style mansion, Casa Loma, located in Toronto, Canada.',\n",
       "  'Danko Jones is a hard rock trio based in Toronto, Canada.',\n",
       "  'Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario, Canada.',\n",
       "  'Neighbor nodes related to \"Never Too Loud\" include hard rock band, Danko Jones, studio album, and Canada.',\n",
       "  'Danko Jones is a hard rock band from Toronto, Canada.',\n",
       "  '\"Never Too Loud\" is a studio album by the Canadian hard rock band Danko Jones.',\n",
       "  'Danko Jones is a Canadian hard rock trio from Toronto.',\n",
       "  '\"Never Too Loud\" is the fourth studio album by Canadian hard rock band Danko Jones.',\n",
       "  'Casa Loma is located in midtown Toronto, Ontario, Canada.'],\n",
       " 'hard rock band': ['Danko Jones is a hard rock trio based in Toronto, Canada.',\n",
       "  'Danko Jones is a hard rock band from Toronto, Canada.',\n",
       "  'Neighbor nodes related to \"Never Too Loud\" include hard rock band, Danko Jones, studio album, and Canada.',\n",
       "  'Danko Jones is a Canadian hard rock trio from Toronto.',\n",
       "  '\"Never Too Loud\" is the fourth studio album by Canadian hard rock band Danko Jones.',\n",
       "  'The studio album \"Never Too Loud\" is performed by the hard rock band Danko Jones.',\n",
       "  '\"Never Too Loud\" is a studio album by the Canadian hard rock band Danko Jones.'],\n",
       " 'hard rock': ['Danko Jones is a Canadian hard rock trio from Toronto.',\n",
       "  'The band’s music includes elements of hard rock and punk.'],\n",
       " 'trio': ['Danko Jones is a Canadian hard rock trio from Toronto.'],\n",
       " 'graph explor': ['Figures 7 to 11 present the prompts employed for Graph Exploration.',\n",
       "  'The process of GraphReader includes three phases: graph construction, graph exploration, and answer reasoning.',\n",
       "  'The method is effective in handling extremely long texts by graph exploration with limited context window LLMs.',\n",
       "  '“w/o Node Selection” indicates applying random selection of initial nodes and neighbor nodes in graph exploration.',\n",
       "  'GraphReader collects supporting facts during graph exploration.',\n",
       "  'The GraphReader approach consists of graph construction, exploration, and answer reasoning.'],\n",
       " 'phase': ['The process of GraphReader includes three phases: graph construction, graph exploration, and answer reasoning.'],\n",
       " 'supporting fact': [\"The intelligent assistant's primary objective is to answer questions by gathering supporting facts from articles.\",\n",
       "  'Our method can identify crucial information and search for supporting facts for input questions efficiently.',\n",
       "  'The agent starts by maintaining a notebook to record supporting facts.',\n",
       "  'The performance of GraphReader closely matches that achieved by directly supplying supporting facts to the LLM.',\n",
       "  'The recall rate of supporting facts is evaluated for different methods using GPT-4-128k with a temperature of 0.1.',\n",
       "  'Any supporting facts discovered will be recorded in the notebook.',\n",
       "  '\"SF-wise\" refers to the granularity of supporting facts.',\n",
       "  'Pre-planning, reflection, and various actions are important for using a graph that contains key information.',\n",
       "  'The call of supporting facts declines across all methods.',\n",
       "  'Golden denotes the settings in which question and its supporting facts are added to LLM directly.',\n",
       "  'The recall of supporting facts is evaluated using GPT-4 on the HotpotWikiQA-mixup dataset.',\n",
       "  'GraphReader collects supporting facts during graph exploration.',\n",
       "  'A sample is considered to be recalled only if all of its supporting facts are recalled at sample granularity.',\n",
       "  'The granularity of supporting facts refers to the recall rate of all supporting facts across the entire dataset.',\n",
       "  'A possible reason for the poor performance of RAG methods is difficulty in recalling all chunks that contain supporting facts.',\n",
       "  'The agent will use search_more if supporting facts are insufficient.'],\n",
       " 'collect': ['The Multi-hop QA Datasets include HotpotQA, which features a collection of 2-hop questions directly authored by native speakers.',\n",
       "  'GraphReader collects supporting facts during graph exploration.'],\n",
       " 'termin': ['Choose termination if no neighboring nodes possess information to answer the question.',\n",
       "  'In HotpotQA, the termination for Exploring Neighbors has a ratio of 65.5%.',\n",
       "  'In HotpotQA, the termination stage has a ratio of 43.9%.',\n",
       "  'In WikiMultihopQA, the termination for Exploring Neighbors has a ratio of 62.7%.',\n",
       "  'GraphReader terminates exploration once sufficient information is gathered to answer a question.',\n",
       "  'The action \"termination()\" indicates that sufficient information may have already been gathered.',\n",
       "  'The agent checks all neighboring nodes and performs one of two functions: reading a neighboring node or terminating.',\n",
       "  'Action options include reading a neighbor node or termination.',\n",
       "  'In WikiMultihopQA, the termination stage has a ratio of 37.1%.',\n",
       "  'The four functions are: search_more, read_previous_chunk, read_subsequent_chunk, termination.',\n",
       "  'The agent will terminate exploration if sufficient information has been gathered.'],\n",
       " 'answer quest': ['Choose termination if no neighboring nodes possess information to answer the question.',\n",
       "  \"The intelligent assistant's primary objective is to answer questions by gathering supporting facts from articles.\",\n",
       "  \"An intelligent assistant's primary objective is to answer questions based on information within the text.\",\n",
       "  'The agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks.',\n",
       "  'Multi-hop questions need to gather information contained by multiple nodes to answer questions.',\n",
       "  'The primary objective of the assistant is to answer questions based on the information within a text.',\n",
       "  'Gist memories are looked up to search for information to answer questions.',\n",
       "  'The assistant must assess whether the available information in a text chunk is sufficient to answer a question.',\n",
       "  'In a single-hop QA dataset, the information needed to answer questions appears at a single location within the text.',\n",
       "  'The rational plan should outline a step-by-step process to resolve the question.',\n",
       "  'The process involves selecting the top-1 chunk for answering questions.',\n",
       "  \"To answer the question, it's necessary to find the length of Danny's and Alice's tennis careers.\",\n",
       "  'The instruction is to read text chunks and answer the question.',\n",
       "  'GraphReader terminates exploration once sufficient information is gathered to answer a question.',\n",
       "  'GPT-4-128k is employed to read retrieved chunks and answer questions in RAG.'],\n",
       " 'figure 2': ['Figure 6 illustrates the prompt used for Graph Construction.',\n",
       "  'Figures 7 to 11 present the prompts employed for Graph Exploration.',\n",
       "  'Figure 16 and Figure 17 contain specific prompts.',\n",
       "  'The results of the experiments are shown in Figure 3.',\n",
       "  'Figure 21 delineates the methodology for constructing the graph.',\n",
       "  'Figure 20 displays the posed question alongside the answer and pertinent supporting passages.',\n",
       "  'Figure 19 displays the specific evaluation prompt.',\n",
       "  'The results of the recall evaluation are displayed in Figure 5.',\n",
       "  'Figure 23 illustrates the sequence of function invocations during the exploration phase.',\n",
       "  'The prompts used for evaluation are presented in Figure 13 and Figure 14 respectively.',\n",
       "  'Figure 12 shows the prompt used for Answer Reasoning.',\n",
       "  'Figure 24 showcases how GraphReader operates.',\n",
       "  'Figure 22 elaborates on the initialization of a pre-planned rational path by GraphReader and the selection of initial nodes.',\n",
       "  'The entire process is illustrated in Figure 2.',\n",
       "  'Figure 3 shows the performance of GraphReader with different initial node numbers on 2WikiMultihopQA and NarrativeQA.',\n",
       "  'The specific prompt can be found in Figure 18.'],\n",
       " 'illustr': ['The entire process is illustrated in Figure 2.'],\n",
       " 'document d': ['The document D is split into chunks while preserving paragraph structure.'],\n",
       " 'paragraph structur': ['The document D is split into chunks while preserving paragraph structure.'],\n",
       " 'split': ['The document D is split into chunks while preserving paragraph structure.'],\n",
       " 'summar': ['In the Chunk Read with Notes approach, the LLM can summarize useful information.',\n",
       "  'The LLM summarizes each chunk into atomic facts to extract nodes.',\n",
       "  'The agent will summarize key information and provide brief clues.'],\n",
       " 'extract nod': ['The LLM summarizes each chunk into atomic facts to extract nodes.'],\n",
       " 'simplifi': ['The LLM extracts atomic facts from text chunks and simplifies them.'],\n",
       " 'extract': ['The LLM also extracts key elements, including essential nouns, verbs, and adjectives from each atomic fact.',\n",
       "  'The method is capable of extracting more valid information from chunks during exploration.'],\n",
       " 'essential noun': ['The LLM also extracts key elements, including essential nouns, verbs, and adjectives from each atomic fact.'],\n",
       " 'verb': ['The LLM also extracts key elements, including essential nouns, verbs, and adjectives from each atomic fact.',\n",
       "  'Nodes represent key elements (nouns, verbs, adjectives) that correspond with various atomic facts.'],\n",
       " 'adject': ['The LLM also extracts key elements, including essential nouns, verbs, and adjectives from each atomic fact.',\n",
       "  'Nodes represent key elements (nouns, verbs, adjectives) that correspond with various atomic facts.'],\n",
       " 'normal': ['The process involves normalizing key elements to manage lexical noise and granularity issues, following Lu et al. (2023).',\n",
       "  'The aggregation of similar nodes caused by normalization results in a slight increase in the number of neighboring nodes.',\n",
       "  'After normalization, each node has an average of about 10 neighbor nodes.'],\n",
       " 'lexical nois': ['The process involves normalizing key elements to manage lexical noise and granularity issues, following Lu et al. (2023).'],\n",
       " 'granularity issu': ['The process involves normalizing key elements to manage lexical noise and granularity issues, following Lu et al. (2023).'],\n",
       " 'lu et al   2023 ': ['The process involves normalizing key elements to manage lexical noise and granularity issues, following Lu et al. (2023).',\n",
       "  'The method employed for reasoning is Chain-of-Thought, as mentioned by Wei et al., 2022.'],\n",
       " 'final set': ['A final set of key elements is created post-normalization.'],\n",
       " 'creat': ['A final set of key elements is created post-normalization.'],\n",
       " 'ki': ['Each node vi=(ki,Ai) is constructed, where ki is a key element and Ai is the set of atomic facts corresponding to ki.'],\n",
       " 'ai': ['Each node vi=(ki,Ai) is constructed, where ki is a key element and Ai is the set of atomic facts corresponding to ki.'],\n",
       " 'vj': ['Nodes vi and vj are linked if key element ki appears in Aj and vice versa.'],\n",
       " 'link': ['Nodes vi and vj are linked if key element ki appears in Aj and vice versa.'],\n",
       " 'aj': ['Nodes vi and vj are linked if key element ki appears in Aj and vice versa.'],\n",
       " 'goal': ['The goal is to determine whether to proceed to the next neighboring node.',\n",
       "  'The goal is to design an agent that autonomously explores the graph using predefined functions.'],\n",
       " 'maintain': ['GraphReader maintains around 60% recall at 256k context length.',\n",
       "  'The agent starts by maintaining a notebook to record supporting facts.'],\n",
       " 'recorded fact': ['The recorded facts are used to derive a final answer.'],\n",
       " 'use': ['The recorded facts are used to derive a final answer.'],\n",
       " 'deriv': ['The recorded facts are used to derive a final answer.'],\n",
       " 'final answ': ['The LLM generates the final answer after considering all available information.',\n",
       "  'The recorded facts are used to derive a final answer.',\n",
       "  'The final answer must consider all available information collected from the notebooks.',\n",
       "  'In the case of Chunk Read with Notes, both the memory and the chunk read at the final answer are evaluated.'],\n",
       " 'initi': ['Figure 22 elaborates on the initialization of a pre-planned rational path by GraphReader and the selection of initial nodes.',\n",
       "  'The agent performs two initializations: defining a rational plan and selecting the initial node.'],\n",
       " 'defin': ['The agent performs two initializations: defining a rational plan and selecting the initial node.'],\n",
       " 'select': ['The agent will select one of four functions based on the updated notebook.',\n",
       "  'Figure 22 elaborates on the initialization of a pre-planned rational path by GraphReader and the selection of initial nodes.',\n",
       "  'The agent performs two initializations: defining a rational plan and selecting the initial node.',\n",
       "  'The agent evaluates the key elements of all nodes V and selects Ninitial nodes.'],\n",
       " 'initial nod': ['Searches are initiated from 5 initial nodes.',\n",
       "  'GraphReader begins from the initial node and follows a rational plan.',\n",
       "  'Fully processed atomic facts and chunk queue indicate thorough exploration of the current node.',\n",
       "  'The agent explores each initial node by first exploring atomic facts.',\n",
       "  'Increasing the number of nodes improves performance up to a certain point, with optimal performance at 5 initial nodes.',\n",
       "  'The Effect of Node Selection is demonstrated through experiments on randomly selecting initial nodes and neighbor nodes.',\n",
       "  '“w/o Node Selection” indicates applying random selection of initial nodes and neighbor nodes in graph exploration.',\n",
       "  'Five initial nodes were set as the default for experiments.',\n",
       "  'The decline in performance beyond 5 initial nodes is likely due to increased noise from too many initial nodes.',\n",
       "  'The agent performs two initializations: defining a rational plan and selecting the initial node.',\n",
       "  'The task is to assess all neighboring nodes of the current node.',\n",
       "  'Figure 22 elaborates on the initialization of a pre-planned rational path by GraphReader and the selection of initial nodes.',\n",
       "  'The initial node is selected from all nodes based on the rational plan.'],\n",
       " 'aim': ['The rational plan is aimed at addressing complex real-world multi-hop questions.'],\n",
       " 'address': ['The rational plan is aimed at addressing complex real-world multi-hop questions.'],\n",
       " 'real world': ['The rational plan is aimed at addressing complex real-world multi-hop questions.'],\n",
       " 'solut': ['Pre-planning the solution helps tackle these complex questions effectively.'],\n",
       " 'tackl': ['Pre-planning the solution helps tackle these complex questions effectively.'],\n",
       " 'original quest': ['The agent breaks down the original question step-by-step.',\n",
       "  'The task requires determining if the text chunk can help answer the previous question.'],\n",
       " 'step': ['The agent breaks down the original question step-by-step.',\n",
       "  'This step is essential because atomic facts merely sum up the overall text.',\n",
       "  'The first step to resolve a question is to create a rational plan based on the question.',\n",
       "  'In the graph exploration stage, a rational plan is introduced to help the agent analyze complex input questions step by step.',\n",
       "  'The rational plan should outline a step-by-step process to resolve the question.'],\n",
       " 'breaks down': ['The agent breaks down the original question step-by-step.'],\n",
       " 'identifi': ['The agent identifies the key information needed.',\n",
       "  'Our method can identify crucial information and search for supporting facts for input questions efficiently.'],\n",
       " 'need': ['The agent identifies the key information needed.'],\n",
       " 'form': ['The castle in the city where Danko Jones was formed is Casa Loma.',\n",
       "  'The agent forms a rational plan.'],\n",
       " 'strategic starting point': ['Choosing strategic starting points is essential for improving search efficiency.'],\n",
       " 'improv': ['Choosing strategic starting points is essential for improving search efficiency.'],\n",
       " 'search effici': ['Choosing strategic starting points is essential for improving search efficiency.'],\n",
       " 'essenti': ['Choosing strategic starting points is essential for improving search efficiency.'],\n",
       " 'ninitial nod': ['The agent evaluates the key elements of all nodes V and selects Ninitial nodes.'],\n",
       " 'chunks of the nod': ['The agent explores chunks of the node after exploring atomic facts.'],\n",
       " 'next nod': ['Choose termination if no neighboring nodes possess information to answer the question.',\n",
       "  'The aggregation of similar nodes caused by normalization results in a slight increase in the number of neighboring nodes.',\n",
       "  'Choose to read a neighboring node if it may contain relevant information to the question.',\n",
       "  'Multi-hop questions need to gather information contained by multiple nodes to answer questions.',\n",
       "  'The agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks.',\n",
       "  'The agent explores neighboring nodes, guided by the question and rational plan.',\n",
       "  'The agent proceeds to explore neighboring nodes.',\n",
       "  'The agent checks all neighboring nodes and performs one of two functions: reading a neighboring node or terminating.',\n",
       "  'After normalization, each node has an average of about 10 neighbor nodes.',\n",
       "  'The task is to assess all neighboring nodes of the current node.',\n",
       "  'The goal is to determine whether to proceed to the next neighboring node.',\n",
       "  'The agent needs to access the next node after processing.',\n",
       "  'The agent determines that none of the neighboring nodes contain useful information and finishes the exploration.',\n",
       "  \"The agent's functions include assessing neighboring nodes to provide answers or denote completion of information gathering.\"],\n",
       " 'guid': ['The agent explores neighboring nodes, guided by the question and rational plan.',\n",
       "  'Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration.'],\n",
       " 'updat': ['The agent continuously updates the notebook with relevant information during the exploration process.'],\n",
       " 'exploration process': ['The agent continuously updates the notebook with relevant information during the exploration process.'],\n",
       " 'impract': ['It is impractical to include all original text chunks related to a node within the context window.'],\n",
       " 'related to': ['It is impractical to include all original text chunks related to a node within the context window.',\n",
       "  'All atomic facts associated with a node can fit within the context window.'],\n",
       " 'coarse to fine strategi': ['The agent employs a coarse-to-fine strategy, progressing from reading atomic facts to the original text.'],\n",
       " 'original text': ['The agent employs a coarse-to-fine strategy, progressing from reading atomic facts to the original text.',\n",
       "  'Text chunks are segments of the original text, while atomic facts are the smallest truths extracted from those chunks.',\n",
       "  'The strategy of ReadAgent excessively compresses original texts into gist memories.',\n",
       "  'Text chunks are segments of the original text.'],\n",
       " 'fit': ['All atomic facts associated with a node can fit within the context window.'],\n",
       " 'chunk id': ['Chunks are grouped by corresponding chunk IDs and fed to the agent.',\n",
       "  'The atomic fact regarding the performer of \"Never Too Loud\" is derived from Chunk ID-6.',\n",
       "  'The agent may insert chunk IDs into the queue.',\n",
       "  'If the agent identifies certain chunks as valuable for further reading, it completes the function parameters with the chunk IDs.',\n",
       "  'The agent appends these IDs to a chunk queue.',\n",
       "  'Another atomic fact about Danko Jones is obtained from Chunk ID-9.'],\n",
       " 'required clu': ['The agent utilizes the question, rational plan, and notes in its notebook to reflect on required clues.'],\n",
       " 'function': ['The agent checks all neighboring nodes and performs one of two functions: reading a neighboring node or terminating.',\n",
       "  'The agent is provided with two functions: read_chunk and stop_and_read_neighbor.',\n",
       "  \"The agent's functions include assessing neighboring nodes to provide answers or denote completion of information gathering.\"],\n",
       " 'read chunk': ['In HotpotQA, the read_previous_chunk function call has a ratio of 21.1%.',\n",
       "  'There are two main strategies during the reading process: Chunk Read and Chunk Read with Notes.',\n",
       "  'The action \"read_previous_chunk()\" may provide useful information.',\n",
       "  'In HotpotQA, the read_chunk function call has a ratio of 46.5%.',\n",
       "  'In MuSiQue, the read_previous_chunk function call has a ratio of 26.6%.',\n",
       "  'The assistant can choose to read a chunk from a list of IDs or stop and read a neighboring chunk.',\n",
       "  'The agent is provided with two functions: read_chunk and stop_and_read_neighbor.',\n",
       "  'In WikiMultihopQA, the read_previous_chunk function call has a ratio of 25.1%.',\n",
       "  'In the Chunk Read approach, the LLM only sees the current chunk during each reading.',\n",
       "  'The most common action on single-hop QA tasks is to read chunks.',\n",
       "  'The exploration process includes the function call to read a chunk of information.',\n",
       "  'In WikiMultihopQA, the read_chunk function call has a ratio of 48.6%.',\n",
       "  'The Chunk Read approach is suitable for single-hop QA tasks.',\n",
       "  'In the case of Chunk Read with Notes, both the memory and the chunk read at the final answer are evaluated.',\n",
       "  'The prompt includes a figure related to Full Text Read and Chunk Read instructions.',\n",
       "  'The agent will read_previous_chunk and read_subsequent_chunk due to truncation issues.',\n",
       "  'In MuSiQue, the read_chunk function call has a ratio of 41.3%.',\n",
       "  'In the Chunk Read with Notes approach, the LLM can summarize useful information.',\n",
       "  'The four functions are: search_more, read_previous_chunk, read_subsequent_chunk, termination.',\n",
       "  'The action \"read_subsequent_chunk()\" may also provide insights.',\n",
       "  'In WikiMultihopQA, the read_subsequent_chunk function call has a ratio of 23.3%.',\n",
       "  'In HotpotQA, the read_subsequent_chunk function call has a ratio of 22.9%.'],\n",
       " 'stop and read neighbor': ['In HotpotQA, the stop_and_read_neighbor function call has a ratio of 53.5%.',\n",
       "  'The assistant can choose to read a chunk from a list of IDs or stop and read a neighboring chunk.',\n",
       "  'The agent is provided with two functions: read_chunk and stop_and_read_neighbor.',\n",
       "  'The stop_and_read_neighbor() action is chosen only when the current text chunk is deemed irrelevant.',\n",
       "  'In WikiMultihopQA, the stop_and_read_neighbor function call has a ratio of 51.4%.',\n",
       "  'In MuSiQue, the stop_and_read_neighbor function call has a ratio of 58.7%.'],\n",
       " 'valuable chunk': ['If the agent identifies certain chunks as valuable for further reading, it completes the function parameters with the chunk IDs.'],\n",
       " 'function paramet': ['If the agent identifies certain chunks as valuable for further reading, it completes the function parameters with the chunk IDs.'],\n",
       " 'chunk queu': ['The agent appends these IDs to a chunk queue.',\n",
       "  'Fully processed atomic facts and chunk queue indicate thorough exploration of the current node.',\n",
       "  'When the chunk queue is non-empty, it indicates the agent has identified multiple text chunks of interest.'],\n",
       " 'interest': ['When the chunk queue is non-empty, it indicates the agent has identified multiple text chunks of interest.'],\n",
       " 'queue': ['The agent may insert chunk IDs into the queue.',\n",
       "  'The agent traverses the queue, reading each chunk.'],\n",
       " 'text': ['A graph has been created from the text, comprising text chunks, atomic facts, and nodes.',\n",
       "  \"An intelligent assistant's primary objective is to answer questions based on information within the text.\",\n",
       "  'John was given a question about the text after reading some text.',\n",
       "  'Characters, such as the protagonist, appear frequently throughout the text of NarrativeQA.',\n",
       "  'This step is essential because atomic facts merely sum up the overall text.',\n",
       "  'In a single-hop QA dataset, the information needed to answer questions appears at a single location within the text.',\n",
       "  'The text asks if John’s answer agrees with the ground truth answer.',\n",
       "  'The text is split into chunks using the method from GraphReader.',\n",
       "  'The proposed method involves dividing the entire text into chunks.',\n",
       "  'Nodes are key elements in the text that correlate with several atomic facts derived from different text chunks.'],\n",
       " 'overal': ['This step is essential because atomic facts merely sum up the overall text.'],\n",
       " 'brief clu': ['The agent will summarize key information and provide brief clues.'],\n",
       " 'obtain': ['Specific details are best obtained directly from the original text chunks.'],\n",
       " 'think': ['The agent thinks about what can be added to the current notebook.'],\n",
       " 'ad': ['The agent thinks about what can be added to the current notebook.'],\n",
       " 'discov': ['Any supporting facts discovered will be recorded in the notebook.'],\n",
       " 'four funct': ['The agent will select one of four functions based on the updated notebook.',\n",
       "  'The four functions are: search_more, read_previous_chunk, read_subsequent_chunk, termination.'],\n",
       " 'search mor': ['In HotpotQA, the search_more function call has a ratio of 12.1%.',\n",
       "  'In MuSiQue, the search_more function call has a ratio of 19.1%.',\n",
       "  'There are action options available for the assistant, including search_more().',\n",
       "  'The four functions are: search_more, read_previous_chunk, read_subsequent_chunk, termination.',\n",
       "  'In WikiMultihopQA, the search_more function call has a ratio of 14.5%.',\n",
       "  'The agent will use search_more if supporting facts are insufficient.'],\n",
       " 'insuffici': ['The agent will use search_more if supporting facts are insufficient.'],\n",
       " 'truncation issu': ['The agent will read_previous_chunk and read_subsequent_chunk due to truncation issues.'],\n",
       " 'insert': ['The agent may insert chunk IDs into the queue.'],\n",
       " 'neighbor nod': ['The average of atomic facts per neighbor node in WikiMultihopQA is 2.1 with a maximum of 17.0.',\n",
       "  'The average neighbor node count in WikiMultihopQA is 9.2.',\n",
       "  'The average neighbor node count in MusiQue is 9.3.',\n",
       "  'Neighbor nodes related to \"Never Too Loud\" include hard rock band, Danko Jones, studio album, and Canada.',\n",
       "  'The Effect of Node Selection is demonstrated through experiments on randomly selecting initial nodes and neighbor nodes.',\n",
       "  'In HotpotQA, the read_neighbor_node function call has a ratio of 35.5%.',\n",
       "  'The average of atomic facts per neighbor node in MusiQue is 2.1 with a maximum of 15.6.',\n",
       "  'The average of atomic facts per neighbor node in HotpotQA is 2.1 with a maximum of 17.8.',\n",
       "  'The most commonly used action on multi-hop QA tasks is to read neighbor nodes.',\n",
       "  'The agent checks all neighboring nodes and performs one of two functions: reading a neighboring node or terminating.',\n",
       "  'Action options include reading a neighbor node or termination.',\n",
       "  '“w/o Node Selection” indicates applying random selection of initial nodes and neighbor nodes in graph exploration.',\n",
       "  'After normalization, each node has an average of about 10 neighbor nodes.',\n",
       "  'Choose to read a neighboring node if it may contain relevant information to the question.',\n",
       "  'In WikiMultihopQA, the read_neighbor_node function call has a ratio of 37.3%.',\n",
       "  'The number of neighbor nodes in HotpotQA has an average of 10.1.'],\n",
       " 'help': ['The agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks.'],\n",
       " 'finish': ['The agent determines that none of the neighboring nodes contain useful information and finishes the exploration.'],\n",
       " 'multiple ag': ['After multiple agents have independently gathered information and stopped their exploration, notes from each agent will be compiled for reasoning.'],\n",
       " 'stopped explor': ['After multiple agents have independently gathered information and stopped their exploration, notes from each agent will be compiled for reasoning.'],\n",
       " 'compil': ['After multiple agents have independently gathered information and stopped their exploration, notes from each agent will be compiled for reasoning.'],\n",
       " 'analyz': ['In the graph exploration stage, a rational plan is introduced to help the agent analyze complex input questions step by step.',\n",
       "  'The LLM analyzes each note by considering complementary information from other memories and using a majority voting strategy to resolve inconsistencies.',\n",
       "  'As context length increases from 16k to 256k, recall performance is analyzed.',\n",
       "  'Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.'],\n",
       " 'memori': ['ReadAgent feeds all mixed memories to the model for page number selection.',\n",
       "  'Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".',\n",
       "  'GraphReader answers the question based on memory from the notebook.',\n",
       "  'For GraphReader, the memory is recorded in the final notebook.',\n",
       "  'The LLM analyzes each note by considering complementary information from other memories and using a majority voting strategy to resolve inconsistencies.',\n",
       "  'In the case of Chunk Read with Notes, both the memory and the chunk read at the final answer are evaluated.',\n",
       "  'GraphReader carries a notebook that records memory.'],\n",
       " 'majority voting strategi': ['The LLM analyzes each note by considering complementary information from other memories and using a majority voting strategy to resolve inconsistencies.',\n",
       "  'A majority voting strategy should be used to resolve inconsistencies in the information.'],\n",
       " 'inconsist': ['The LLM analyzes each note by considering complementary information from other memories and using a majority voting strategy to resolve inconsistencies.',\n",
       "  'A majority voting strategy should be used to resolve inconsistencies in the information.'],\n",
       " 'consid': ['The LLM generates the final answer after considering all available information.',\n",
       "  'A sample is considered to be recalled only if all of its supporting facts are recalled at sample granularity.'],\n",
       " 'procedur': [\"The document discusses the procedures involved in an agent's exploration and the reasoning process following information gathering.\"],\n",
       " 'information gath': [\"The document discusses the procedures involved in an agent's exploration and the reasoning process following information gathering.\",\n",
       "  \"The agent's functions include assessing neighboring nodes to provide answers or denote completion of information gathering.\"],\n",
       " 'chain of thought': ['The method employed for reasoning is Chain-of-Thought, as mentioned by Wei et al., 2022.'],\n",
       " 'methodolog': ['The experiments section denotes ongoing analysis and findings related to the outlined methodology.',\n",
       "  'Figure 21 delineates the methodology for constructing the graph.',\n",
       "  'The overall performance reflects different methodologies and their effectiveness in handling the datasets.'],\n",
       " 'assess': ['The assistant must assess whether the available information in a text chunk is sufficient to answer a question.',\n",
       "  'The task is to assess all neighboring nodes of the current node.',\n",
       "  'For the RAG methods, retrieved chunks are assessed.',\n",
       "  \"The agent's functions include assessing neighboring nodes to provide answers or denote completion of information gathering.\"],\n",
       " 'two typ': ['The experiments are conducted on two types of long-context QA benchmarks.'],\n",
       " 'hotpotqa': ['BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'In HotpotQA, the search_more function call has a ratio of 12.1%.',\n",
       "  'In HotpotQA, the termination for Exploring Neighbors has a ratio of 65.5%.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) shows data regarding HotpotQA.',\n",
       "  'In HotpotQA, the read_previous_chunk function call has a ratio of 21.1%.',\n",
       "  'In HotpotQA, the Exploring Atomic Facts stage has a ratio of 42.0%.',\n",
       "  'In HotpotQA, the read_chunk function call has a ratio of 46.5%.',\n",
       "  'HotpotWikiQA-mixup is included in the evaluation from LV-Eval.',\n",
       "  'HotpotQAGraphReader achieved scores of 84.3, 89.7, and 70.0.',\n",
       "  'The sample dimension for HotpotQA has an average of 583.8 node numbers and a maximum of 1945.0.',\n",
       "  'Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'HotpotQAGraphReader without Node Selection scored 66.0, 71.7, and 54.1.',\n",
       "  'HotpotQA was developed by Yang et al. in 2018.',\n",
       "  'Table 5 refers to the comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k.',\n",
       "  'For HotpotWikiQA-mixup16k, the average nodes is 1741.6, and the maximum node count is 3822.0.',\n",
       "  'The Multi-hop QA Datasets include HotpotQA, which features a collection of 2-hop questions directly authored by native speakers.',\n",
       "  'HotpotWikiQA-mixup is derived from LV-Eval and uses a mixup construction method.',\n",
       "  'In HotpotQA, the stop_and_read_neighbor function call has a ratio of 53.5%.',\n",
       "  'In HotpotQA, the Exploring Chunks stage has a ratio of 31.9%.',\n",
       "  'The number of atomic facts for HotpotQA has an average of 244.0 and a maximum of 645.0.',\n",
       "  'HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The number of neighbor nodes in HotpotQA has an average of 10.1.',\n",
       "  'The dataset includes various models such as NarrativeQA and HotpotWikiQA-mixup16k.',\n",
       "  'In HotpotQA, the Exploring Neighbors stage has a ratio of 26.1%.',\n",
       "  'The dataset HotpotQA has an average of 9.4k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The average of atomic facts per neighbor node in HotpotQA is 2.1 with a maximum of 17.8.',\n",
       "  'The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'HotpotQAGraphReader without Rational Plan scored 81.7, 87.7, and 63.8.',\n",
       "  'The recall of supporting facts is evaluated using GPT-4 on the HotpotWikiQA-mixup dataset.',\n",
       "  'The multi-hop long-context QA benchmarks include HotpotQA, 2WikiMulti-hopQA, and MuSiQue.',\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'The paper \"Hotpotqa\" was presented at the EMNLP conference and includes pages 2369 to 2380.',\n",
       "  'In HotpotQA, the termination stage has a ratio of 43.9%.',\n",
       "  'In HotpotQA, the read_neighbor_node function call has a ratio of 35.5%.',\n",
       "  'For HotpotWikiQA-mixup from LV-Eval, two hyperparameters are scaled using the same approach as in the ReadAgent paper.',\n",
       "  'In HotpotQA, the read_subsequent_chunk function call has a ratio of 22.9%.'],\n",
       " 'wikimultihopqa': ['2WikiMulti-hopQA was developed by Ho et al. in 2020.',\n",
       "  'The dataset 2WikiMultihopQA has an average of 8.8k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'In WikiMultihopQA, the Exploring Chunks stage has a ratio of 34.5%.',\n",
       "  'The number of atomic facts for WikiMultihopQA has an average of 217.7 and a maximum of 545.0.',\n",
       "  'In WikiMultihopQA, the search_more function call has a ratio of 14.5%.',\n",
       "  '2WikiMultihopQA is comprised of complex questions up to 5-hops in length.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) also applies to WikiMultihopQA.',\n",
       "  'The average of atomic facts per neighbor node in WikiMultihopQA is 2.1 with a maximum of 17.0.',\n",
       "  'In WikiMultihopQA, the Exploring Atomic Facts stage has a ratio of 40.4%.',\n",
       "  'In WikiMultihopQA, the read_previous_chunk function call has a ratio of 25.1%.',\n",
       "  '2WikiMultihopQAGraphReader without Node Selection scored 65.3, 68.7, and 49.7.',\n",
       "  'In WikiMultihopQA, the stop_and_read_neighbor function call has a ratio of 51.4%.',\n",
       "  'In WikiMultihopQA, the read_neighbor_node function call has a ratio of 37.3%.',\n",
       "  'The average neighbor node count in WikiMultihopQA is 9.2.',\n",
       "  'In WikiMultihopQA, the read_chunk function call has a ratio of 48.6%.',\n",
       "  'In WikiMultihopQA, the termination for Exploring Neighbors has a ratio of 62.7%.',\n",
       "  'The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'The multi-hop long-context QA benchmarks include HotpotQA, 2WikiMulti-hopQA, and MuSiQue.',\n",
       "  'In WikiMultihopQA, the Exploring Neighbors stage has a ratio of 25.1%.',\n",
       "  'Figure 3 shows the performance of GraphReader with different initial node numbers on 2WikiMultihopQA and NarrativeQA.',\n",
       "  '2WikiMultihopQAGraphReader without Rational Plan scored 81.3, 86.0, and 65.4.',\n",
       "  'The sample dimension for WikiMultihopQA has an average of 515.8 node numbers and a maximum of 1691.0.',\n",
       "  '2WikiMultihopQAGraphReader achieved scores of 83.7, 87.0, and 70.1.',\n",
       "  'In WikiMultihopQA, the read_subsequent_chunk function call has a ratio of 23.3%.',\n",
       "  'In WikiMultihopQA, the termination stage has a ratio of 37.1%.'],\n",
       " 'musiqu': ['The paper titled \"Musique: Multi-hop questions via single-hop question composition\" was authored by Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal in 2022.',\n",
       "  'In MuSiQue, the read_previous_chunk function call has a ratio of 26.6%.',\n",
       "  'The number of atomic facts for MusiQue has an average of 419.9 and a maximum of 586.0.',\n",
       "  'In MuSiQue, the Exploring Chunks stage has a ratio of 31.2%.',\n",
       "  'In MuSiQue, the search_more function call has a ratio of 19.1%.',\n",
       "  'The dataset MuSiQue has an average of 15.5k tokens, a maximum of 16.0k tokens, and 200 samples.',\n",
       "  'The average neighbor node count in MusiQue is 9.3.',\n",
       "  'MuSiQue was developed by Trivedi et al. in 2022.',\n",
       "  'The sample dimension for MusiQue has an average of 1029.4 node numbers and a maximum of 2142.0.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) is applicable to MuSiQue.',\n",
       "  'The average of atomic facts per neighbor node in MusiQue is 2.1 with a maximum of 15.6.',\n",
       "  'The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'The multi-hop long-context QA benchmarks include HotpotQA, 2WikiMulti-hopQA, and MuSiQue.',\n",
       "  'In MuSiQue, the stop_and_read_neighbor function call has a ratio of 58.7%.',\n",
       "  'In MuSiQue, the Exploring Atomic Facts stage has a ratio of 40.0%.',\n",
       "  'The document includes statistics of function calls on MuSiQue and NarrativeQA.',\n",
       "  'In MuSiQue, the read_chunk function call has a ratio of 41.3%.'],\n",
       " 'yang et al ': ['HotpotQA was developed by Yang et al. in 2018.'],\n",
       " '2018': ['Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'HotpotQA was developed by Yang et al. in 2018.',\n",
       "  'NarrativeQA was developed by Kociský et al. in 2018.',\n",
       "  'Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'ho et al ': ['2WikiMulti-hopQA was developed by Ho et al. in 2020.'],\n",
       " 'trivedi et al ': ['MuSiQue was developed by Trivedi et al. in 2022.'],\n",
       " 'narrativeqa': ['NarrativeQAGraphReader without Rational Plan scored 63.0, 78.5, and 26.6.',\n",
       "  'The single-hop long-context QA benchmark is NarrativeQA.',\n",
       "  'NarrativeQA is a single-hop QA dataset that tests comprehension abilities for long documents sourced from movie scripts.',\n",
       "  'The dataset includes various models such as NarrativeQA and HotpotWikiQA-mixup16k.',\n",
       "  'For NarrativeQA, the average nodes is 966.0, while the maximum node count is 3110.0.',\n",
       "  'Characters, such as the protagonist, appear frequently throughout the text of NarrativeQA.',\n",
       "  'NarrativeQAGraphReader achieved scores of 65.0, 80.0, and 29.8.',\n",
       "  'The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'NarrativeQA was developed by Kociský et al. in 2018.',\n",
       "  'The maximum average number of atomic facts is found in NarrativeQA.',\n",
       "  'A possible explanation for the maximum average is that NarrativeQA is mainly derived from movie scripts.',\n",
       "  'The document includes statistics of function calls on MuSiQue and NarrativeQA.',\n",
       "  'NarrativeQAGraphReader without Node Selection scored 53.0, 65.5, and 24.0.',\n",
       "  'Figure 3 shows the performance of GraphReader with different initial node numbers on 2WikiMultihopQA and NarrativeQA.'],\n",
       " 'kocisk  et al ': ['NarrativeQA was developed by Kociský et al. in 2018.'],\n",
       " 'longbench': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'LongBench is authored by Bai et al. in 2023.',\n",
       "  'In the experiments, datasets from LongBench are used.',\n",
       "  'The performance comparison of different baselines on datasets from LongBench is provided in Table 2.'],\n",
       " 'bai et al ': ['LongBench is authored by Bai et al. in 2023.'],\n",
       " 'multi hop benchmark': ['HotpotWikiQA-mixup is a multi-hop benchmark featuring five levels of text length: 16k, 32k, 64k, 128k, and 256k.'],\n",
       " 'table 1': ['Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration.',\n",
       "  'The average tokens for ReadAgent and GraphReader are shown in Table 5.',\n",
       "  'In Table 8, “avg.” indicates the average number of nodes in each graph.',\n",
       "  'Table 4 presents the results of an ablation study.',\n",
       "  'Table 5 refers to the comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k.',\n",
       "  'Recall rates are presented in Table 6 for different granularities.',\n",
       "  'Table 1 provides statistics about these benchmarks.',\n",
       "  'Table 4 illustrates the outcomes of the experiments related to node selection.',\n",
       "  'In Table 8, “max” refers to the largest node count across all graphs.',\n",
       "  'Statistics of graphs from various datasets are presented in Table 8.',\n",
       "  'The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'The results shown in Table 3 indicate that the GraphReader outperforms all baseline methods across text lengths ranging from 16k to 256k tokens.',\n",
       "  'The performance comparison of different baselines on datasets from LongBench is provided in Table 2.',\n",
       "  'Long-context benchmarks are shown in Table 2 and Table 3.',\n",
       "  \"Table 3 presents performance percentages of different baselines on datasets from LV-Eval, with F1* indicating LV-Eval's optimized F1.\",\n",
       "  'The statistics of benchmarks employed in the evaluation are presented in Table 1.'],\n",
       " 'statist': ['Statistics of graphs from various datasets are presented in Table 8.',\n",
       "  'Table 1 provides statistics about these benchmarks.',\n",
       "  'These statistics indicate the effectiveness of GraphReader.',\n",
       "  'The document includes statistics of function calls on MuSiQue and NarrativeQA.',\n",
       "  'Statistics on function calls at each stage across two datasets are made to verify the actions of GraphReader.',\n",
       "  'The statistics of benchmarks employed in the evaluation are presented in Table 1.'],\n",
       " 'appendix ': ['The evaluation metrics details can be found in Appendix B.',\n",
       "  'Detailed information is available in Appendix C.',\n",
       "  'The detailed calculations are presented in Appendix E.',\n",
       "  'The methods are detailed in Appendix D.',\n",
       "  'Further details and evaluation prompts can be found in Appendix E.'],\n",
       " 'evaluation metr': ['The evaluation metrics are introduced by LV-Eval in 2024.',\n",
       "  'The evaluation metrics details can be found in Appendix B.',\n",
       "  'The evaluation metrics are cost-effective.',\n",
       "  'The performance metrics for BM25 (top-1) and BM25 (top-3) vary across different evaluation metrics such as precision for multiple datasets.',\n",
       "  'The evaluation metrics employed include F1 score, Exact Match (EM) score, and optimized F1* score.'],\n",
       " 'f1 score': ['The dataset methods and results are presented in the context of LR-1 and LR-2 in terms of F1 scores.',\n",
       "  'The F1 scores are listed for different dataset comparisons, such as Hotpot and WikiQA-mixup.',\n",
       "  'The comparison includes methods and their performance metrics: LR-1, LR-2 and F1 scores are measured.',\n",
       "  'The evaluation metrics employed include F1 score, Exact Match (EM) score, and optimized F1* score.',\n",
       "  'F1 score is calculated if the recall exceeds a certain threshold.'],\n",
       " 'exact match  em  scor': ['The evaluation metrics employed include F1 score, Exact Match (EM) score, and optimized F1* score.'],\n",
       " 'optimized f1  scor': ['The evaluation metrics employed include F1 score, Exact Match (EM) score, and optimized F1* score.'],\n",
       " 'f1': ['BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'F1* computes the recall of golden answer keywords first.',\n",
       "  \"Table 3 presents performance percentages of different baselines on datasets from LV-Eval, with F1* indicating LV-Eval's optimized F1.\",\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.'],\n",
       " 'recal': ['If the recall does not exceed the threshold, the score defaults to zero.',\n",
       "  'The recall of supporting facts is evaluated using GPT-4 on the HotpotWikiQA-mixup dataset.',\n",
       "  'F1 score is calculated if the recall exceeds a certain threshold.',\n",
       "  'A sample is considered to be recalled only if all of its supporting facts are recalled at sample granularity.',\n",
       "  'GraphReader maintains around 60% recall at 256k context length.',\n",
       "  'F1* computes the recall of golden answer keywords first.',\n",
       "  'The recall for the final notebook is slightly higher than the recall of atomic facts.'],\n",
       " 'golden answer keyword': ['F1* computes the recall of golden answer keywords first.'],\n",
       " 'threshold': ['Beyond the threshold of 5 initial nodes, performance declines, especially in single-hop scenarios.',\n",
       "  'If the recall does not exceed the threshold, the score defaults to zero.',\n",
       "  'F1 score is calculated if the recall exceeds a certain threshold.',\n",
       "  'When the chunk size L exceeds a certain threshold, performance declines because larger chunks cause the model to overlook essential details.'],\n",
       " 'score': ['The performance metrics for LR-2 correspond to scores like 18.0, 11.9, and 6.0 for increasing input sizes.',\n",
       "  'BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'Ada-002 (top-3) achieved scores of 72.0, 77.3, 45.0, 58.1, 65.7, 66.7, 44.7, 55.3, 40.0, 45.5, 24.5, 32.1, 45.5, 53.0, 7.5, and 19.5 in various metrics.',\n",
       "  'GPT-4-128k (chunk w/ notes) achieved scores of 72.3, 76.7, 45.7, 59.5, 65.7, 68.7, 46.3, 56.6, 39.5, 43.0, 25.0, 32.5, 56.5, 65.0, 8.5, and 24.3.',\n",
       "  'ReadAgent 128k achieved scores of 72.3, 78.7, 48.0, 62.0, 79.0, 81.0, 52.7, 63.7, 54.5, 61.0, 35.0, 45.1, 63.0, 75.5, 5.0, and 18.9.',\n",
       "  'GPT-4-128k achieves a score with a collection size of 128k with values 38.0, 38.0, 35.7, 26.0, 30.0, 26.0, 22.0, 24.0, 20.6, 16.0, 16.0, 14.6, 14.0, 16.0, and 10.3.',\n",
       "  'If the recall does not exceed the threshold, the score defaults to zero.',\n",
       "  'GraphReader 4k achieved scores of 84.3, 89.7, 55.0, 70.0, 83.7, 87.0, 59.3, 70.1, 59.0, 63.5, 38.0, 47.4, 65.0, 80.0, 15.5, and 29.8.',\n",
       "  'Ada-002 (top-1) achieved scores of 63.0, 70.7, 40.0, 53.2, 57.0, 59.3, 41.0, 49.4, 34.5, 37.0, 20.0, 26.6, 37.5, 46.5, 5.0, and 15.5 in various metrics.',\n",
       "  'This involves two scores: LLM-Rating-1 (LR-1) and LLM-Rating-2 (LR-2).',\n",
       "  'The performance metrics for LR-1 show scores like 10.0, 16.0, 12.0 for 4k, 10k, and 16k inputs respectively.',\n",
       "  'GPT-4-128k achieved scores of 83.3, 88.3, 53.0, 68.4, 77.3, 80.0, 58.7, 70.0, 52.0, 59.5, 33.5, 42.7, 63.5, 77.0, 11.5, and 29.4 in various metrics.',\n",
       "  'GPT-4-128k (chunk) achieved scores of 71.3, 74.7, 45.7, 59.5, 59.3, 62.3, 40.7, 50.5, 41.0, 43.0, 23.0, 32.1, 58.0, 69.5, 9.50, and 25.5.',\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.'],\n",
       " 'zero': ['If the recall does not exceed the threshold, the score defaults to zero.'],\n",
       " 'cost effect': ['The evaluation metrics are cost-effective.'],\n",
       " 'automatic metr': ['Automatic metrics can be affected by the response format.'],\n",
       " 'accuraci': ['Smaller chunks lead to more semantic truncation, hindering comprehension and accuracy in extracting atomic facts.',\n",
       "  'The accuracy of the latter is based on the question and correct answer.',\n",
       "  'Automatic metrics can be affected by the response format.'],\n",
       " 'response format': ['Automatic metrics can be affected by the response format.'],\n",
       " 'llm rater': ['If either LLM Rater deems an answer correct, it is considered correct.',\n",
       "  'The B LLM Rater evaluates given a question, a golden answer, and an answer to be evaluated using an LLM.',\n",
       "  'We utilize GPT-4-128k as the LLM Rater with the temperature set to 0.1.',\n",
       "  'LLM Raters are implemented for answer correctness evaluation.'],\n",
       " 'evalu': ['Agent-based Method selects ReadAgent for its evaluation.',\n",
       "  'For ReadAgent, the evaluation focuses on the final text segments reviewed.',\n",
       "  'Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"',\n",
       "  'Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.',\n",
       "  'Up to 5 pages can be read in the evaluation.',\n",
       "  '\"Sample-wise\" refers to the granularity of sample evaluation.',\n",
       "  'The B LLM Rater evaluates given a question, a golden answer, and an answer to be evaluated using an LLM.',\n",
       "  'LLM Raters are implemented for answer correctness evaluation.',\n",
       "  'In the case of Chunk Read with Notes, both the memory and the chunk read at the final answer are evaluated.',\n",
       "  'The prompts used for evaluation are presented in Figure 13 and Figure 14 respectively.',\n",
       "  'The title of the paper \"Lv-eval\" suggests a focus on evaluation in a balanced manner.',\n",
       "  'The statistics of benchmarks employed in the evaluation are presented in Table 1.'],\n",
       " 'llm rating 1': ['LLM-Rating-2 (LR-2) is a more lenient scoring criterion.',\n",
       "  'LLM Raters are denoted as LLM-Rating-1 (LR-1) and LLM-Rating-2 (LR-2).',\n",
       "  'This involves two scores: LLM-Rating-1 (LR-1) and LLM-Rating-2 (LR-2).',\n",
       "  'LLM-Rating-1 (LR-1) represents a strict scoring criterion.'],\n",
       " 'baseline method': ['GPT-4-128k is used for both the method and baseline approaches in the experiments.',\n",
       "  'The model consistently outperforms other baseline methods.',\n",
       "  'The results shown in Table 3 indicate that the GraphReader outperforms all baseline methods across text lengths ranging from 16k to 256k tokens.',\n",
       "  'The approach is compared with baseline methods: retrieval augmented generation (RAG), long-context LLM, and agent-based methods.',\n",
       "  'Baseline methods are referenced, indicating methods against which experiments may be compared.'],\n",
       " 'rag': ['RAG uses Okapi BM25 or OpenAI API embedding model Ada-002 for retrieval.'],\n",
       " 'okapi bm25': ['RAG uses Okapi BM25 or OpenAI API embedding model Ada-002 for retrieval.',\n",
       "  'Retrieval methods based on Okapi BM25 and OpenAI API embedding model are included in the comparison.'],\n",
       " 'openai api embedding model': ['RAG uses Okapi BM25 or OpenAI API embedding model Ada-002 for retrieval.',\n",
       "  'Retrieval methods based on Okapi BM25 and OpenAI API embedding model are included in the comparison.'],\n",
       " 'ada 002': ['Ada-002 achieves a top-1 score of 4k with values 10.0, 12.0, 14.5, 14.0, 18.0, 11.3, 10.0, 12.0, 12.5, 12.0, 14.0, 9.4, 8.0, 8.0, and 7.0.',\n",
       "  'Ada-002 (top-3) achieved scores of 72.0, 77.3, 45.0, 58.1, 65.7, 66.7, 44.7, 55.3, 40.0, 45.5, 24.5, 32.1, 45.5, 53.0, 7.5, and 19.5 in various metrics.',\n",
       "  'Ada-002 achieves a top-3 score of 4k with values 24.0, 28.0, 21.3, 20.0, 30.0, 19.8, 14.0, 20.0, 12.9, 16.0, 20.0, 12.0, 14.0, 18.0, and 10.8.',\n",
       "  'RAG uses Okapi BM25 or OpenAI API embedding model Ada-002 for retrieval.',\n",
       "  'Ada-002 (top-1) achieved scores of 63.0, 70.7, 40.0, 53.2, 57.0, 59.3, 41.0, 49.4, 34.5, 37.0, 20.0, 26.6, 37.5, 46.5, 5.0, and 15.5 in various metrics.',\n",
       "  'The results indicate that RAG methods based on BM25 and Ada-002 exhibit the worst performance compared to long-context LLM and agent-based methods.'],\n",
       " 'full text read': [\"The Full Text Read method cannot be used for texts that exceed the LLM's input window tokens.\",\n",
       "  \"The Full Text Read method can be used for texts that are fewer than the LLM's input window tokens.\",\n",
       "  'The performance of GPT-4-128k full-text reading degrades gradually with an increase in the length of the input context.',\n",
       "  'The GraphReader achieves a performance gain of 10.53% relatively on LR-1 over GPT-4-128k full-text reading under 16k context length.',\n",
       "  'The prompt includes a figure related to Full Text Read and Chunk Read instructions.',\n",
       "  'Long-context LLM uses GPT-4-128k for reading full text or for segmenting the text into chunks.',\n",
       "  'ReadAgent performs worse than GPT-4-128k full-text reading.'],\n",
       " 'segmenting text': ['Long-context LLM uses GPT-4-128k for reading full text or for segmenting the text into chunks.'],\n",
       " 'lee et al ': ['ReadAgent is authored by Lee et al., 2024.',\n",
       "  'Lee et al. truncated the text to fit it into the LLM when it exceeded token limits.'],\n",
       " 'execut': ['The superior performance of GPT-4-128k lies in its ability to process long texts and execute multi-hop reasoning tasks.',\n",
       "  'An agent-based system is employed for the execution of retrieval and reading processes for long-context QA.'],\n",
       " 'reading process': ['There are two main strategies during the reading process: Chunk Read and Chunk Read with Notes.',\n",
       "  'An agent-based system is employed for the execution of retrieval and reading processes for long-context QA.'],\n",
       " 'temperatur': ['The recall rate of supporting facts is evaluated for different methods using GPT-4-128k with a temperature of 0.1.',\n",
       "  'The temperature is set to 0.2.',\n",
       "  'We utilize GPT-4-128k as the LLM Rater with the temperature set to 0.1.'],\n",
       " '10': ['BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'The performance metrics for LR-2 correspond to scores like 18.0, 11.9, and 6.0 for increasing input sizes.',\n",
       "  'In HotpotQA, the search_more function call has a ratio of 12.1%.',\n",
       "  'NarrativeQAGraphReader without Rational Plan scored 63.0, 78.5, and 26.6.',\n",
       "  'A function call limit of 10 is imposed for each search path.',\n",
       "  'In HotpotQA, the termination for Exploring Neighbors has a ratio of 65.5%.',\n",
       "  'ReadAgent has a capacity of 128k, displaying efficiency measures of 24.0, 26.0, 29.2, 20.0, 22.0, 16.9, 24.0, 30.0, 15.3, 14.0, 18.0, 13.6, 20.0, 22.0, and 10.4.',\n",
       "  'ReadAgent 128k achieved scores of 72.3, 78.7, 48.0, 62.0, 79.0, 81.0, 52.7, 63.7, 54.5, 61.0, 35.0, 45.1, 63.0, 75.5, 5.0, and 18.9.',\n",
       "  'GPT-4-128k (chunk w/ notes) achieved scores of 72.3, 76.7, 45.7, 59.5, 65.7, 68.7, 46.3, 56.6, 39.5, 43.0, 25.0, 32.5, 56.5, 65.0, 8.5, and 24.3.',\n",
       "  'In HotpotQA, the Exploring Atomic Facts stage has a ratio of 42.0%.',\n",
       "  'In HotpotQA, the read_previous_chunk function call has a ratio of 21.1%.',\n",
       "  'In HotpotQA, the read_chunk function call has a ratio of 46.5%.',\n",
       "  'In WikiMultihopQA, the Exploring Chunks stage has a ratio of 34.5%.',\n",
       "  'We utilize GPT-4-128k as the LLM Rater with the temperature set to 0.1.',\n",
       "  'In MuSiQue, the Exploring Atomic Facts stage has a ratio of 40.0%.',\n",
       "  'The performance metrics for LR-1 show scores like 10.0, 16.0, 12.0 for 4k, 10k, and 16k inputs respectively.',\n",
       "  'HotpotQAGraphReader achieved scores of 84.3, 89.7, and 70.0.',\n",
       "  'GPT-4-128k achieved scores of 83.3, 88.3, 53.0, 68.4, 77.3, 80.0, 58.7, 70.0, 52.0, 59.5, 33.5, 42.7, 63.5, 77.0, 11.5, and 29.4 in various metrics.',\n",
       "  'In WikiMultihopQA, the search_more function call has a ratio of 14.5%.',\n",
       "  'The average number of atomic facts in the node with the most atomic facts in each graph ranges from 15 to 50.',\n",
       "  'In MuSiQue, the read_previous_chunk function call has a ratio of 26.6%.',\n",
       "  'In MuSiQue, the search_more function call has a ratio of 19.1%.',\n",
       "  'HotpotQAGraphReader without Node Selection scored 66.0, 71.7, and 54.1.',\n",
       "  'In WikiMultihopQA, the Exploring Atomic Facts stage has a ratio of 40.4%.',\n",
       "  'In WikiMultihopQA, the read_previous_chunk function call has a ratio of 25.1%.',\n",
       "  'GPT-4-128k achieves a score with a collection size of 128k with values 38.0, 38.0, 35.7, 26.0, 30.0, 26.0, 22.0, 24.0, 20.6, 16.0, 16.0, 14.6, 14.0, 16.0, and 10.3.',\n",
       "  '2WikiMultihopQAGraphReader without Node Selection scored 65.3, 68.7, and 49.7.',\n",
       "  'MuSiQueGraphReader without Node Selection scored 35.0, 38.5, and 25.2.',\n",
       "  'In WikiMultihopQA, the stop_and_read_neighbor function call has a ratio of 51.4%.',\n",
       "  'In MuSiQue, the stop_and_read_neighbor function call has a ratio of 58.7%.',\n",
       "  'The GraphReader achieves a performance gain of 10.53% relatively on LR-1 over GPT-4-128k full-text reading under 16k context length.',\n",
       "  'In WikiMultihopQA, the read_neighbor_node function call has a ratio of 37.3%.',\n",
       "  'GPT-4-128k (chunk) achieved scores of 71.3, 74.7, 45.7, 59.5, 59.3, 62.3, 40.7, 50.5, 41.0, 43.0, 23.0, 32.1, 58.0, 69.5, 9.50, and 25.5.',\n",
       "  'NarrativeQAGraphReader without Node Selection scored 53.0, 65.5, and 24.0.',\n",
       "  'BM25 achieves a top-3 score of 4k with values 16.0, 22.0, 13.9, 18.0, 28.0, 13.3, 16.0, 18.0, 11.8, 12.0, 16.0, 11.8, 12.0, 22.0, and 9.3.',\n",
       "  'The temperature is set to 0.2.',\n",
       "  'Ada-002 (top-3) achieved scores of 72.0, 77.3, 45.0, 58.1, 65.7, 66.7, 44.7, 55.3, 40.0, 45.5, 24.5, 32.1, 45.5, 53.0, 7.5, and 19.5 in various metrics.',\n",
       "  'In HotpotQA, the stop_and_read_neighbor function call has a ratio of 53.5%.',\n",
       "  'In HotpotQA, the Exploring Chunks stage has a ratio of 31.9%.',\n",
       "  'In WikiMultihopQA, the read_chunk function call has a ratio of 48.6%.',\n",
       "  'In MuSiQue, the Exploring Chunks stage has a ratio of 31.2%.',\n",
       "  'GraphReader has a token capacity of 4k, with recorded values of 42.0, 42.0, 38.2, 32.0, 38.0, 36.4, 30.0, 36.0, 32.9, 28.0, 34.0, 30.6, 30.0, 38.0, and 33.0.',\n",
       "  'In HotpotQA, the Exploring Neighbors stage has a ratio of 26.1%.',\n",
       "  'In WikiMultihopQA, the termination for Exploring Neighbors has a ratio of 62.7%.',\n",
       "  'GraphReader 4k achieved scores of 84.3, 89.7, 55.0, 70.0, 83.7, 87.0, 59.3, 70.1, 59.0, 63.5, 38.0, 47.4, 65.0, 80.0, 15.5, and 29.8.',\n",
       "  'HotpotQAGraphReader without Rational Plan scored 81.7, 87.7, and 63.8.',\n",
       "  'In WikiMultihopQA, the Exploring Neighbors stage has a ratio of 25.1%.',\n",
       "  'In MuSiQue, the read_chunk function call has a ratio of 41.3%.',\n",
       "  'NarrativeQAGraphReader achieved scores of 65.0, 80.0, and 29.8.',\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'GPT-4-128k (chunk with notes) also has a maximum token count of 4,000, showing performance metrics of 22.0, 32.0, 24.2, 26.0, 30.0, 21.3, 28.0, 32.0, 22.0, 24.0, 26.0, 17.4, 26.0, 26.0, and 14.8.',\n",
       "  'Ada-002 achieves a top-1 score of 4k with values 10.0, 12.0, 14.5, 14.0, 18.0, 11.3, 10.0, 12.0, 12.5, 12.0, 14.0, 9.4, 8.0, 8.0, and 7.0.',\n",
       "  'Ada-002 achieves a top-3 score of 4k with values 24.0, 28.0, 21.3, 20.0, 30.0, 19.8, 14.0, 20.0, 12.9, 16.0, 20.0, 12.0, 14.0, 18.0, and 10.8.',\n",
       "  'MuSiQueGraphReader without Rational Plan scored 56.0, 61.0, and 42.4.',\n",
       "  '2WikiMultihopQAGraphReader without Rational Plan scored 81.3, 86.0, and 65.4.',\n",
       "  'In HotpotQA, the termination stage has a ratio of 43.9%.',\n",
       "  '2WikiMultihopQAGraphReader achieved scores of 83.7, 87.0, and 70.1.',\n",
       "  'In HotpotQA, the read_neighbor_node function call has a ratio of 35.5%.',\n",
       "  'In WikiMultihopQA, the read_subsequent_chunk function call has a ratio of 23.3%.',\n",
       "  'Ada-002 (top-1) achieved scores of 63.0, 70.7, 40.0, 53.2, 57.0, 59.3, 41.0, 49.4, 34.5, 37.0, 20.0, 26.6, 37.5, 46.5, 5.0, and 15.5 in various metrics.',\n",
       "  'In WikiMultihopQA, the termination stage has a ratio of 37.1%.',\n",
       "  'GPT-4-128k (chunk) has a maximum token count of 4,000 with varying data points across different categories: 18.0, 22.0, 24.6, 16.0, 20.0, 17.7, 20.0, 24.0, 17.0, 20.0, 24.0, 14.7, 28.0, 30.0, and 10.7.',\n",
       "  'MuSiQueGraphReader achieved scores of 59.0, 63.5, and 47.4.',\n",
       "  'Recall performance at sample-wise granularity is 64.7% for atomic facts and 85.3% for the final notebook.',\n",
       "  'In HotpotQA, the read_subsequent_chunk function call has a ratio of 22.9%.',\n",
       "  'GraphReader shows a recall performance of 76.4% for atomic facts and 90.5% for the final notebook at SF-wise granularity.'],\n",
       " 'input window': ['The input window for GPT-4-128k is 128k and 256k.',\n",
       "  'The maximum length of the chunk is set to fill the input window as much as possible.',\n",
       "  'The method enables handling overly long texts with a limited input window.',\n",
       "  'BM25 (top-1) shows performance metrics for various input window sizes.',\n",
       "  'The method is compared with similar approaches for handling long texts with small input windows.',\n",
       "  'Various input window sizes are evaluated: 4k, 10k, 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The input window size for GraphReader is configured to 4k tokens unless stated otherwise.',\n",
       "  'The input window is controlled to 4k in the experiments.'],\n",
       " '4k token': ['GraphReader has an average context of 358.3k tokens and an average cost of 52.8k tokens.',\n",
       "  'The dataset 2WikiMultihopQA has an average of 8.8k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The dataset MuSiQue has an average of 15.5k tokens, a maximum of 16.0k tokens, and 200 samples.',\n",
       "  'The maximum chunk size is limited to 2k tokens.',\n",
       "  'The dataset HotpotQA has an average of 9.4k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'ReadAgent has an average context of 358.3k tokens and an average cost of 48.7k tokens.',\n",
       "  'The input window size for GraphReader is configured to 4k tokens unless stated otherwise.',\n",
       "  'Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.'],\n",
       " 'maximum chunk s': ['The maximum chunk size is limited to 2k tokens.'],\n",
       " 'function call limit': ['A function call limit of 10 is imposed for each search path.'],\n",
       " 'search path': ['A function call limit of 10 is imposed for each search path.'],\n",
       " 'result': ['The dataset methods and results are presented in the context of LR-1 and LR-2 in terms of F1 scores.',\n",
       "  'The results of the experiments are shown in Figure 3.',\n",
       "  'Results of long-context LLMs are discussed later in the text.',\n",
       "  'The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'The results of the recall evaluation are displayed in Figure 5.',\n",
       "  'The results shown in Table 3 indicate that the GraphReader outperforms all baseline methods across text lengths ranging from 16k to 256k tokens.',\n",
       "  'Results are obtained from three types of methods on four multi-hop long-context benchmarks and one single-hop task.'],\n",
       " 'averag': ['The dataset 2WikiMultihopQA has an average of 8.8k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The number of atomic facts for WikiMultihopQA has an average of 217.7 and a maximum of 545.0.',\n",
       "  'The average number of atomic facts in the node with the most atomic facts in each graph ranges from 15 to 50.',\n",
       "  'The sample dimension for HotpotQA has an average of 583.8 node numbers and a maximum of 1945.0.',\n",
       "  'The average of atomic facts per neighbor node in WikiMultihopQA is 2.1 with a maximum of 17.0.',\n",
       "  'The sample dimension for MusiQue has an average of 1029.4 node numbers and a maximum of 2142.0.',\n",
       "  'Each piece of data performs an average of 3 to 4 actions corresponding to the average number of function calls.',\n",
       "  'After normalization, each node has an average of about 10 neighbor nodes.',\n",
       "  'The number of atomic facts for MusiQue has an average of 419.9 and a maximum of 586.0.',\n",
       "  'The number of atomic facts for HotpotQA has an average of 244.0 and a maximum of 645.0.',\n",
       "  'The average neighbor node count in WikiMultihopQA is 9.2.',\n",
       "  'The average neighbor node count in MusiQue is 9.3.',\n",
       "  'The average is calculated for neighbor node counts.',\n",
       "  'The dataset HotpotQA has an average of 9.4k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The average of atomic facts per neighbor node in MusiQue is 2.1 with a maximum of 15.6.',\n",
       "  'The average of atomic facts per neighbor node in HotpotQA is 2.1 with a maximum of 17.8.',\n",
       "  'On average, each node is associated with about 2 atomic facts.',\n",
       "  'The values represent performance metrics including statistical figures like averages and maximums for each model.',\n",
       "  'The dataset MuSiQue has an average of 15.5k tokens, a maximum of 16.0k tokens, and 200 samples.',\n",
       "  'The sample dimension for WikiMultihopQA has an average of 515.8 node numbers and a maximum of 1691.0.',\n",
       "  'The number of neighbor nodes in HotpotQA has an average of 10.1.'],\n",
       " 'max': ['For datasets with lengths of 256k and 128k, the max_words is 10000 and min_words is 2000.',\n",
       "  'The dataset 2WikiMultihopQA has an average of 8.8k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The excessive length of the dataset leads to selection of the first 50 entries from each context length for experimentation to control costs.',\n",
       "  'For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.',\n",
       "  \"To answer the question, it's necessary to find the length of Danny's and Alice's tennis careers.\",\n",
       "  'The number of atomic facts for WikiMultihopQA has an average of 217.7 and a maximum of 545.0.',\n",
       "  '2WikiMultihopQA is comprised of complex questions up to 5-hops in length.',\n",
       "  'The sample dimension for HotpotQA has an average of 583.8 node numbers and a maximum of 1945.0.',\n",
       "  'The average of atomic facts per neighbor node in WikiMultihopQA is 2.1 with a maximum of 17.0.',\n",
       "  'The sample dimension for MusiQue has an average of 1029.4 node numbers and a maximum of 2142.0.',\n",
       "  'When opting for the top-3 chunks, the maximum length of each chunk is set to 1k.',\n",
       "  'The number of atomic facts for MusiQue has an average of 419.9 and a maximum of 586.0.',\n",
       "  'The number of atomic facts for HotpotQA has an average of 244.0 and a maximum of 645.0.',\n",
       "  'The maximum length of the chunk is set to fill the input window as much as possible.',\n",
       "  'The dataset HotpotQA has an average of 9.4k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The average of atomic facts per neighbor node in MusiQue is 2.1 with a maximum of 15.6.',\n",
       "  'The average of atomic facts per neighbor node in HotpotQA is 2.1 with a maximum of 17.8.',\n",
       "  'The lengths of four benchmarks are significantly shorter than the 128k context window.',\n",
       "  'The values represent performance metrics including statistical figures like averages and maximums for each model.',\n",
       "  'The dataset MuSiQue has an average of 15.5k tokens, a maximum of 16.0k tokens, and 200 samples.',\n",
       "  'In Table 8, “max” refers to the largest node count across all graphs.',\n",
       "  'The sample dimension for WikiMultihopQA has an average of 515.8 node numbers and a maximum of 1691.0.',\n",
       "  'The performance of GPT-4-128k full-text reading degrades gradually with an increase in the length of the input context.',\n",
       "  'The maximum length of the chunk is set to 2k.'],\n",
       " 'sampl': ['The dataset 2WikiMultihopQA has an average of 8.8k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The dataset MuSiQue has an average of 15.5k tokens, a maximum of 16.0k tokens, and 200 samples.',\n",
       "  'The dataset HotpotQA has an average of 9.4k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The total number of benchmarks is denoted by #Samples.',\n",
       "  'A sample is considered to be recalled only if all of its supporting facts are recalled at sample granularity.'],\n",
       " '300': ['The dataset 2WikiMultihopQA has an average of 8.8k tokens, a maximum of 15.9k tokens, and 300 samples.',\n",
       "  'The dataset HotpotQA has an average of 9.4k tokens, a maximum of 15.9k tokens, and 300 samples.'],\n",
       " '200': ['The dataset MuSiQue has an average of 15.5k tokens, a maximum of 16.0k tokens, and 200 samples.'],\n",
       " 'token numb': ['The average context tokens refer to the average token number of the original dataset.',\n",
       "  'The token number is calculated using the GPT-4 tokenizer from TikToken.'],\n",
       " 'tiktoken': ['The token number is calculated using the GPT-4 tokenizer from TikToken.'],\n",
       " 'total numb': ['Each piece of data performs an average of 3 to 4 actions corresponding to the average number of function calls.',\n",
       "  'For longer texts, there tends to be a higher average number of nodes and atomic facts.',\n",
       "  'The total number of benchmarks is denoted by #Samples.'],\n",
       " 'rag method': ['The work compares traditional RAG methods.',\n",
       "  'The context window will limit the effectiveness of RAG methods.',\n",
       "  'Employing GPT-4-128k to directly answer questions with long contexts significantly outperforms RAG methods.',\n",
       "  'For the RAG methods, retrieved chunks are assessed.',\n",
       "  'The results indicate that RAG methods based on BM25 and Ada-002 exhibit the worst performance compared to long-context LLM and agent-based methods.',\n",
       "  'A possible reason for the poor performance of RAG methods is difficulty in recalling all chunks that contain supporting facts.'],\n",
       " 'poor perform': ['A possible reason for the poor performance of RAG methods is difficulty in recalling all chunks that contain supporting facts.'],\n",
       " 'recalled chunk': ['Increasing the number of recalled chunks could improve the performance of text retrieval.',\n",
       "  'A possible reason for the poor performance of RAG methods is difficulty in recalling all chunks that contain supporting facts.'],\n",
       " 'text retriev': ['Increasing the number of recalled chunks could improve the performance of text retrieval.'],\n",
       " 'multi hop reasoning task': ['The superior performance of GPT-4-128k lies in its ability to process long texts and execute multi-hop reasoning tasks.'],\n",
       " 'abil': ['The superior performance of GPT-4-128k lies in its ability to process long texts and execute multi-hop reasoning tasks.'],\n",
       " 'significantly short': ['The lengths of four benchmarks are significantly shorter than the 128k context window.'],\n",
       " 'shorter benchmark length': ['Shorter benchmark lengths mitigate the impact of “lost in the middle” on model performance.'],\n",
       " 'mitig': ['Shorter benchmark lengths mitigate the impact of “lost in the middle” on model performance.'],\n",
       " 'baselin': [\"Table 3 presents performance percentages of different baselines on datasets from LV-Eval, with F1* indicating LV-Eval's optimized F1.\",\n",
       "  'The performance comparison of different baselines on datasets from LongBench is provided in Table 2.',\n",
       "  'Our approach consistently performs better than all baselines on four long-context benchmarks.'],\n",
       " 'consistently performs bett': ['Our approach consistently performs better than all baselines on four long-context benchmarks.'],\n",
       " 'benefit': [\"Our method benefits from the graph's ability to capture relationships between detailed information.\"],\n",
       " 'input quest': ['In the graph exploration stage, a rational plan is introduced to help the agent analyze complex input questions step by step.',\n",
       "  'Our method can identify crucial information and search for supporting facts for input questions efficiently.'],\n",
       " 'effici': ['Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal published \"Leave no context behind: Efficient infinite context transformers with infinite\" in 2024.',\n",
       "  'Our method can identify crucial information and search for supporting facts for input questions efficiently.',\n",
       "  'The approach of GraphReader unlocks the capabilities of constrained context window LLMs more efficiently in processing long context.'],\n",
       " 'capabl': ['The strategy significantly boosts the agent’s capability in multi-hop reasoning and capturing long-range dependencies of key information.',\n",
       "  'The method is capable of extracting more valid information from chunks during exploration.',\n",
       "  'The approach of GraphReader unlocks the capabilities of constrained context window LLMs more efficiently in processing long context.'],\n",
       " 'reada methodinput': ['The results in Table 2 show that ReadA-MethodInput significantly improves performance in various tasks such as HotpotQA, 2WikiMultihopQA, MuSiQue, and NarrativeQA.',\n",
       "  'The ReadAgent-S method is employed.'],\n",
       " 'lr 1': ['BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'The performance metrics for LR-2 correspond to scores like 18.0, 11.9, and 6.0 for increasing input sizes.',\n",
       "  'The dataset methods and results are presented in the context of LR-1 and LR-2 in terms of F1 scores.',\n",
       "  'L = 2k was chosen as the default chunk size.',\n",
       "  'The best performance of GraphReader is achieved with a chunk size of L = 2k.',\n",
       "  'The comparison includes methods and their performance metrics: LR-1, LR-2 and F1 scores are measured.',\n",
       "  'The average scores for different metrics are illustrated in the graph, with LR-1 and LR-2 as comparisons.',\n",
       "  'The GraphReader achieves a performance gain of 10.53% relatively on LR-1 over GPT-4-128k full-text reading under 16k context length.',\n",
       "  'The performance metrics for LR-1 show scores like 10.0, 16.0, 12.0 for 4k, 10k, and 16k inputs respectively.',\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.'],\n",
       " 'em': ['BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.'],\n",
       " '4k': ['BM25 (top-3) achieved scores of 74.7 for LR-1, 78.3 for LR-2, 45.7 for EM, and 58.5 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'BM25 achieves a top-3 score of 4k with values 16.0, 22.0, 13.9, 18.0, 28.0, 13.3, 16.0, 18.0, 11.8, 12.0, 16.0, 11.8, 12.0, 22.0, and 9.3.',\n",
       "  'Ada-002 achieves a top-1 score of 4k with values 10.0, 12.0, 14.5, 14.0, 18.0, 11.3, 10.0, 12.0, 12.5, 12.0, 14.0, 9.4, 8.0, 8.0, and 7.0.',\n",
       "  'Ada-002 achieves a top-3 score of 4k with values 24.0, 28.0, 21.3, 20.0, 30.0, 19.8, 14.0, 20.0, 12.9, 16.0, 20.0, 12.0, 14.0, 18.0, and 10.8.',\n",
       "  'GraphReader has a token capacity of 4k, with recorded values of 42.0, 42.0, 38.2, 32.0, 38.0, 36.4, 30.0, 36.0, 32.9, 28.0, 34.0, 30.6, 30.0, 38.0, and 33.0.',\n",
       "  'Various input window sizes are evaluated: 4k, 10k, 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The performance metrics for LR-1 show scores like 10.0, 16.0, 12.0 for 4k, 10k, and 16k inputs respectively.',\n",
       "  'GPT-4-128k (chunk) has a maximum token count of 4,000 with varying data points across different categories: 18.0, 22.0, 24.6, 16.0, 20.0, 17.7, 20.0, 24.0, 17.0, 20.0, 24.0, 14.7, 28.0, 30.0, and 10.7.',\n",
       "  'The input window is controlled to 4k in the experiments.',\n",
       "  'BM25 (top-1) achieved scores of 57.7 for LR-1, 63.0 for LR-2, 33.7 for EM, and 43.8 for F1 in the HotpotQA dataset with a context length of 4k.',\n",
       "  'GPT-4-128k (chunk with notes) also has a maximum token count of 4,000, showing performance metrics of 22.0, 32.0, 24.2, 26.0, 30.0, 21.3, 28.0, 32.0, 22.0, 24.0, 26.0, 17.4, 26.0, 26.0, and 14.8.'],\n",
       " 'performance metr': ['The performance metrics for LR-2 correspond to scores like 18.0, 11.9, and 6.0 for increasing input sizes.',\n",
       "  'The values represent performance metrics including statistical figures like averages and maximums for each model.',\n",
       "  'The performance metrics for BM25 (top-1) and BM25 (top-3) vary across different evaluation metrics such as precision for multiple datasets.',\n",
       "  'BM25 (top-1) shows performance metrics for various input window sizes.',\n",
       "  'The comparison includes methods and their performance metrics: LR-1, LR-2 and F1 scores are measured.',\n",
       "  'The performance metrics for LR-1 show scores like 10.0, 16.0, 12.0 for 4k, 10k, and 16k inputs respectively.'],\n",
       " 'precis': ['The performance metrics for BM25 (top-1) and BM25 (top-3) vary across different evaluation metrics such as precision for multiple datasets.'],\n",
       " 'dataset': ['For datasets with lengths of 256k and 128k, the max_words is 10000 and min_words is 2000.',\n",
       "  'The dataset methods and results are presented in the context of LR-1 and LR-2 in terms of F1 scores.',\n",
       "  'The excessive length of the dataset leads to selection of the first 50 entries from each context length for experimentation to control costs.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) shows data regarding HotpotQA.',\n",
       "  'The overall performance reflects different methodologies and their effectiveness in handling the datasets.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) is applicable to MuSiQue.',\n",
       "  'For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) also applies to WikiMultihopQA.',\n",
       "  'Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'The F1 scores are listed for different dataset comparisons, such as Hotpot and WikiQA-mixup.',\n",
       "  'In the experiments, datasets from LongBench are used.',\n",
       "  'The performance comparison of different baselines on datasets from LongBench is provided in Table 2.',\n",
       "  'The granularity of supporting facts refers to the recall rate of all supporting facts across the entire dataset.',\n",
       "  'The original dataset’s token count is significantly reduced.',\n",
       "  'Each question in the original datasets is supported by 2-4 paragraphs providing evidence for simple reasoning and additional decoy paragraphs.',\n",
       "  'The performance metrics for BM25 (top-1) and BM25 (top-3) vary across different evaluation metrics such as precision for multiple datasets.',\n",
       "  'The dataset includes various models such as NarrativeQA and HotpotWikiQA-mixup16k.',\n",
       "  'The average context tokens refer to the average token number of the original dataset.',\n",
       "  'Statistics on function calls at each stage across two datasets are made to verify the actions of GraphReader.',\n",
       "  'Statistics of graphs from various datasets are presented in Table 8.'],\n",
       " 'top 1': ['BM25 achieves a top-3 score of 4k with values 16.0, 22.0, 13.9, 18.0, 28.0, 13.3, 16.0, 18.0, 11.8, 12.0, 16.0, 11.8, 12.0, 22.0, and 9.3.',\n",
       "  'Ada-002 achieves a top-1 score of 4k with values 10.0, 12.0, 14.5, 14.0, 18.0, 11.3, 10.0, 12.0, 12.5, 12.0, 14.0, 9.4, 8.0, 8.0, and 7.0.',\n",
       "  'Ada-002 (top-3) achieved scores of 72.0, 77.3, 45.0, 58.1, 65.7, 66.7, 44.7, 55.3, 40.0, 45.5, 24.5, 32.1, 45.5, 53.0, 7.5, and 19.5 in various metrics.',\n",
       "  'Ada-002 achieves a top-3 score of 4k with values 24.0, 28.0, 21.3, 20.0, 30.0, 19.8, 14.0, 20.0, 12.9, 16.0, 20.0, 12.0, 14.0, 18.0, and 10.8.',\n",
       "  'Ada-002 (top-1) achieved scores of 63.0, 70.7, 40.0, 53.2, 57.0, 59.3, 41.0, 49.4, 34.5, 37.0, 20.0, 26.6, 37.5, 46.5, 5.0, and 15.5 in various metrics.'],\n",
       " 'readagent 128k': ['ReadAgent 128k achieved scores of 72.3, 78.7, 48.0, 62.0, 79.0, 81.0, 52.7, 63.7, 54.5, 61.0, 35.0, 45.1, 63.0, 75.5, 5.0, and 18.9.'],\n",
       " 'performance comparison': ['The performance comparison of different baselines on datasets from LongBench is provided in Table 2.'],\n",
       " 'golden': ['Golden denotes the settings in which question and its supporting facts are added to LLM directly.'],\n",
       " 'best perform': ['The best performance of GraphReader is achieved with a chunk size of L = 2k.',\n",
       "  'The best performance is denoted in bold and the second-best performance in underlined fonts.',\n",
       "  'The best performance is denoted in bold and the second-best performance is denoted in underlined fonts.'],\n",
       " 'bold': ['The best performance is denoted in bold and the second-best performance is denoted in underlined fonts.'],\n",
       " 'underlined font': ['The best performance is denoted in bold and the second-best performance in underlined fonts.',\n",
       "  'The best performance is denoted in bold and the second-best performance is denoted in underlined fonts.'],\n",
       " '10k': ['Various input window sizes are evaluated: 4k, 10k, 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'The performance metrics for LR-1 show scores like 10.0, 16.0, 12.0 for 4k, 10k, and 16k inputs respectively.'],\n",
       " 'hotpot': ['The F1 scores are listed for different dataset comparisons, such as Hotpot and WikiQA-mixup.'],\n",
       " 'wikiqa mixup': ['The F1 scores are listed for different dataset comparisons, such as Hotpot and WikiQA-mixup.'],\n",
       " '11 9': ['The performance metrics for LR-2 correspond to scores like 18.0, 11.9, and 6.0 for increasing input sizes.'],\n",
       " 'overall perform': ['The overall performance reflects different methodologies and their effectiveness in handling the datasets.',\n",
       "  \"ReadAgent's strategy affects its overall performance.\"],\n",
       " 'collection size 128k': ['GPT-4-128k achieves a score with a collection size of 128k with values 38.0, 38.0, 35.7, 26.0, 30.0, 26.0, 22.0, 24.0, 20.6, 16.0, 16.0, 14.6, 14.0, 16.0, and 10.3.'],\n",
       " 'performance percentag': [\"Table 3 presents performance percentages of different baselines on datasets from LV-Eval, with F1* indicating LV-Eval's optimized F1.\"],\n",
       " 'optimized f1': [\"Table 3 presents performance percentages of different baselines on datasets from LV-Eval, with F1* indicating LV-Eval's optimized F1.\"],\n",
       " 'bold font': ['The best performance is denoted in bold and the second-best performance in underlined fonts.'],\n",
       " 'underperform': ['ReadAgent significantly underperforms the method in handling extremely long contexts.',\n",
       "  'ReadAgent with a 128k context window setup underperforms GraphReader with a 4k context window.'],\n",
       " 'performs wors': ['ReadAgent performs worse than GPT-4-128k full-text reading.'],\n",
       " 'compress': ['The strategy of ReadAgent excessively compresses original texts into gist memories.'],\n",
       " 'gist memori': ['ReadAgent is mentioned as a method that segments long texts and generates gist memories.',\n",
       "  'The strategy of ReadAgent excessively compresses original texts into gist memories.',\n",
       "  'Gist memories are looked up to search for information to answer questions.'],\n",
       " 'feed': ['ReadAgent feeds all mixed memories to the model for page number selection.'],\n",
       " 'page select': ['The lack of detailed information about the content of each page makes page selection very difficult for ReadAgent.',\n",
       "  'ReadAgent feeds all mixed memories to the model for page number selection.'],\n",
       " 'restrict': [\"Compared to GraphReader, ReadAgent's strategy may restrict the agent's ability to identify specific details.\"],\n",
       " 'affect': [\"ReadAgent's strategy affects its overall performance.\"],\n",
       " 'unlock': ['The approach of GraphReader unlocks the capabilities of constrained context window LLMs more efficiently in processing long context.'],\n",
       " 'llm context window': ['The effectiveness of employing a limited context window LLM for long-context tasks with the GraphReader is demonstrated in previous experiments.',\n",
       "  'The approach of GraphReader unlocks the capabilities of constrained context window LLMs more efficiently in processing long context.',\n",
       "  'The method is effective in handling extremely long texts by graph exploration with limited context window LLMs.',\n",
       "  'The method can effectively address challenges of processing extremely long context with limited context window LLMs by exploring graphs containing fine-grained information.',\n",
       "  'Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.'],\n",
       " 'match': ['The performance of GraphReader closely matches that achieved by directly supplying supporting facts to the LLM.'],\n",
       " 'directly suppli': ['The performance of GraphReader closely matches that achieved by directly supplying supporting facts to the LLM.'],\n",
       " 'incorpor': ['The method of GraphReader incorporates various elements.'],\n",
       " 'various el': ['The method of GraphReader incorporates various elements.'],\n",
       " 'various act': ['Pre-planning, reflection, and various actions are important for using a graph that contains key information.'],\n",
       " 'previous experi': ['The effectiveness of employing a limited context window LLM for long-context tasks with the GraphReader is demonstrated in previous experiments.'],\n",
       " 'impact': ['The impact of extremely long context on the GraphReader is studied.',\n",
       "  \"The impact of chunk size on GraphReader's performance was investigated.\"],\n",
       " 'robust': ['GraphReader demonstrates robustness towards different initial node numbers.',\n",
       "  'The GraphReader exhibits robustness with the expansion of context length.'],\n",
       " 'degrad': ['The performance of GPT-4-128k full-text reading degrades gradually with an increase in the length of the input context.'],\n",
       " 'input context': ['The performance of GPT-4-128k full-text reading degrades gradually with an increase in the length of the input context.'],\n",
       " 'performance gain': [\"The performance gain can be attributed to the fact that certain conditions improve the model's efficiency.\",\n",
       "  'The context length increasing to 128k resulted in a performance gain of 75.00% over GPT-4-128k.',\n",
       "  'The GraphReader achieves a performance gain of 10.53% relatively on LR-1 over GPT-4-128k full-text reading under 16k context length.'],\n",
       " 'node select': ['Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration.',\n",
       "  'HotpotQAGraphReader without Node Selection scored 66.0, 71.7, and 54.1.',\n",
       "  'The Effect of Node Selection is demonstrated through experiments on randomly selecting initial nodes and neighbor nodes.',\n",
       "  'Table 4 illustrates the outcomes of the experiments related to node selection.',\n",
       "  '2WikiMultihopQAGraphReader without Node Selection scored 65.3, 68.7, and 49.7.',\n",
       "  'MuSiQueGraphReader without Node Selection scored 35.0, 38.5, and 25.2.',\n",
       "  '“w/o Node Selection” indicates applying random selection of initial nodes and neighbor nodes in graph exploration.',\n",
       "  'NarrativeQAGraphReader without Node Selection scored 53.0, 65.5, and 24.0.'],\n",
       " 'musiquegraphread': ['MuSiQueGraphReader achieved scores of 59.0, 63.5, and 47.4.',\n",
       "  'MuSiQueGraphReader without Node Selection scored 35.0, 38.5, and 25.2.',\n",
       "  'MuSiQueGraphReader without Rational Plan scored 56.0, 61.0, and 42.4.'],\n",
       " 'ablation studi': ['Table 4 presents the results of an ablation study.'],\n",
       " 'initialization stag': ['“w/o Rational Plan” indicates the removal of the rational plan in the agent initialization stage.'],\n",
       " 'random select': ['“w/o Node Selection” indicates applying random selection of initial nodes and neighbor nodes in graph exploration.',\n",
       "  'The Effect of Node Selection is demonstrated through experiments on randomly selecting initial nodes and neighbor nodes.',\n",
       "  'Random selection results in a significant performance drop, with an average decline of 18%.'],\n",
       " '75 00 ': ['The context length increasing to 128k resulted in a performance gain of 75.00% over GPT-4-128k.'],\n",
       " 'model s effici': [\"The performance gain can be attributed to the fact that certain conditions improve the model's efficiency.\"],\n",
       " ' lost in the middle  effect': ['As the context length increases, the impact of the “lost in the middle” effect on GPT-4-128k becomes progressively more severe.'],\n",
       " 'progressively more sever': ['As the context length increases, the impact of the “lost in the middle” effect on GPT-4-128k becomes progressively more severe.'],\n",
       " 'content': ['The lack of detailed information about the content of each page makes page selection very difficult for ReadAgent.'],\n",
       " 'each pag': ['The lack of detailed information about the content of each page makes page selection very difficult for ReadAgent.'],\n",
       " 'very difficult': ['The lack of detailed information about the content of each page makes page selection very difficult for ReadAgent.'],\n",
       " 'fine grained inform': ['The method can effectively address challenges of processing extremely long context with limited context window LLMs by exploring graphs containing fine-grained information.'],\n",
       " 'effectively address': ['The method can effectively address challenges of processing extremely long context with limited context window LLMs by exploring graphs containing fine-grained information.'],\n",
       " 'graph exploration stag': ['In the graph exploration stage, a rational plan is introduced to help the agent analyze complex input questions step by step.'],\n",
       " 'agent initi': ['The effectiveness of the rational plan is verified by removing it during agent initialization and conducting experiments on four long-context QA benchmarks.'],\n",
       " 'verifi': ['The effectiveness of the rational plan is verified by removing it during agent initialization and conducting experiments on four long-context QA benchmarks.'],\n",
       " 'necess': ['The necessity of selecting which nodes to visit is based on reasoning about required information.'],\n",
       " 'performance drop': ['Random selection results in a significant performance drop, with an average decline of 18%.'],\n",
       " 'average declin': ['Random selection results in a significant performance drop, with an average decline of 18%.'],\n",
       " 'outcom': ['Table 4 illustrates the outcomes of the experiments related to node selection.'],\n",
       " 'initial node numb': ['GraphReader demonstrates robustness towards different initial node numbers.',\n",
       "  'Figure 3 shows the performance of GraphReader with different initial node numbers on 2WikiMultihopQA and NarrativeQA.'],\n",
       " 'comparison': ['Table 5 refers to the comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k.',\n",
       "  'The average scores for different metrics are illustrated in the graph, with LR-1 and LR-2 as comparisons.',\n",
       "  \"After gathering the data, comparison of Danny's and Alice's careers can be made.\",\n",
       "  'Baseline methods are referenced, indicating methods against which experiments may be compared.',\n",
       "  'A new method is proposed that does not lose information and allows for better comparison.'],\n",
       " 'initial node count': ['Experiments were conducted with different initial node counts on multi-hop and single-hop QA datasets to assess the effect of initial nodes on GraphReader’s performance.'],\n",
       " 'increasing the number of nod': ['Increasing the number of nodes improves performance up to a certain point, with optimal performance at 5 initial nodes.'],\n",
       " 'improves perform': ['Increasing the number of nodes improves performance up to a certain point, with optimal performance at 5 initial nodes.'],\n",
       " 'optimal perform': ['Increasing the number of nodes improves performance up to a certain point, with optimal performance at 5 initial nodes.'],\n",
       " 'default': ['Five initial nodes were set as the default for experiments.'],\n",
       " 'performance declin': ['Beyond the threshold of 5 initial nodes, performance declines, especially in single-hop scenarios.',\n",
       "  'When the chunk size L exceeds a certain threshold, performance declines because larger chunks cause the model to overlook essential details.'],\n",
       " 'decline in perform': ['The decline in performance beyond 5 initial nodes is likely due to increased noise from too many initial nodes.'],\n",
       " 'increased nois': ['The decline in performance beyond 5 initial nodes is likely due to increased noise from too many initial nodes.'],\n",
       " 'chunk siz': [\"The impact of chunk size on GraphReader's performance was investigated.\"],\n",
       " 'semantic trunc': ['Smaller chunks lead to more semantic truncation, hindering comprehension and accuracy in extracting atomic facts.'],\n",
       " 'hindering comprehens': ['Smaller chunks lead to more semantic truncation, hindering comprehension and accuracy in extracting atomic facts.'],\n",
       " 'default chunk s': ['L = 2k was chosen as the default chunk size.'],\n",
       " 'section titl': ['The section title is \"Further Analysis.\"'],\n",
       " 'inference cost': ['The analysis evaluates the inference cost of the approach.'],\n",
       " 'token consumpt': ['The document preprocessing affects token consumption in subsequent exploration.',\n",
       "  'Table 5 refers to the comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k.',\n",
       "  'The analysis compares the average token consumption of ReadAgent and GraphReader for each question.'],\n",
       " 'per quest': ['Table 5 refers to the comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k.',\n",
       "  'The analysis compares the average token consumption of ReadAgent and GraphReader for each question.'],\n",
       " '1 08 time': ['GraphReader uses 1.08 times more tokens than ReadAgent.'],\n",
       " 'over twic': ['GraphReader achieves over twice the performance improvement compared to ReadAgent.'],\n",
       " 'performance improv': ['GraphReader achieves over twice the performance improvement compared to ReadAgent.'],\n",
       " 'document preprocess': ['The document preprocessing affects token consumption in subsequent exploration.'],\n",
       " 'average context': ['ReadAgent has an average context of 358.3k tokens and an average cost of 48.7k tokens.',\n",
       "  'GraphReader has an average context of 358.3k tokens and an average cost of 52.8k tokens.'],\n",
       " 'average cost': ['ReadAgent has an average context of 358.3k tokens and an average cost of 48.7k tokens.',\n",
       "  'GraphReader has an average context of 358.3k tokens and an average cost of 52.8k tokens.'],\n",
       " 'average context token': ['The average context tokens refer to the average token number of the original dataset.'],\n",
       " 'average cost token': ['The average cost tokens comprise both input tokens and output tokens during exploration.'],\n",
       " 'input token': ['The average cost tokens comprise both input tokens and output tokens during exploration.'],\n",
       " 'output token': ['The average cost tokens comprise both input tokens and output tokens during exploration.'],\n",
       " 'recall evalu': ['The results of the recall evaluation are displayed in Figure 5.'],\n",
       " 'recall r': ['Tabel 6 shows the recall rates discussed.',\n",
       "  'The recall rate of GraphReader is calculated at different granularities.',\n",
       "  'Recall rates are presented in Table 6 for different granularities.',\n",
       "  'The recall rate of supporting facts is evaluated for different methods using GPT-4-128k with a temperature of 0.1.',\n",
       "  'The granularity of supporting facts refers to the recall rate of all supporting facts across the entire dataset.'],\n",
       " 'granular': ['The recall rate of GraphReader is calculated at different granularities.',\n",
       "  'Recall rates are presented in Table 6 for different granularities.',\n",
       "  '\"Sample-wise\" refers to the granularity of sample evaluation.',\n",
       "  '\"SF-wise\" refers to the granularity of supporting facts.',\n",
       "  'A sample is considered to be recalled only if all of its supporting facts are recalled at sample granularity.',\n",
       "  'Recall performance at sample-wise granularity is 64.7% for atomic facts and 85.3% for the final notebook.'],\n",
       " 'sf wise': ['\"SF-wise\" refers to the granularity of supporting facts.'],\n",
       " 'sample wis': ['\"Sample-wise\" refers to the granularity of sample evaluation.'],\n",
       " 'recall perform': ['Recall performance at sample-wise granularity is 64.7% for atomic facts and 85.3% for the final notebook.',\n",
       "  'As context length increases from 16k to 256k, recall performance is analyzed.',\n",
       "  'GraphReader shows a recall performance of 76.4% for atomic facts and 90.5% for the final notebook at SF-wise granularity.'],\n",
       " 'final notebook': ['For GraphReader, the memory is recorded in the final notebook.',\n",
       "  'Recall performance at sample-wise granularity is 64.7% for atomic facts and 85.3% for the final notebook.',\n",
       "  'The recall for the final notebook is slightly higher than the recall of atomic facts.',\n",
       "  'GraphReader shows a recall performance of 76.4% for atomic facts and 90.5% for the final notebook at SF-wise granularity.'],\n",
       " 'sf wise granular': ['GraphReader shows a recall performance of 76.4% for atomic facts and 90.5% for the final notebook at SF-wise granularity.'],\n",
       " 'token count': ['The original dataset’s token count is significantly reduced.'],\n",
       " 'significantly reduc': ['The original dataset’s token count is significantly reduced.'],\n",
       " 'declin': ['The call of supporting facts declines across all methods.'],\n",
       " 'all method': ['The call of supporting facts declines across all methods.'],\n",
       " 'scalabl': ['GraphReader demonstrates scalability and effectiveness in processing long contexts.'],\n",
       " 'evaluation prompt': ['Further details and evaluation prompts can be found in Appendix E.',\n",
       "  'Figure 19 displays the specific evaluation prompt.'],\n",
       " 'calcul': ['The detailed calculations are presented in Appendix E.',\n",
       "  'The recall rate of GraphReader is calculated at different granularities.'],\n",
       " 'granularity of supporting fact': ['The granularity of supporting facts refers to the recall rate of all supporting facts across the entire dataset.'],\n",
       " 'refers to': ['The granularity of supporting facts refers to the recall rate of all supporting facts across the entire dataset.'],\n",
       " 'higher': ['The recall for the final notebook is slightly higher than the recall of atomic facts.'],\n",
       " 'intellig': [\"The findings indirectly reflect GraphReader's intelligence and effectiveness in exploration.\"],\n",
       " 'present': ['The detailed calculations are presented in Appendix E.'],\n",
       " 'tabel 6': ['Tabel 6 shows the recall rates discussed.'],\n",
       " 'show': ['Tabel 6 shows the recall rates discussed.'],\n",
       " 'discuss': ['Tabel 6 shows the recall rates discussed.'],\n",
       " 'autonomous ag': ['GraphReader organizes long texts into graph structures and employs an autonomous agent to explore the graph.'],\n",
       " 'question answering benchmark': ['Experiments demonstrate that GraphReader outperforms GPT-4 with a 128k input length across various long-context single-hop and multi-hop question-answering benchmarks.'],\n",
       " 'close sourc': ['GraphReader is close-sourced, which may lead to potential restrictions such as limits on Queries Per Second (QPS) and regional constraints.'],\n",
       " 'limits on queries per second  qps ': ['GraphReader is close-sourced, which may lead to potential restrictions such as limits on Queries Per Second (QPS) and regional constraints.'],\n",
       " 'regional constraint': ['GraphReader is close-sourced, which may lead to potential restrictions such as limits on Queries Per Second (QPS) and regional constraints.'],\n",
       " 'future work': ['Future work will involve collecting data, training models, and making them open-source to contribute to the wider community.'],\n",
       " 'gathering data': [\"After gathering the data, comparison of Danny's and Alice's careers can be made.\",\n",
       "  'Future work will involve collecting data, training models, and making them open-source to contribute to the wider community.'],\n",
       " 'training model': ['Future work will involve collecting data, training models, and making them open-source to contribute to the wider community.'],\n",
       " 'open sourc': ['Future work will involve collecting data, training models, and making them open-source to contribute to the wider community.'],\n",
       " 'wider commun': ['Future work will involve collecting data, training models, and making them open-source to contribute to the wider community.'],\n",
       " 'efficiency of the ag': ['The efficiency of the agent depends on its planning and reasoning capabilities.'],\n",
       " 'planning cap': ['The efficiency of the agent depends on its planning and reasoning capabilities.'],\n",
       " 'reasoning cap': ['The efficiency of the agent depends on its planning and reasoning capabilities.'],\n",
       " 'enhanc': ['Future research will explore enhancements of planning and reasoning features to improve the effectiveness of the method.'],\n",
       " 'planning featur': ['Future research will explore enhancements of planning and reasoning features to improve the effectiveness of the method.'],\n",
       " 'improve effectiveness of the method': ['Future research will explore enhancements of planning and reasoning features to improve the effectiveness of the method.'],\n",
       " 'ge bai': ['Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"'],\n",
       " 'xingyuan bu': ['Yanan Wu, Jie Liu, Xingyuan Bu, and Jiaheng Liu are also relevant authors mentioned in the context of these works, though specific details about their contributions are not provided.',\n",
       "  'Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"'],\n",
       " 'zhanhui zh': ['Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"'],\n",
       " 'zhuoran lin': ['Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"'],\n",
       " 'wenbo su': ['Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"'],\n",
       " 'tiezheng g': ['Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"',\n",
       "  'Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.'],\n",
       " 'bo zheng': ['Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"'],\n",
       " 'mt bench 101': ['Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"'],\n",
       " 'multi turn dialogu': ['Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"'],\n",
       " 'arxiv': ['The paper \"Conceptmath\" is available as a preprint on arXiv with the identifier arXiv:2402.14660.',\n",
       "  'Albert Gu and Tri Dao introduced \"Mamba: Linear-time sequence modeling with selective state spaces\" in 2023.',\n",
       "  'The paper was published as an arXiv preprint (arXiv:2307.07697).',\n",
       "  'Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and others authored the paper titled \"Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues.\"',\n",
       "  'The \"Yarn\" paper is available as an arXiv preprint under the identifier arXiv:2309.00071.',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"',\n",
       "  'The study by Wenhao Wu and colleagues is available as arXiv preprint arXiv:2405.03939.',\n",
       "  'The paper \"React\" is available as a preprint on arXiv with the identifier arXiv:2210.03629.',\n",
       "  'The \"Yara parser\" paper is available as a Computing Research Repository entry under the identifier arXiv:1503.06733.',\n",
       "  'The \"Webgpt\" paper is available as an arXiv preprint under the identifier arXiv:2112.09332.',\n",
       "  'The publication is titled \"Raptor: Recursive abstractive processing for tree-organized retrieval,\" which is available as an arXiv preprint (arXiv:2401.18059).',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.',\n",
       "  'Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis authored the paper titled \"Generalization through memorization: Nearest neighbor language models\" in 2019.',\n",
       "  'Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.',\n",
       "  'This inquiry is presented in an arXiv paper.',\n",
       "  'Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.',\n",
       "  'Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer authored the paper titled \"A human-inspired reading agent with gist memory of very long contexts\" in 2024.'],\n",
       " 'yushi bai': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'xin lv': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'yuze h': ['Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'ji qi': ['Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'lei hou': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'jie tang': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'yuxiao dong': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'juanzi li': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'longalign': ['Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'long context align': ['Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li authored the paper titled \"Longalign: A recipe for long context alignment of large language models.\"'],\n",
       " 'hong lyu': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"'],\n",
       " 'jiankai tang': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"'],\n",
       " 'zhidian huang': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"'],\n",
       " 'zhengxiao du': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"'],\n",
       " 'ao han zeng': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"'],\n",
       " 'bilingu': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"',\n",
       "  'Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.'],\n",
       " 'multitask benchmark': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"'],\n",
       " 'abs 2308 14508': ['Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-han Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li authored the paper titled \"Longbench: A bilingual, multitask benchmark for long context understanding.\"'],\n",
       " 'howard chen': ['Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz authored the paper titled \"Walking down the memory maze: Beyond context limit through interactive methods.\"'],\n",
       " 'ramakanth pasunuru': ['Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz authored the paper titled \"Walking down the memory maze: Beyond context limit through interactive methods.\"'],\n",
       " 'jason weston': ['Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz authored the paper titled \"Walking down the memory maze: Beyond context limit through interactive methods.\"'],\n",
       " 'asli celikyilmaz': ['Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz authored the paper titled \"Walking down the memory maze: Beyond context limit through interactive methods.\"'],\n",
       " 'walking down the memory maz': ['Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz authored the paper titled \"Walking down the memory maze: Beyond context limit through interactive methods.\"'],\n",
       " 'interactive method': ['Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz authored the paper titled \"Walking down the memory maze: Beyond context limit through interactive methods.\"'],\n",
       " 'sherman wong': ['Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian published a paper in 2023 titled \"Extending context window of large language models via positional interpolation\".'],\n",
       " 'yuandong tian': ['Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian published a paper in 2023 titled \"Extending context window of large language models via positional interpolation\".'],\n",
       " 'haotian tang': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".'],\n",
       " 'xin lai': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".'],\n",
       " 'song han': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".'],\n",
       " 'longlora': ['Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia published a paper in 2023 titled \"Longlora: Efficient fine-tuning of long-context large language models\".'],\n",
       " 'zihang dai': ['Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".'],\n",
       " 'zhilin yang': ['Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.',\n",
       "  'Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".'],\n",
       " 'yiming yang': ['Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".'],\n",
       " 'jaime carbonel': ['Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".'],\n",
       " 'quoc v l': ['The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.',\n",
       "  'Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".'],\n",
       " 'ruslan salakhutdinov': ['Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".'],\n",
       " 'transformer xl': ['Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov published a paper in 2019 titled \"Transformer-xl: Attentive language models beyond a fixed-length context\".'],\n",
       " 'michiel de jong': ['Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".'],\n",
       " 'yury zemlyanskiy': ['Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".'],\n",
       " 'nicholas fitzgerald': ['Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.',\n",
       "  'Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".'],\n",
       " 'fei sha': ['Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".'],\n",
       " 'william cohen': ['Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".'],\n",
       " 'textual knowledg': ['Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".'],\n",
       " 'transform': ['Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".'],\n",
       " 'entity mention attent': ['Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, and William Cohen published a paper in 2021 titled \"Mention memory: incorporating textual knowledge into transformers through entity mention attention\".'],\n",
       " 'yiran d': ['Yiran Ding, Li Lyna Zhang, Chengruidong Zhang are mentioned in the provided text without a corresponding publication title or year.'],\n",
       " 'li lyna zhang': ['Yiran Ding, Li Lyna Zhang, Chengruidong Zhang are mentioned in the provided text without a corresponding publication title or year.'],\n",
       " 'ning shang': ['Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.'],\n",
       " 'fan yang': ['Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.'],\n",
       " 'mao yang': ['Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.'],\n",
       " 'longrop': ['Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang presented a work titled \"Longrope: Extending llm context window beyond 2 million tokens\" in 2024.'],\n",
       " 'thibault f vri': ['Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.'],\n",
       " 'livio baldini soar': ['Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.'],\n",
       " 'eunsol choi': ['Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.'],\n",
       " 'tom kwiatkowski': ['Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.'],\n",
       " 'sparse memory access': ['Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.'],\n",
       " 'entity supervis': ['Thibault Févry, Livio Baldini Soares, Nicholas Fitzgerald, Eunsol Choi, and Tom Kwiatkowski published a paper titled \"Entities as experts: Sparse memory access with entity supervision\" in 2020.'],\n",
       " 'yao fu': ['The study by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li in 2024a addressed \"Long context alignment with short instructions and synthesized positions.\"',\n",
       "  'Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'rameswar panda': ['Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'xinyao niu': ['Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'xiang yu': ['The study by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li in 2024a addressed \"Long context alignment with short instructions and synthesized positions.\"',\n",
       "  'Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'hannaneh hajishirzi': ['Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'yoon kim': ['Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'hao peng': ['Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'data engin': ['Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng authored a study titled \"Data engineering for scaling language models to 128k context\" in 2024.'],\n",
       " 'albert gu': ['Albert Gu and Tri Dao introduced \"Mamba: Linear-time sequence modeling with selective state spaces\" in 2023.'],\n",
       " 'tri dao': ['Albert Gu and Tri Dao introduced \"Mamba: Linear-time sequence modeling with selective state spaces\" in 2023.'],\n",
       " 'mamba': ['Albert Gu and Tri Dao introduced \"Mamba: Linear-time sequence modeling with selective state spaces\" in 2023.'],\n",
       " 'linear time sequence model': ['Albert Gu and Tri Dao introduced \"Mamba: Linear-time sequence modeling with selective state spaces\" in 2023.'],\n",
       " 'selective state spac': ['Albert Gu and Tri Dao introduced \"Mamba: Linear-time sequence modeling with selective state spaces\" in 2023.'],\n",
       " 'xanh ho': ['Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.'],\n",
       " 'anh khoa duong nguyen': ['Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.'],\n",
       " 'saku sugawara': ['Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.'],\n",
       " 'akiko aizawa': ['Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.'],\n",
       " 'cole': ['Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.'],\n",
       " 'pages 1 22': ['The paper \"Hotpotqa\" was presented at the EMNLP conference and includes pages 2369 to 2380.',\n",
       "  'Omar Khattab and Matei Zaharia authored the paper titled \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\" in 2020.',\n",
       "  'The \"Generative agents\" paper covers pages 1–22.',\n",
       "  'Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.',\n",
       "  'The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.',\n",
       "  'This research was published in the Advances in Neural Information Processing Systems, volume 35, pages 24824–24837.',\n",
       "  'The journal for this publication is \"Transactions of the Association for Computational Linguistics,\" with pages 600–616.',\n",
       "  'The paper was published in the Transactions of the Association for Computational Linguistics, volume 10, pages 539–554.',\n",
       "  'The journal referenced is \"Found. Trends Inf. Retr.\" and the pages cited are 333–389.',\n",
       "  '\"Lost in the middle\" was published in the Transactions of the Association for Computational Linguistics, volume 12, pages 157–173.'],\n",
       " 'association for computational linguist': ['The paper \"Hotpotqa\" was presented at the EMNLP conference and includes pages 2369 to 2380.',\n",
       "  'Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa constructed a multi-hop QA dataset for comprehensive evaluation of reasoning steps in 2020 at COLING, pages 6609–6625.',\n",
       "  'The journal for this publication is \"Transactions of the Association for Computational Linguistics,\" with pages 600–616.',\n",
       "  'The paper was published in the Transactions of the Association for Computational Linguistics, volume 10, pages 539–554.',\n",
       "  '\"Lost in the middle\" was published in the Transactions of the Association for Computational Linguistics, volume 12, pages 157–173.'],\n",
       " 'urvashi khandelw': ['Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis authored the paper titled \"Generalization through memorization: Nearest neighbor language models\" in 2019.'],\n",
       " 'omer levi': ['Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis authored the paper titled \"Generalization through memorization: Nearest neighbor language models\" in 2019.'],\n",
       " 'dan jurafski': ['Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis authored the paper titled \"Generalization through memorization: Nearest neighbor language models\" in 2019.'],\n",
       " 'luke zettlemoy': ['Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis authored the paper titled \"Generalization through memorization: Nearest neighbor language models\" in 2019.',\n",
       "  'Authored by Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.'],\n",
       " 'mike lewi': ['Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis authored the paper titled \"Generalization through memorization: Nearest neighbor language models\" in 2019.',\n",
       "  'Authored by Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.'],\n",
       " 'generalization through memorization  nearest neighbor language model': ['Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis authored the paper titled \"Generalization through memorization: Nearest neighbor language models\" in 2019.'],\n",
       " 'colbert  efficient and effective passage search via contextualized late interaction over bert': ['Omar Khattab and Matei Zaharia authored the paper titled \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\" in 2020.'],\n",
       " 'proceedings of the 43rd international acm sigir conference on research and development in information retriev': ['Omar Khattab and Matei Zaharia authored the paper titled \"Colbert: Efficient and effective passage search via contextualized late interaction over bert\" in 2020.'],\n",
       " 'tom s kocisk ': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'jonathan schwarz': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'phil blunsom': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'chris dyer': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'karl moritz hermann': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'g bor m': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'edward grefenstett': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'the narrativeqa reading comprehension challeng': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'trans  assoc  comput  linguist': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " '6 317 328': ['Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette authored \"The narrativeqa reading comprehension challenge\" in 2018.'],\n",
       " 'kuang huei le': ['Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer authored the paper titled \"A human-inspired reading agent with gist memory of very long contexts\" in 2024.'],\n",
       " 'hiroki furuta': ['Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer authored the paper titled \"A human-inspired reading agent with gist memory of very long contexts\" in 2024.'],\n",
       " 'john canni': ['Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer authored the paper titled \"A human-inspired reading agent with gist memory of very long contexts\" in 2024.'],\n",
       " 'ian fisch': ['Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer authored the paper titled \"A human-inspired reading agent with gist memory of very long contexts\" in 2024.'],\n",
       " 'a human inspired reading agent with gist memory of very long context': ['Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer authored the paper titled \"A human-inspired reading agent with gist memory of very long contexts\" in 2024.'],\n",
       " 'xingxuan li': ['Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing are mentioned but without specific titles or publications in the provided text.'],\n",
       " 'ruochen zhao': ['Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing are mentioned but without specific titles or publications in the provided text.'],\n",
       " 'yew ken chia': ['Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing are mentioned but without specific titles or publications in the provided text.'],\n",
       " 'bosheng d': ['Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing are mentioned but without specific titles or publications in the provided text.'],\n",
       " 'shafiq joti': ['Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing are mentioned but without specific titles or publications in the provided text.'],\n",
       " 'soujanya poria': ['Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing are mentioned but without specific titles or publications in the provided text.'],\n",
       " 'lidong b': ['Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing are mentioned but without specific titles or publications in the provided text.'],\n",
       " 'the twelfth international conference on learning represent': ['The Twelfth International Conference on Learning Representations took place in 2023.'],\n",
       " 'chain of knowledg': ['Jerry Liu published \"Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources\" in 2023.'],\n",
       " 'ground': ['Jerry Liu published \"Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources\" in 2023.'],\n",
       " 'dynamic knowledge adapt': ['Jerry Liu published \"Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources\" in 2023.'],\n",
       " 'heterogeneous sourc': ['Jerry Liu published \"Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources\" in 2023.'],\n",
       " 'kevin lin': ['Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.'],\n",
       " 'john hewitt': ['Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.'],\n",
       " 'ashwin paranjap': ['Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.'],\n",
       " 'michele bevilacqua': ['Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.'],\n",
       " 'fabio petroni': ['Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.'],\n",
       " 'percy liang': ['Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang authored \"Lost in the middle: How language models use long contexts\" in 2024.',\n",
       "  'Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'volume 12': ['\"Lost in the middle\" was published in the Transactions of the Association for Computational Linguistics, volume 12, pages 157–173.',\n",
       "  'The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.',\n",
       "  'This research was published in the Advances in Neural Information Processing Systems, volume 35, pages 24824–24837.',\n",
       "  'The paper was published in the Transactions of the Association for Computational Linguistics, volume 10, pages 539–554.'],\n",
       " 'keming lu': ['Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.'],\n",
       " 'runji lin': ['Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.'],\n",
       " 'junyang lin': ['Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.'],\n",
       " 'chuanqi tan': ['Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.'],\n",
       " 'hui zhou': ['Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.',\n",
       "  'The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.',\n",
       "  'Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.',\n",
       "  'Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.'],\n",
       " 'jingren zh': ['Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.'],\n",
       " ' instag': ['Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.'],\n",
       " 'instruction tag': ['Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou presented \"#instag: Instruction tagging for analyzing supervised fine-tuning of large language models\" in 2023.'],\n",
       " 'linhao luo': ['Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.'],\n",
       " 'yuan fang li': ['Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.'],\n",
       " 'gholamreza haffari': ['Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.'],\n",
       " 'shirui pan': ['Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.'],\n",
       " 'faith': ['Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.'],\n",
       " 'interpret': ['Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan explored \"Reasoning on graphs: Faithful and interpretable large language model reasoning\" in 2023.'],\n",
       " 'tsendsuren munkhdalai': ['Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal published \"Leave no context behind: Efficient infinite context transformers with infinite\" in 2024.'],\n",
       " 'manaal faruqui': ['Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal published \"Leave no context behind: Efficient infinite context transformers with infinite\" in 2024.'],\n",
       " 'siddharth gop': ['Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal published \"Leave no context behind: Efficient infinite context transformers with infinite\" in 2024.'],\n",
       " 'leave no context behind': ['Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal published \"Leave no context behind: Efficient infinite context transformers with infinite\" in 2024.'],\n",
       " 'infinite context transform': ['Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal published \"Leave no context behind: Efficient infinite context transformers with infinite\" in 2024.'],\n",
       " 'reiichiro nakano': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'jacob hilton': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'suchir balaji': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'jeff wu': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'long ouyang': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'christina kim': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'christopher hess': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'shantanu jain': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'vineet kosaraju': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'william saund': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'browser assisted question answ': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'human feedback': ['Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and others authored a paper titled \"Webgpt: Browser-assisted question-answering with human feedback\" in 2021.'],\n",
       " 'joon sung park': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'joseph o brien': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'carrie jun cai': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'meredith ringel morri': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'michael s bernstein': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'generative ag': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.',\n",
       "  'The \"Generative agents\" paper covers pages 1–22.'],\n",
       " 'interactive simulacra': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'human behavior': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'proceedings of the 36th annual acm symposium on user interface software and technolog': ['Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein published \"Generative agents: Interactive simulacra of human behavior\" in the Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology.'],\n",
       " 'bowen peng': ['Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole authored \"Yarn: Efficient context window extension of large language models\" in 2023.'],\n",
       " 'jeffrey quesnel': ['Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole authored \"Yarn: Efficient context window extension of large language models\" in 2023.'],\n",
       " 'honglu fan': ['Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole authored \"Yarn: Efficient context window extension of large language models\" in 2023.'],\n",
       " 'enrico shippol': ['Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole authored \"Yarn: Efficient context window extension of large language models\" in 2023.'],\n",
       " 'yarn': ['Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole authored \"Yarn: Efficient context window extension of large language models\" in 2023.',\n",
       "  'The \"Yarn\" paper is available as an arXiv preprint under the identifier arXiv:2309.00071.'],\n",
       " 'mohammad sadegh rasooli': ['Mohammad Sadegh Rasooli and Joel R. Tetreault published \"Yara parser: A fast and accurate dependency parser\" in 2015.'],\n",
       " 'yara pars': ['Mohammad Sadegh Rasooli and Joel R. Tetreault published \"Yara parser: A fast and accurate dependency parser\" in 2015.',\n",
       "  'The \"Yara parser\" paper is available as a Computing Research Repository entry under the identifier arXiv:1503.06733.'],\n",
       " 'fast': ['Mohammad Sadegh Rasooli and Joel R. Tetreault published \"Yara parser: A fast and accurate dependency parser\" in 2015.'],\n",
       " 'accur': ['Mohammad Sadegh Rasooli and Joel R. Tetreault published \"Yara parser: A fast and accurate dependency parser\" in 2015.'],\n",
       " 'dependency pars': ['Mohammad Sadegh Rasooli and Joel R. Tetreault published \"Yara parser: A fast and accurate dependency parser\" in 2015.'],\n",
       " 'computing research repositori': ['The \"Yara parser\" paper is available as a Computing Research Repository entry under the identifier arXiv:1503.06733.'],\n",
       " 'stephen e  robertson': ['Stephen E. Robertson and Hugo Zaragoza authored a paper in 2009.'],\n",
       " 'hugo zaragoza': ['Stephen E. Robertson and Hugo Zaragoza authored a paper in 2009.'],\n",
       " '2009': ['Stephen E. Robertson and Hugo Zaragoza authored a paper in 2009.'],\n",
       " 'probabilistic relevance framework': ['The paper discusses the \"probabilistic relevance framework,\" specifically focusing on \"BM25 and beyond.\"'],\n",
       " 'beyond': ['The paper discusses the \"probabilistic relevance framework,\" specifically focusing on \"BM25 and beyond.\"'],\n",
       " 'found  trends inf  retr ': ['The journal referenced is \"Found. Trends Inf. Retr.\" and the pages cited are 333–389.'],\n",
       " 'devendra singh sachan': ['Authored by Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.'],\n",
       " 'dani yogatama': ['Authored by Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.'],\n",
       " 'joelle pineau': ['Authored by Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.'],\n",
       " 'manzil zah': ['Authored by Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.'],\n",
       " 'dense passage retriev': ['The year of publication is 2023, and it discusses how \"questions are all you need to train a dense passage retriever.\"'],\n",
       " 'parth sarthi': ['Authored by Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.'],\n",
       " 'salman abdullah': ['Authored by Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.'],\n",
       " 'aditi tuli': ['Authored by Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.'],\n",
       " 'shubh khanna': ['Authored by Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.'],\n",
       " 'anna goldi': ['Authored by Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.'],\n",
       " 'christopher d man': ['Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.',\n",
       "  'Authored by Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.'],\n",
       " 'raptor': ['The publication is titled \"Raptor: Recursive abstractive processing for tree-organized retrieval,\" which is available as an arXiv preprint (arXiv:2401.18059).'],\n",
       " 'recursive abstractive process': ['The publication is titled \"Raptor: Recursive abstractive processing for tree-organized retrieval,\" which is available as an arXiv preprint (arXiv:2401.18059).'],\n",
       " 'tree organized retriev': ['The publication is titled \"Raptor: Recursive abstractive processing for tree-organized retrieval,\" which is available as an arXiv preprint (arXiv:2401.18059).'],\n",
       " 'jiashuo sun': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'chengjin xu': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'lumingyuan tang': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'chen lin': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'yeyun gong': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'heung yeung shum': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'jian guo': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'deep reason': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'knowledge graph': ['Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo authored another publication in 2023, titled \"Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.\"'],\n",
       " 'simeng sun': ['Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer authored a paper in 2021, questioning whether \"long-range language models actually use long-range context.\"'],\n",
       " 'kalpesh krishna': ['Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer authored a paper in 2021, questioning whether \"long-range language models actually use long-range context.\"'],\n",
       " 'andrew mattarella mick': ['Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer authored a paper in 2021, questioning whether \"long-range language models actually use long-range context.\"'],\n",
       " 'mohit iyy': ['Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer authored a paper in 2021, questioning whether \"long-range language models actually use long-range context.\"'],\n",
       " 'paper': ['The paper titled \"Musique: Multi-hop questions via single-hop question composition\" was authored by Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal in 2022.'],\n",
       " 'harsh trivedi': ['The paper titled \"Musique: Multi-hop questions via single-hop question composition\" was authored by Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal in 2022.'],\n",
       " 'niranjan balasubramanian': ['The paper titled \"Musique: Multi-hop questions via single-hop question composition\" was authored by Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal in 2022.'],\n",
       " 'tushar khot': ['The paper titled \"Musique: Multi-hop questions via single-hop question composition\" was authored by Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal in 2022.'],\n",
       " 'ashish sabharw': ['The paper titled \"Musique: Multi-hop questions via single-hop question composition\" was authored by Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal in 2022.'],\n",
       " 'nedim lipka': ['The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.'],\n",
       " 'ryan a rossi': ['The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.'],\n",
       " 'alexa siu': ['The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.'],\n",
       " 'tyler derr': ['The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.'],\n",
       " 'knowledge graph prompting for multi document question answ': ['The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.'],\n",
       " 'aaai conference on artificial intellig': ['The work by Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, titled \"Knowledge graph prompting for multi-document question answering,\" was presented in 2024 at the AAAI Conference on Artificial Intelligence, volume 38, pages 19206–19214.'],\n",
       " 'jason wei': ['The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.'],\n",
       " 'dale schuurman': ['The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.'],\n",
       " 'maarten bosma': ['The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.'],\n",
       " 'fei xia': ['The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.'],\n",
       " 'ed chi': ['The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.'],\n",
       " 'denny zh': ['The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.'],\n",
       " 'chain of thought prompting elicits reasoning in large language model': ['The research titled \"Chain-of-thought prompting elicits reasoning in large language models\" was conducted by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others in 2022.'],\n",
       " 'advances in neural information processing system': ['This research was published in the Advances in Neural Information Processing Systems, volume 35, pages 24824–24837.'],\n",
       " 'wenhao wu': ['The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.',\n",
       "  'The study by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li in 2024a addressed \"Long context alignment with short instructions and synthesized positions.\"'],\n",
       " 'dawei zhu': ['The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.',\n",
       "  'The study by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li in 2024a addressed \"Long context alignment with short instructions and synthesized positions.\"'],\n",
       " 'sujian li': ['The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.',\n",
       "  'The study by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li in 2024a addressed \"Long context alignment with short instructions and synthesized positions.\"'],\n",
       " 'long context alignment with short instructions and synthesized posit': ['The study by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li in 2024a addressed \"Long context alignment with short instructions and synthesized positions.\"'],\n",
       " 'yanan wu': ['Yanan Wu, Jie Liu, Xingyuan Bu, and Jiaheng Liu are also relevant authors mentioned in the context of these works, though specific details about their contributions are not provided.'],\n",
       " 'zhiqi bai': ['Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.'],\n",
       " 'haibin chen': ['Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.'],\n",
       " 'conceptmath': ['The paper \"Conceptmath\" is available as a preprint on arXiv with the identifier arXiv:2402.14660.',\n",
       "  'Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.'],\n",
       " 'mathematical reason': ['Hui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. published a paper titled \"Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models\" in 2024.'],\n",
       " 'yoshua bengio': ['Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.'],\n",
       " 'divers': ['Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.'],\n",
       " 'explain': ['Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning authored the paper \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\" in 2018.'],\n",
       " 'emnlp': ['The paper \"Hotpotqa\" was presented at the EMNLP conference and includes pages 2369 to 2380.'],\n",
       " 'zhuyu yao': ['Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.',\n",
       "  'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.'],\n",
       " 'jeffrey zhao': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.',\n",
       "  'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.'],\n",
       " 'dian yu': ['Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.'],\n",
       " 'nan du': ['Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.'],\n",
       " 'izhak shafran': ['Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.'],\n",
       " 'karthik narasimhan': ['Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.'],\n",
       " 'yuan cao': ['Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.'],\n",
       " 'react': ['The paper \"React\" is available as a preprint on arXiv with the identifier arXiv:2210.03629.',\n",
       "  'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.'],\n",
       " 'act': ['Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao published a paper titled \"React: Synergizing reasoning and acting in language models\" in 2022.',\n",
       "  'Each action has a certain probability of being chosen.'],\n",
       " 'xuefei n': ['Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.'],\n",
       " 'shiyao li': ['Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.'],\n",
       " 'minghui zhuang': ['Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.'],\n",
       " 'zheyue tan': ['Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.'],\n",
       " 'dahua lin': ['Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.'],\n",
       " 'boxun li': ['Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.'],\n",
       " 'guohao dai': ['Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.'],\n",
       " 'balanc': ['The title of the paper \"Lv-eval\" suggests a focus on evaluation in a balanced manner.',\n",
       "  'Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang authored a paper titled \"Lv-eval: A balanced long-...\" in 2024.'],\n",
       " 'five length level': ['The context benchmark consists of five length levels up to 256k.'],\n",
       " 'junyi li': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'tianyi tang': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'yupeng h': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'yingqian min': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'zican dong': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'yifan du': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'chen yang': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'jinhao jiang': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'ruiyang ren': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'xinyu tang': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'jian yun ni': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'ji rong wen': ['The paper titled \"A survey of large language models\" was authored by Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen, and published in 2023.'],\n",
       " 'pose  efficient context window extension of llms via positional skip wise train': ['The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.'],\n",
       " 'nan yang': ['The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.'],\n",
       " 'yifan song': ['The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.'],\n",
       " 'furu wei': ['The paper titled \"Pose: Efficient context window extension of llms via positional skip-wise training\" was authored by Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li, and published in 2023.'],\n",
       " 'prompt': ['Figure 6 illustrates the prompt used for Graph Construction.',\n",
       "  'Figures 7 to 11 present the prompts employed for Graph Exploration.',\n",
       "  'Figure 16 and Figure 17 contain specific prompts.',\n",
       "  'The prompts used for evaluation are presented in Figure 13 and Figure 14 respectively.',\n",
       "  'The prompt includes a figure related to Full Text Read and Chunk Read instructions.',\n",
       "  'Figure 12 shows the prompt used for Answer Reasoning.',\n",
       "  'The specific prompt can be found in Figure 18.'],\n",
       " 'golden answ': ['The B LLM Rater evaluates given a question, a golden answer, and an answer to be evaluated using an LLM.'],\n",
       " 'strict scoring criterion': ['LLM-Rating-2 (LR-2) is a more lenient scoring criterion.',\n",
       "  'LLM-Rating-1 (LR-1) represents a strict scoring criterion.'],\n",
       " 'strict scor': ['If the strict scorer finds an answer incorrect while the lenient scorer deems it partially correct, the answer is classified as partially correct.'],\n",
       " 'incorrect': ['If the strict scorer finds an answer incorrect while the lenient scorer deems it partially correct, the answer is classified as partially correct.',\n",
       "  'If neither scorer finds the answer correct, it is adjudged incorrect.'],\n",
       " 'lenient scor': ['If the strict scorer finds an answer incorrect while the lenient scorer deems it partially correct, the answer is classified as partially correct.'],\n",
       " 'partially correct': ['If the strict scorer finds an answer incorrect while the lenient scorer deems it partially correct, the answer is classified as partially correct.'],\n",
       " 'native speak': ['The Multi-hop QA Datasets include HotpotQA, which features a collection of 2-hop questions directly authored by native speakers.'],\n",
       " '5 hop': ['2WikiMultihopQA is comprised of complex questions up to 5-hops in length.'],\n",
       " 'templat': ['Questions in 2WikiMultihopQA are constructed through carefully designed templates to prevent shortcut solutions.'],\n",
       " 'shortcut solut': ['Questions in 2WikiMultihopQA are constructed through carefully designed templates to prevent shortcut solutions.'],\n",
       " 'musique dataset': ['In the MuSiQue dataset, questions are intricately crafted starting from straightforward scenarios.'],\n",
       " 'straightforward scenario': ['In the MuSiQue dataset, questions are intricately crafted starting from straightforward scenarios.'],\n",
       " 'annot': ['Annotators rephrase questions to avoid shortcut answers and maintain natural linguistic quality.'],\n",
       " 'shortcut answ': ['Annotators rephrase questions to avoid shortcut answers and maintain natural linguistic quality.'],\n",
       " 'natural linguistic qu': ['Annotators rephrase questions to avoid shortcut answers and maintain natural linguistic quality.'],\n",
       " '2 4 paragraph': ['Each question in the original datasets is supported by 2-4 paragraphs providing evidence for simple reasoning and additional decoy paragraphs.'],\n",
       " 'evid': ['Each question in the original datasets is supported by 2-4 paragraphs providing evidence for simple reasoning and additional decoy paragraphs.'],\n",
       " 'decoy paragraph': ['Each question in the original datasets is supported by 2-4 paragraphs providing evidence for simple reasoning and additional decoy paragraphs.'],\n",
       " 'mixup method': ['The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.',\n",
       "  'HotpotWikiQA-mixup is derived from LV-Eval and uses a mixup construction method.'],\n",
       " 'support docu': ['The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.'],\n",
       " 'distracting docu': ['The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.'],\n",
       " 'qa pair': ['The mixup method randomly blends support documents with distracting documents to create five different context lengths for a QA pair: 16k, 32k, 64k, 128k, and 256k.'],\n",
       " 'first 50 entri': ['The excessive length of the dataset leads to selection of the first 50 entries from each context length for experimentation to control costs.'],\n",
       " 'experiment': ['The excessive length of the dataset leads to selection of the first 50 entries from each context length for experimentation to control costs.'],\n",
       " 'control cost': ['The excessive length of the dataset leads to selection of the first 50 entries from each context length for experimentation to control costs.'],\n",
       " 'comprehension ': ['NarrativeQA is a single-hop QA dataset that tests comprehension abilities for long documents sourced from movie scripts.'],\n",
       " 'long docu': ['NarrativeQA is a single-hop QA dataset that tests comprehension abilities for long documents sourced from movie scripts.'],\n",
       " 'movie script': ['A possible explanation for the maximum average is that NarrativeQA is mainly derived from movie scripts.',\n",
       "  'NarrativeQA is a single-hop QA dataset that tests comprehension abilities for long documents sourced from movie scripts.'],\n",
       " 'single loc': ['In a single-hop QA dataset, the information needed to answer questions appears at a single location within the text.'],\n",
       " 'truncat': ['Truncating texts results in information loss.',\n",
       "  'Lee et al. truncated the text to fit it into the LLM when it exceeded token limits.'],\n",
       " 'token limit': ['Lee et al. truncated the text to fit it into the LLM when it exceeded token limits.'],\n",
       " 'information loss': ['Truncating texts results in information loss.',\n",
       "  'A new method is proposed that does not lose information and allows for better comparison.'],\n",
       " 'same chunking method': ['The same chunking method as GraphReader is used in the proposed method.'],\n",
       " 'sequenti': ['The LLM reads these chunks sequentially according to the text order.'],\n",
       " 'text ord': ['The LLM reads these chunks sequentially according to the text order.'],\n",
       " 'current chunk': ['In the Chunk Read approach, the LLM only sees the current chunk during each reading.',\n",
       "  'The current chunk is suitable for multi-hop QA tasks.'],\n",
       " '2k': ['When opting for the top-3 chunks, the maximum length of each chunk is set to 1k.',\n",
       "  'The maximum length of the chunk is set to 2k.'],\n",
       " 'work': ['The work compares traditional RAG methods.'],\n",
       " 'relevance scor': ['The top-n chunks with the highest relevance scores are input for the LLM to answer.',\n",
       "  'Relevance scores are calculated between the question and the chunks.'],\n",
       " 'segments long text': ['ReadAgent is mentioned as a method that segments long texts and generates gist memories.'],\n",
       " 'default hyperparamet': ['The default hyperparameters declared in the ReadAgent paper include a max_words of 600 and min_words of 280 when splitting pages.'],\n",
       " 'readagent pap': ['The default hyperparameters declared in the ReadAgent paper include a max_words of 600 and min_words of 280 when splitting pages.',\n",
       "  'For HotpotWikiQA-mixup from LV-Eval, two hyperparameters are scaled using the same approach as in the ReadAgent paper.'],\n",
       " 'max word': ['For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.',\n",
       "  'For datasets with lengths of 256k and 128k, the max_words is 10000 and min_words is 2000.',\n",
       "  'The default hyperparameters declared in the ReadAgent paper include a max_words of 600 and min_words of 280 when splitting pages.'],\n",
       " '600': ['The default hyperparameters declared in the ReadAgent paper include a max_words of 600 and min_words of 280 when splitting pages.'],\n",
       " 'min word': ['For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.',\n",
       "  'For datasets with lengths of 256k and 128k, the max_words is 10000 and min_words is 2000.',\n",
       "  'The default hyperparameters declared in the ReadAgent paper include a max_words of 600 and min_words of 280 when splitting pages.'],\n",
       " '280': ['The default hyperparameters declared in the ReadAgent paper include a max_words of 600 and min_words of 280 when splitting pages.'],\n",
       " 'splitting pag': ['The default hyperparameters declared in the ReadAgent paper include a max_words of 600 and min_words of 280 when splitting pages.'],\n",
       " 'two hyperparamet': ['For HotpotWikiQA-mixup from LV-Eval, two hyperparameters are scaled using the same approach as in the ReadAgent paper.'],\n",
       " 'scale': ['For HotpotWikiQA-mixup from LV-Eval, two hyperparameters are scaled using the same approach as in the ReadAgent paper.'],\n",
       " '1000': ['For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.',\n",
       "  'For datasets with lengths of 256k and 128k, the max_words is 10000 and min_words is 2000.'],\n",
       " '2000': ['For datasets with lengths of 256k and 128k, the max_words is 10000 and min_words is 2000.'],\n",
       " '5000': ['For datasets with lengths of 64k, 32k, and 16k, the max_words is 5000 and min_words is 1000.'],\n",
       " 'sequenc': ['Figure 23 illustrates the sequence of function invocations during the exploration phase.',\n",
       "  'The reading of the pages should be in sequence to be the most effective.'],\n",
       " '5 page': ['Up to 5 pages can be read in the evaluation.'],\n",
       " 'retrieved chunk': ['For the RAG methods, retrieved chunks are assessed.'],\n",
       " 'ration': ['The rationality and utility of agent actions are verified under various circumstances of GraphReader.',\n",
       "  'The rationality of the action set is justified.'],\n",
       " 'util': ['The rationality and utility of agent actions are verified under various circumstances of GraphReader.'],\n",
       " 'stage': ['Statistics on function calls at each stage across two datasets are made to verify the actions of GraphReader.'],\n",
       " 'piece of data': ['Each piece of data performs an average of 3 to 4 actions corresponding to the average number of function calls.'],\n",
       " '3 to 4 act': ['Each piece of data performs an average of 3 to 4 actions corresponding to the average number of function calls.'],\n",
       " 'minimize resource usag': ['The graph was constructed with GraphReader being able to swiftly locate key information while minimizing resource usage.'],\n",
       " 'probabl': ['Each action has a certain probability of being chosen.'],\n",
       " 'chosen': ['Each action has a certain probability of being chosen.'],\n",
       " 'action set': ['The rationality of the action set is justified.'],\n",
       " 'justifi': ['The rationality of the action set is justified.'],\n",
       " 'commonly used act': ['The most commonly used action on multi-hop QA tasks is to read neighbor nodes.'],\n",
       " 'common act': ['The most common action on single-hop QA tasks is to read chunks.'],\n",
       " 'retir': ['The plan specifies the key information required to formulate a comprehensive answer.',\n",
       "  'The information needed includes the start and retirement of both Danny’s and Alice’s careers.',\n",
       "  'Single-hop data sets often require only one atomic fact.'],\n",
       " 'occurring simultan': ['The number of key elements occurring simultaneously in each atomic fact is generally of this magnitude.'],\n",
       " 'magnitud': ['The number of key elements occurring simultaneously in each atomic fact is generally of this magnitude.'],\n",
       " 'aggreg': ['The aggregation of similar nodes caused by normalization results in a slight increase in the number of neighboring nodes.'],\n",
       " 'slight increas': ['The aggregation of similar nodes caused by normalization results in a slight increase in the number of neighboring nodes.'],\n",
       " 'even distribut': ['There is a relatively even distribution of atomic facts in the nodes.'],\n",
       " 'maximum averag': ['The maximum average number of atomic facts is found in NarrativeQA.'],\n",
       " 'possible explan': ['A possible explanation for the maximum average is that NarrativeQA is mainly derived from movie scripts.'],\n",
       " 'charact': ['Characters, such as the protagonist, appear frequently throughout the text of NarrativeQA.'],\n",
       " 'protagonist': ['Characters, such as the protagonist, appear frequently throughout the text of NarrativeQA.'],\n",
       " 'frequent appear': ['Characters, such as the protagonist, appear frequently throughout the text of NarrativeQA.'],\n",
       " 'section': ['This section presents a case study of the GraphReader workflow.'],\n",
       " 'case studi': ['This section presents a case study of the GraphReader workflow.'],\n",
       " 'workflow': ['This section presents a case study of the GraphReader workflow.'],\n",
       " 'posed quest': ['Figure 20 displays the posed question alongside the answer and pertinent supporting passages.'],\n",
       " 'supporting passag': ['Figure 20 displays the posed question alongside the answer and pertinent supporting passages.'],\n",
       " 'construct': ['Figure 21 delineates the methodology for constructing the graph.',\n",
       "  'Casa Loma was constructed from 1911 to 1914 as a residence for financier Sir Henry Pellatt.'],\n",
       " 'pre planned rational path': ['Figure 22 elaborates on the initialization of a pre-planned rational path by GraphReader and the selection of initial nodes.'],\n",
       " 'function invoc': ['Figure 23 illustrates the sequence of function invocations during the exploration phase.'],\n",
       " 'exploration phas': ['Figure 23 illustrates the sequence of function invocations during the exploration phase.'],\n",
       " 'oper': ['Figure 24 showcases how GraphReader operates.'],\n",
       " 'avg ': ['Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) shows data regarding HotpotQA.',\n",
       "  'In Table 8, “avg.” indicates the average number of nodes in each graph.',\n",
       "  'The term “avg. avg.” denotes the average of the average neighbor node counts per graph.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) is applicable to MuSiQue.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) also applies to WikiMultihopQA.'],\n",
       " 'function call stag': ['Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) shows data regarding HotpotQA.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) also applies to WikiMultihopQA.',\n",
       "  'Dataset #Avg. Function Call Stage Stage Ratio(%) Function Call Ratio(%) is applicable to MuSiQue.'],\n",
       " 'exploring chunk': ['In WikiMultihopQA, the Exploring Chunks stage has a ratio of 34.5%.',\n",
       "  'In MuSiQue, the Exploring Chunks stage has a ratio of 31.2%.',\n",
       "  'In HotpotQA, the Exploring Chunks stage has a ratio of 31.9%.'],\n",
       " 'exploring neighbor': ['In HotpotQA, the Exploring Neighbors stage has a ratio of 26.1%.',\n",
       "  'In WikiMultihopQA, the Exploring Neighbors stage has a ratio of 25.1%.'],\n",
       " 'sample dimens': ['The sample dimension for WikiMultihopQA has an average of 515.8 node numbers and a maximum of 1691.0.',\n",
       "  'The sample dimension for HotpotQA has an average of 583.8 node numbers and a maximum of 1945.0.',\n",
       "  'The sample dimension for MusiQue has an average of 1029.4 node numbers and a maximum of 2142.0.'],\n",
       " 'node num': ['The sample dimension for WikiMultihopQA has an average of 515.8 node numbers and a maximum of 1691.0.',\n",
       "  'The sample dimension for HotpotQA has an average of 583.8 node numbers and a maximum of 1945.0.',\n",
       "  'The sample dimension for MusiQue has an average of 1029.4 node numbers and a maximum of 2142.0.'],\n",
       " 'statistical figur': ['The values represent performance metrics including statistical figures like averages and maximums for each model.'],\n",
       " 'average nod': ['The 32k model has an average of 2827.3 nodes and a maximum of 5086.0 nodes.',\n",
       "  'In Table 8, “avg.” indicates the average number of nodes in each graph.',\n",
       "  'The 128k model has an average of 8828.5 nodes and a maximum of 14592.0 nodes.',\n",
       "  'For NarrativeQA, the average nodes is 966.0, while the maximum node count is 3110.0.',\n",
       "  'For HotpotWikiQA-mixup16k, the average nodes is 1741.6, and the maximum node count is 3822.0.',\n",
       "  'The 64k model shows an average of 5054.1 nodes and a maximum of 8918.0 nodes.',\n",
       "  'The 256k model features an average of 14853.3 nodes and a maximum of 24981.0 nodes.'],\n",
       " 'maximum node count': ['The term “avg. avg.” denotes the average of the average neighbor node counts per graph.',\n",
       "  'The term “avg. max” means the average of the maximum neighbor node counts per graph.',\n",
       "  'The average is calculated for neighbor node counts.',\n",
       "  'For NarrativeQA, the average nodes is 966.0, while the maximum node count is 3110.0.',\n",
       "  'For HotpotWikiQA-mixup16k, the average nodes is 1741.6, and the maximum node count is 3822.0.',\n",
       "  'Each graph has associated neighbor node counts.'],\n",
       " '32k model': ['The 128k model has an average of 8828.5 nodes and a maximum of 14592.0 nodes.',\n",
       "  'The 32k model has an average of 2827.3 nodes and a maximum of 5086.0 nodes.',\n",
       "  'The 64k model shows an average of 5054.1 nodes and a maximum of 8918.0 nodes.',\n",
       "  'The 256k model features an average of 14853.3 nodes and a maximum of 24981.0 nodes.'],\n",
       " 'maximum nod': ['The 128k model has an average of 8828.5 nodes and a maximum of 14592.0 nodes.',\n",
       "  'The 256k model features an average of 14853.3 nodes and a maximum of 24981.0 nodes.',\n",
       "  'The 64k model shows an average of 5054.1 nodes and a maximum of 8918.0 nodes.',\n",
       "  'The 32k model has an average of 2827.3 nodes and a maximum of 5086.0 nodes.'],\n",
       " 'largest node count': ['In Table 8, “max” refers to the largest node count across all graphs.'],\n",
       " 'avg  max': ['The term “avg. max” means the average of the maximum neighbor node counts per graph.'],\n",
       " 'intelligent assist': [\"The intelligent assistant's primary objective is to answer questions by gathering supporting facts from articles.\",\n",
       "  \"An intelligent assistant's primary objective is to answer questions based on information within the text.\"],\n",
       " 'object': [\"The intelligent assistant's primary objective is to answer questions by gathering supporting facts from articles.\"],\n",
       " 'articl': [\"The intelligent assistant's primary objective is to answer questions by gathering supporting facts from articles.\"],\n",
       " 'outlin': ['The rational plan should outline a step-by-step process to resolve the question.'],\n",
       " 'formul': ['The plan specifies the key information required to formulate a comprehensive answer.'],\n",
       " 'longer tennis car': ['An example question is: Who had a longer tennis career, Danny or Alice?'],\n",
       " 'danni': [\"To answer the question, it's necessary to find the length of Danny's and Alice's tennis careers.\",\n",
       "  'The information needed includes the start and retirement of both Danny’s and Alice’s careers.',\n",
       "  \"After gathering the data, comparison of Danny's and Alice's careers can be made.\",\n",
       "  'An example question is: Who had a longer tennis career, Danny or Alice?'],\n",
       " 'alic': [\"To answer the question, it's necessary to find the length of Danny's and Alice's tennis careers.\",\n",
       "  'The information needed includes the start and retirement of both Danny’s and Alice’s careers.',\n",
       "  \"After gathering the data, comparison of Danny's and Alice's careers can be made.\",\n",
       "  'An example question is: Who had a longer tennis career, Danny or Alice?'],\n",
       " 'tennis car': [\"To answer the question, it's necessary to find the length of Danny's and Alice's tennis careers.\"],\n",
       " 'information need': ['The information needed includes the start and retirement of both Danny’s and Alice’s careers.'],\n",
       " 'start': ['The information needed includes the start and retirement of both Danny’s and Alice’s careers.'],\n",
       " 'career': [\"After gathering the data, comparison of Danny's and Alice's careers can be made.\",\n",
       "  'The information needed includes the start and retirement of both Danny’s and Alice’s careers.'],\n",
       " 'assist': ['The assistant must assess whether the available information in a text chunk is sufficient to answer a question.',\n",
       "  'The assistant can choose to read a chunk from a list of IDs or stop and read a neighboring chunk.',\n",
       "  'There are action options available for the assistant, including search_more().',\n",
       "  'The primary objective of the assistant is to answer questions based on the information within a text.'],\n",
       " 'list id ': ['The assistant can choose to read a chunk from a list of IDs or stop and read a neighboring chunk.'],\n",
       " 'action opt': ['There are action options available for the assistant, including search_more().',\n",
       "  'The assistant can choose to read a chunk from a list of IDs or stop and read a neighboring chunk.',\n",
       "  'Action options include reading a neighbor node or termination.'],\n",
       " 'relev': ['It is emphasized that even slightly relevant atomic facts should be explored.'],\n",
       " 'irrelev': ['The stop_and_read_neighbor() action is chosen only when the current text chunk is deemed irrelevant.'],\n",
       " 'primary object': ['The primary objective of the assistant is to answer questions based on the information within a text.',\n",
       "  \"An intelligent assistant's primary objective is to answer questions based on information within the text.\"],\n",
       " 'smallest truth': ['Atomic facts are the smallest, indivisible truths extracted from text chunks.',\n",
       "  'Text chunks are segments of the original text, while atomic facts are the smallest truths extracted from those chunks.'],\n",
       " 'noun': ['Nodes represent key elements (nouns, verbs, adjectives) that correspond with various atomic facts.'],\n",
       " 'suffici': ['The assistant must assess whether the available information in a text chunk is sufficient to answer a question.'],\n",
       " 'avail': ['There are action options available for the assistant, including search_more().'],\n",
       " 'lacking clar': ['The question is still lacking clarity.'],\n",
       " 'segment': ['Text chunks are segments of the original text.'],\n",
       " 'indivisible truth': ['If John\\'s response is more specific than the ground truth answer, it is rated as \"Yes\".',\n",
       "  'If John\\'s response contains the ground truth answer it is rated as \"Yes\".',\n",
       "  'If John\\'s response has any overlap with the ground truth answer, it is rated as \"Yes, partially\".',\n",
       "  'The ground truth answer was referenced as a response.',\n",
       "  'The text asks if John’s answer agrees with the ground truth answer.',\n",
       "  'Atomic facts are the smallest, indivisible truths extracted from text chunks.'],\n",
       " 'determin': ['The goal is to determine whether to proceed to the next neighboring node.'],\n",
       " 'proceed': ['The goal is to determine whether to proceed to the next neighboring node.'],\n",
       " 'choos': ['Choose to read a neighboring node if it may contain relevant information to the question.',\n",
       "  'Choose termination if no neighboring nodes possess information to answer the question.'],\n",
       " 'previous act': ['Strategy involves reflecting on previous actions to prevent redundant revisiting of nodes or chunks.'],\n",
       " 'prevent': ['Strategy involves reflecting on previous actions to prevent redundant revisiting of nodes or chunks.'],\n",
       " 'redundant revisit': ['Strategy involves reflecting on previous actions to prevent redundant revisiting of nodes or chunks.'],\n",
       " 'other not': ['It is important to incorporate complementary information from other notes.'],\n",
       " 'john': ['John was given a question about the text after reading some text.',\n",
       "  'The text asks if John’s answer agrees with the ground truth answer.',\n",
       "  'John’s answer to the question was a model response.'],\n",
       " 'given': ['John was given a question about the text after reading some text.'],\n",
       " 'model respons': ['John’s answer to the question was a model response.'],\n",
       " 'reference respons': ['The ground truth answer was referenced as a response.'],\n",
       " 'agre': ['The text asks if John’s answer agrees with the ground truth answer.'],\n",
       " 'ye': ['If John\\'s response is more specific than the ground truth answer, it is rated as \"Yes\".',\n",
       "  'In another figure, John is rated on his answer with three options: \"Yes\", \"Yes, partially\", or \"No\".',\n",
       "  'If John\\'s response contains the ground truth answer it is rated as \"Yes\".',\n",
       "  'The answer to the agreement question could be \"Yes\" or \"No\".'],\n",
       " 'no': ['In another figure, John is rated on his answer with three options: \"Yes\", \"Yes, partially\", or \"No\".',\n",
       "  'The answer to the agreement question could be \"Yes\" or \"No\".'],\n",
       " 'rate': ['If John\\'s response is more specific than the ground truth answer, it is rated as \"Yes\".',\n",
       "  'In another figure, John is rated on his answer with three options: \"Yes\", \"Yes, partially\", or \"No\".',\n",
       "  'If John\\'s response contains the ground truth answer it is rated as \"Yes\".',\n",
       "  'If John\\'s response has any overlap with the ground truth answer, it is rated as \"Yes, partially\".'],\n",
       " 'option': ['In another figure, John is rated on his answer with three options: \"Yes\", \"Yes, partially\", or \"No\".'],\n",
       " 'yes  parti': ['In another figure, John is rated on his answer with three options: \"Yes\", \"Yes, partially\", or \"No\".',\n",
       "  'If John\\'s response has any overlap with the ground truth answer, it is rated as \"Yes, partially\".'],\n",
       " 'john s respons': ['If John\\'s response is more specific than the ground truth answer, it is rated as \"Yes\".',\n",
       "  'If John\\'s response contains the ground truth answer it is rated as \"Yes\".',\n",
       "  'If John\\'s response has any overlap with the ground truth answer, it is rated as \"Yes, partially\".'],\n",
       " 'overlap': ['If John\\'s response has any overlap with the ground truth answer, it is rated as \"Yes, partially\".'],\n",
       " 'specif': ['If John\\'s response is more specific than the ground truth answer, it is rated as \"Yes\".'],\n",
       " 'passag': ['Another prompt requires reading a passage and answering a question based on that passage.'],\n",
       " 'based on': ['Another prompt requires reading a passage and answering a question based on that passage.'],\n",
       " 'passage cont': ['The question is based on the passage content.'],\n",
       " 'instruct': ['The instruction is to read text chunks and answer the question.'],\n",
       " 'respons': ['If insufficient information is present, the response should be [unanswerable].',\n",
       "  'If able to answer, the response should be [answerable] followed by the answer.'],\n",
       " 'insufficient inform': ['If insufficient information is present, the response should be [unanswerable].'],\n",
       " ' unanswerable ': ['If insufficient information is present, the response should be [unanswerable].'],\n",
       " 'figur': ['The prompt includes a figure related to Full Text Read and Chunk Read instructions.'],\n",
       " 'studio 606': ['\"Never Too Loud\" was recorded at Studio 606 in Los Angeles, with producer Nick Raskulinecz.'],\n",
       " 'los angel': ['\"Never Too Loud\" was recorded at Studio 606 in Los Angeles, with producer Nick Raskulinecz.'],\n",
       " 'produc': ['\"Never Too Loud\" was recorded at Studio 606 in Los Angeles, with producer Nick Raskulinecz.'],\n",
       " 'nick raskulinecz': ['\"Never Too Loud\" was recorded at Studio 606 in Los Angeles, with producer Nick Raskulinecz.'],\n",
       " 'john  jc  calabres': ['The band consists of Danko Jones, John \"JC\" Calabrese, and Rich Knox.',\n",
       "  'Danko Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums) make up the ensemble.'],\n",
       " 'rich knox': ['The band consists of Danko Jones, John \"JC\" Calabrese, and Rich Knox.',\n",
       "  'Danko Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums) make up the ensemble.'],\n",
       " 'vocal': ['Danko Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums) make up the ensemble.'],\n",
       " 'guitar': ['Danko Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums) make up the ensemble.'],\n",
       " 'bass': ['Danko Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums) make up the ensemble.'],\n",
       " 'drum': ['Danko Jones (vocals/guitar), John \"JC\" Calabrese (bass), and Rich Knox (drums) make up the ensemble.'],\n",
       " 'band s mus': ['The band’s music includes elements of hard rock and punk.'],\n",
       " 'element': ['The band’s music includes elements of hard rock and punk.'],\n",
       " 'punk': ['The band’s music includes elements of hard rock and punk.'],\n",
       " 'known': ['Danko Jones is known for their energetic live shows.'],\n",
       " 'energet': ['Danko Jones is known for their energetic live shows.'],\n",
       " 'live show': ['Danko Jones is known for their energetic live shows.'],\n",
       " 'gothic reviv': ['Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario, Canada.',\n",
       "  '\"Never Too Loud\" is associated with the Gothic Revival castle-style mansion, Casa Loma, located in Toronto, Canada.',\n",
       "  'Casa Loma is characterized by its Gothic Revival architecture.',\n",
       "  'Casa Loma is located in midtown Toronto, Ontario, Canada.'],\n",
       " 'castle style mans': ['Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario, Canada.',\n",
       "  '\"Never Too Loud\" is associated with the Gothic Revival castle-style mansion, Casa Loma, located in Toronto, Canada.',\n",
       "  'Casa Loma is located in midtown Toronto, Ontario, Canada.'],\n",
       " 'garden': ['Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario, Canada.'],\n",
       " 'midtown toronto': ['Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario, Canada.',\n",
       "  'Casa Loma is located in midtown Toronto, Ontario, Canada.'],\n",
       " 'ontario': ['Casa Loma is a Gothic Revival castle-style mansion and garden in midtown Toronto, Ontario, Canada.',\n",
       "  'Casa Loma is located in midtown Toronto, Ontario, Canada.'],\n",
       " 'historic house museum': ['Casa Loma is now a historic house museum and landmark.'],\n",
       " 'landmark': ['Casa Loma is now a historic house museum and landmark.',\n",
       "  'E. J. Lennox designed several other city landmarks.'],\n",
       " '1911': ['Casa Loma was constructed from 1911 to 1914 as a residence for financier Sir Henry Pellatt.'],\n",
       " 'resid': ['Casa Loma was constructed from 1911 to 1914 as a residence for financier Sir Henry Pellatt.'],\n",
       " 'financi': ['Casa Loma was constructed from 1911 to 1914 as a residence for financier Sir Henry Pellatt.'],\n",
       " 'sir henry pellatt': ['Casa Loma was constructed from 1911 to 1914 as a residence for financier Sir Henry Pellatt.'],\n",
       " 'architect': ['The architect of Casa Loma was E. J. Lennox.',\n",
       "  'Casa Loma is characterized by its Gothic Revival architecture.'],\n",
       " 'e  j  lennox': ['The architect of Casa Loma was E. J. Lennox.',\n",
       "  'E. J. Lennox designed several other city landmarks.'],\n",
       " 'sever': ['E. J. Lennox designed several other city landmarks.']}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_greader = aggregate_tags(greader_dict)\n",
    "print(len(agg_greader))\n",
    "agg_greader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
